label,Issue_KEY,Commit_SHA,Issue_Text,Commit_Text,Commit_Code
0,BAHIR-29,29d8c7622cf9663e295d7616ae1e1b089fe80da9,Add documentation for streaming-mqtt connector  ,[BAHIR-31] Add basic documentation for ZeroMQ connector, ADD README.md
0,BAHIR-68,cc9cf1ba2097fad00f8ff173434bfdabf4e3f10c,Add documentation for existing Flink connectors  ,[BAHIR-62] Prepare release based on Apache Spark 2.0.1, MODIFY pom.xml
0,BAHIR-22,76bfd8b2509128ae2a4ed7f59bcf4dac75c0296a,"Add script to run examples Apache Spark has a convenience script {{./bin/run-example}} to allow users to quickly run the pre-packaged examples without having to compose a long(ish) spark-submit command. The JavaDoc of most examples refers to that  {{./bin/run-example}} script in their description of how to run that example.

The Apache Bahir project should have a similar convenience script to be consistent with Apache Spark, existing documentation and to (at least initially) hide additional complexities of the spark-submit command.

Example:
{code}
./bin/run-example \
  org.apache.spark.examples.streaming.akka.ActorWordCount localhost 9999
{code}
...translates to this {{spark-submit}} command:
{code}
${SPARK_HOME}/bin/spark-submit \
  --packages org.apache.bahir:spark-streaming-akka_2.11:2.0.0-SNAPSHOT \
  --class org.apache.spark.examples.streaming.akka.ActorWordCount \
    streaming-akka/target/spark-streaming-akka_2.11-2.0.0-SNAPSHOT-tests.jar \
  localhost 9999
{code}","[BAHIR-20] Create release helper scripts

Release script to automate :
- Preparing release artifacts
- Publishing maven artifacts in Scala 2.10 and 2.11
- Publishing snapshots", ADD release-build.sh
0,BAHIR-41,9ad566815b8e2e654547d6022d20016025d49923,[WEBSITE] Fix logo display in small devices Reported by [~jreijn],[BAHIR-44] Add new sql-streaming-mqtt to distribution profile, MODIFY pom.xml
0,BAHIR-41,1abeab29c8a5e884f4603ef12abd85971a9105b0,[WEBSITE] Fix logo display in small devices Reported by [~jreijn],[BAHIR-42] Refactor sql-streaming-mqtt scala example, RENAME MQTTStreamWordCount.scala
0,BAHIR-29,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,Add documentation for streaming-mqtt connector  ,[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
1,BAHIR-149,f9a67de735fee8c89518cf37a513766c9e9e6b15,Upgrade cloudant dependency to release 2.11.0  ,[BAHIR-149] Update Cloudant dependency to release 2.11.0, MODIFY pom.xml
1,BAHIR-100,e3d9e6960941696ba073735e9d039c85146c217a,"Providing MQTT Spark Streaming to return encoded Byte[] message without corruption Now a days Network bandwidth is becoming a serious resource that need to be conserver in IoT ecosystem, For this puropse we are using different byte[] based encoding such as Protocol Buffer and flat Buffer, Once this encoded message is converted into string the data becomes corrupted, So same byte[] format need to be preserved when forwarded.","[BAHIR-100] Enhance MQTT connector to support byte arrays

Closes #47", MODIFY README.md MODIFY MQTTInputDStream.scala ADD MQTTPairedByteArrayInputDStream.scala onStart connectionLost getReceiver onStop messageArrived deliveryComplete MODIFY MQTTPairedInputDStream.scala MODIFY MQTTUtils.scala createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedStream createPairedStream createPairedStream MODIFY JavaMQTTStreamSuite.java testMQTTStream MODIFY MQTTStreamSuite.scala
1,BAHIR-84,bce9cd15989e648be03468c6a5b848ee8193df4d,"Build log flooded with test log messages The maven build log/console gets flooded with INFO messages from {{org.apache.parquet.hadoop.*}} during the {{test}} phase of module {{sql-streaming-mqtt}} . This makes it hard to find actual problems and test results especially when the log messages intersect with build and test status messages throwing off line breaks etc.

*Excerpt of build log:*

{code:title=$ mvn clean package}
...
Discovery completed in 293 milliseconds.
Run starting. Expected test count is: 7
BasicMQTTSourceSuite:
- basic usage
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
...
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is o- Send and receive 100 messages.
- no server up
- params not provided.
- Recovering offset from the last processed offset. !!! IGNORED !!!
StressTestMQTTSource:
- Send and receive messages of size 250MB. !!! IGNORED !!!
LocalMessageStoreSuite:
- serialize and deserialize
- Store and retreive
- Max offset stored
MQTTStreamSourceSuite:
Run completed in 20 seconds, 622 milliseconds.
Total number of tests run: 7
Suites: completed 5, aborted 0
Tests: succeeded 7, failed 0, canceled 0, ignored 2, pending 0
All tests passed.
ff
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 48
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 48
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [value] BINARY: 1 values, 34B raw, 36B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [timestamp] INT96: 1 values, 8B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries,...
{code}","[BAHIR-84] Suppress Parquet-MR build log messages

Since Parquet-MR (1.7.0) uses Java Simple Logging (not Log4j) we
need to add a logging.properties file and add it to the configuration
of the maven-surefire-plugin and scalatest-maven-plugin.
Since Parquet-MR is logging everything to System.out despite log file
handler settings we raise the threshold to ERROR.

https://github.com/Parquet/parquet-mr/issues/390
https://github.com/Parquet/parquet-mr/issues/425

Closes #33", MODIFY pom.xml ADD logging.properties
0,BAHIR-98,f0d9a84f76cb34a432e1d2db053d2471a8ab2ba4,"Jenkins does not run Scalatests Our Jenkins integration server currently only runs Java tests (JUnit) but no Scala tests.

i.e. [build log|http://169.45.79.58:8080/job/Apache%20Bahir%20-%20Pull%20Request%20Builder/35/console] for PR #[38|https://github.com/apache/bahir/pull/38#issuecomment-289624943]

...for *JUnit tests*:
{code}
17:20:37 [INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ spark-streaming-akka_2.11 ---
17:20:37 
17:20:37 -------------------------------------------------------
17:20:37  T E S T S
17:20:37 -------------------------------------------------------
17:20:37 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
17:20:37 Running org.apache.spark.streaming.akka.JavaAkkaUtilsSuite
17:20:39 Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.483 sec - in org.apache.spark.streaming.akka.JavaAkkaUtilsSuite
17:20:39 
17:20:39 Results :
17:20:39 
17:20:39 Tests run: 1, Failures: 0, Errors: 0, Skipped: 0
17:20:39 
{code}

...for *Scala tests*:
{code}
17:20:39 [INFO] --- maven-surefire-plugin:2.19.1:test (test) @ spark-streaming-akka_2.11 ---
17:20:39 [INFO] Skipping execution of surefire because it has already been run for this configuration
17:20:39 [INFO] 
17:20:39 [INFO] --- scalatest-maven-plugin:1.0:test (test) @ spark-streaming-akka_2.11 ---
17:20:39 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
17:20:39 Discovery starting.
17:20:39 Discovery completed in 34 milliseconds.
17:20:39 Run starting. Expected test count is: 0
17:20:39 DiscoverySuite:
17:20:39 Run completed in 95 milliseconds.
17:20:39 Total number of tests run: 0
17:20:39 Suites: completed 1, aborted 0
17:20:39 Tests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0
17:20:39 No tests were executed.
{code}","[BAHIR-101] Spark SQL datasource for CounchDB/Cloudant

Initial code supporting CounchDB/Cloudant as an Spark SQL
data source. The initial source contains the core connector,
examples, and basic documentation on the README.

Closes #39.", MODIFY README.md MODIFY pom.xml ADD README.md ADD CloudantApp.py ADD CloudantDF.py ADD CloudantDFOption.py ADD CloudantApp.scala main ADD CloudantDF.scala main ADD CloudantDFOption.scala main ADD CloudantStreaming.scala getInstance main ADD CloudantStreamingSelector.scala main ADD pom.xml ADD application.conf ADD reference.conf ADD CloudantConfig.scala getRangeUrl getDbname getTotalRows getForbiddenErrStr getConflictErrStr getOneUrlExcludeDDoc1 getSubSetUrl allowPartition getLastNum getTotalUrl getBulkRows calculateCondition getCreateDBonSave getLastUrl getContinuousChangesUrl getChangesUrl calculate getAllDocsUrlExcludeDDoc getOneUrl getSelector getAllDocsUrl getSchemaSampleSize getRows getBulkPostUrl getDbUrl getOneUrlExcludeDDoc2 ADD CloudantReceiver.scala onStart onStop receive run ADD DefaultSource.scala buildScan createRelation create insert createRelation createRelation ADD FilterUtil.scala filter containsFiltersFor apply analyze evaluate getFiltersForPostProcess getFilterAttribute getInfo ADD JsonStoreConfigManager.scala getString getConfig getConfig getBool getLong getInt ADD JsonStoreDataAccess.scala getChanges convert convertSkip getOne getClPostRequest saveAll getTotalRows createDB processAll getMany getIterator processIterator ADD JsonStoreRDD.scala compute ADD JsonUtil.scala getField
0,BAHIR-40,91e82f42bac58fd8d912e892a5ebfca79f6b8268,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],[BAHIR-37] Update Spark to release 2.0.0, MODIFY pom.xml
0,BAHIR-17,61f5592e4bd334a6c4c39cae2229dce53ee66535,Prepare release based on Apache Spark 2.0.0-preview  ,"[BAHIR-22] add shell script to run examples

Apache Spark has a convenience script ./bin/run-example to allow users to
quickly run the pre-packaged examples without having to compose a long(ish)
spark-submit command.
The JavaDoc of most examples refers to that ./bin/run-example script
in their description of how to run that example.

This adds a similar convenience script to the Apache Bahir project in order
to keep consistent with existing (Apache Spark) documentation and to
(at least initially) hide additional complexities of the spark-submit command.

Example:

./bin/run-example \
  org.apache.spark.examples.streaming.akka.ActorWordCount localhost 9999

...translates to this spark-submit command:

${SPARK_HOME}/bin/spark-submit \
  --packages org.apache.bahir:spark-streaming-akka_2.11:2.0.0-SNAPSHOT \
  --class org.apache.spark.examples.streaming.akka.ActorWordCount \
    streaming-akka/target/spark-streaming-akka_2.11-2.0.0-SNAPSHOT-tests.jar \
  localhost 9999

Closes #4", ADD run-example
1,BAHIR-175,5cfd7ac3154621b1780e2eb4719731030fc7d80a,"Recovering from Failures with Checkpointing Exception(Mqtt) Spark Version:2.2.0 spark-streaming-sql-mqtt version: 2.2.0

 
 # Reading checkpoints offsets error

{code:java}
org.apache.spark.sql.execution.streaming.SerializedOffset cannot be cast to org.apache.spark.sql.execution.streaming.LongOffset{code}
 

solution:

The MqttStreamSource.scala source file: 

Line 149, getBatch Method: 

 
{code:java}
val startIndex = start.getOrElse(LongOffset(0)) match
{ case offset: SerializedOffset => offset.json.toInt case offset: LongOffset => offset.offset.toInt }
val endIndex = end match
{ case offset: SerializedOffset => offset.json.toInt case offset: LongOffset => offset.offset.toInt }
{code}
 

2. The MqttStreamSource.scala source file

getBatch Method:

 
{code:java}
val data: ArrayBuffer[(String, Timestamp)] = ArrayBuffer.empty
 // Move consumed messages to persistent store.
 (startIndex + 1 to endIndex).foreach
{ id => val element = messages.getOrElse(id, store.retrieve(id)) data += element store.store(id, element) messages.remove(id, element) }
The following line：
val element = messages.getOrElse(id, store.retrieve(id)) throws error：
java.lang.ClassCastException: scala.Tuple2 cannot be cast to scala.runtime.Nothing$
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource$$anonfun$getBatch$1$$anonfun$3.apply(MQTTStreamSource.scala:160)
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource$$anonfun$getBatch$1$$anonfun$3.apply(MQTTStreamSource.scala:160)
 at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)
 at scala.collection.concurrent.TrieMap.getOrElse(TrieMap.scala:633)
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource$$anonfun$getBatch$1.apply$mcZI$sp(MQTTStreamSource.scala:160)
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource$$anonfun$getBatch$1.apply(MQTTStreamSource.scala:159)
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource$$anonfun$getBatch$1.apply(MQTTStreamSource.scala:159)
 at scala.collection.immutable.Range.foreach(Range.scala:160)
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource.getBatch(MQTTStreamSource.scala:159)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$3.apply(StreamExecution.scala:470)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$3.apply(StreamExecution.scala:466)
 at scala.collection.Iterator$class.foreach(Iterator.scala:891)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
 at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
 at org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:25)
 at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(StreamExecution.scala:466)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(StreamExecution.scala:297)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$apply$mcZ$sp$1.apply(StreamExecution.scala:294)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$apply$mcZ$sp$1.apply(StreamExecution.scala:294)
 at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:279)
 at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1.apply$mcZ$sp(StreamExecution.scala:294)
 at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
 at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:290)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:206)
{code}
 

 

solution:

 
{code:java}
val element: (String, Timestamp) = messages.getOrElse(id, store.retrieve[(String, Timestamp)](id))
 
{code}
 

 ","[BAHIR-175] Fix MQTT recovery after checkpoint

Closes #79", MODIFY MQTTStreamSource.scala messageArrived commit createDataReaderFactories initialize MODIFY MessageStore.scala MODIFY MQTTStreamSourceSuite.scala
0,BAHIR-22,335f605e75d426f62052378920880ab22729083e,"Add script to run examples Apache Spark has a convenience script {{./bin/run-example}} to allow users to quickly run the pre-packaged examples without having to compose a long(ish) spark-submit command. The JavaDoc of most examples refers to that  {{./bin/run-example}} script in their description of how to run that example.

The Apache Bahir project should have a similar convenience script to be consistent with Apache Spark, existing documentation and to (at least initially) hide additional complexities of the spark-submit command.

Example:
{code}
./bin/run-example \
  org.apache.spark.examples.streaming.akka.ActorWordCount localhost 9999
{code}
...translates to this {{spark-submit}} command:
{code}
${SPARK_HOME}/bin/spark-submit \
  --packages org.apache.bahir:spark-streaming-akka_2.11:2.0.0-SNAPSHOT \
  --class org.apache.spark.examples.streaming.akka.ActorWordCount \
    streaming-akka/target/spark-streaming-akka_2.11-2.0.0-SNAPSHOT-tests.jar \
  localhost 9999
{code}",[BAHIR-17] Update Apache Spark version back to 2.0.0-SNAPSHOT, MODIFY pom.xml
0,BAHIR-43,1abeab29c8a5e884f4603ef12abd85971a9105b0,Add missing apache license header to sql-mqtt file  ,[BAHIR-42] Refactor sql-streaming-mqtt scala example, RENAME MQTTStreamWordCount.scala
1,BAHIR-14,2dfcd08d11e94b535a39c31c87cf690f99944357,Cleanup maven pom from Spark dependencies There are a lot of dependencies that came from Spark and are not necessary for these extensions. We should cleanup the current poms and make the dependencies as lean as possible.,"[[BAHIR-14] More parent pom cleanup

Remove Spark assembly related configuration, and
stop producing source jars for non-jar projects.", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
1,BAHIR-182,9373fa4e7feb402f5b85367174afc5ad6b593a04,Create PubNub extension for Apache Spark Streaming Implement new connector for PubNub ([https://www.pubnub.com/)] which is increasing in popularity cloud messaging infrastructure.,"[BAHIR-182] Spark Streaming PubNub connector

Implement new connector for PubNub (https://www.pubnub.com/)
which is increasing in popularity as a cloud messaging infrastructure.

Closes #70", MODIFY README.md MODIFY pom.xml ADD README.md ADD PubNubWordCount.scala main ADD pom.xml ADD PubNubInputDStream.scala readExternal onStart message onStop writeVariableLength status writeExternal getReceiver presence readVariableLength ADD PubNubUtils.scala createStream createStream ADD LocalJavaStreamingContext.java setUp tearDown ADD JavaPubNubStreamSuite.java testPubNubStream ADD log4j.properties ADD MessageSerializationSuite.scala checkMessageSerialization ADD PubNubStreamSuite.scala beforeAll publishMessage afterAll
1,BAHIR-36,48e91fca54f7fab6f6171be4c05747a985876483,Update readme.md with build instructions  ,"[BAHIR-36] Update Readme.md

- Add how to build and test project

Closes #12", MODIFY README.md
1,BAHIR-88,ba68b3587ad4011a093bcaad921035f26907967c,"Source distribution contains release build artifacts There are 'releaseBackup' files and 'release.properties' in the 'source distribution' which are artifacts of the release build process.
We appear to be missing to run the mvn {{release:clean}} target _after_
the {{release:prepare}} target. We should fix that for the next release. All prior
Apache Bahir source distributions contained these release build artifacts.

[Mailing list post|https://www.mail-archive.com/dev@bahir.apache.org/msg00735.html]

Fixed by Luciano on June 7 with commits: [ba68b35|https://github.com/apache/bahir/commit/ba68b3587ad4011a093bcaad921035f26907967c], [6d9a4d7|https://github.com/apache/bahir/commit/6d9a4d7ab0c1eff0bf63e91cec32b601c263f790], [dcb4bbd|https://github.com/apache/bahir/commit/dcb4bbd2e4d75bc0872ce32c159b03a1d0f90047]",[BAHIR-88] Produce distributions without release temp files, MODIFY release-build.sh
1,BAHIR-116,56613263ca405aa5b45f32565ad4641f0a7b9752,Add Spark streaming connector for Google cloud Pub/Sub A spark streaming connector for [Google Pub/Sub|https://cloud.google.com/pubsub/] ,"[BAHIR-116] Add spark streaming connector to Google Cloud Pub/Sub

Cloases #42.", MODIFY pom.xml ADD README.md ADD PubsubWordCount.scala main ADD pom.xml ADD PubsubInputDStream.scala readExternal getData retryable onStart writeExternal getReceiver getMessageId getAttributes receive run getPublishTime onStop ADD PubsubUtils.scala createStream createStream createStream ADD SparkGCPCredentials.scala p12ServiceAccount jsonServiceAccount metadataServiceAccount build ADD LocalJavaStreamingContext.java setUp tearDown ADD JavaPubsubStreamSuite.java testPubsubStream ADD log4j.properties ADD PubsubFunSuite.scala testIfEnabled runIfTestsEnabled ADD PubsubStreamSuite.scala afterAll beforeAll ADD PubsubTestUtils.scala removeSubscription generatorMessages removeTopic getFullTopicPath publishData getFullSubscriptionPath createSubscription createTopic ADD SparkGCPCredentialsBuilderSuite.scala
0,BAHIR-147,55c60e5dd25c7c696118d2f2c8760fe5a17c1354,Update Flink extensions documentation with latest contents [~rmetzger] Looks like the website documentation for the current extensions has fallen behind and is causing some user issues (e.g. BAHIR-142). We should update the website with latest contents and references on how to add the spanshot to the test applications.,"[BAHIR-148] Use consistent MQTT client dependency version

Create a property to use a consistent version of the MQTT
client across all extensions based on MQTT.

For now, use org.eclipse.paho.client.mqttv3:1.1.0", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-45,eab486427186cee3c0f7ed8e440971f67f7ed832,"Add cleanup support to SQL-STREAMING-MQTT Source once SPARK-16963 is fixed. Currently, source tries to persist all the incoming messages to allow for minimal level of fault tolerance. Once SPARK-16963 is fixed, we would have a way to perform cleanup of messages no longer required. This would allow us to make the connector more fault tolerant.","[BAHIR-52] Update README.md formatting for source code

Update source code paragraphs to use tabs instead of ```
which is the supported way in vanilla Jekyll.", MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
1,BAHIR-65,a45bd84210b8e68640f97dd328e7e7053c8276e6,"Add test that Twitter streaming connector can receive data Add test cases that verify that the *Twitter streaming connector* can receive streaming data.

See [BAHIR-63|https://issues.apache.org/jira/browse/BAHIR-63]","[BAHIR-65] Twitter integration test

Closes #80", MODIFY README.md MODIFY TwitterStreamSuite.scala shouldRunTest
0,BAHIR-29,c78af705f5697ab11d93f933d033d96cc48403a0,Add documentation for streaming-mqtt connector  ,[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
1,BAHIR-37,c317def2a7575713d31353f87025ddacaf30e503,Prepare release based on Apache Spark 2.0.0  ,[BAHIR-37] Start building against Spark Master -  2.1.0-SNAPSHOT, MODIFY pom.xml MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
0,BAHIR-27,c78af705f5697ab11d93f933d033d96cc48403a0,Add documentation for existing streaming connectors  ,[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
0,BAHIR-36,91e82f42bac58fd8d912e892a5ebfca79f6b8268,Update readme.md with build instructions  ,[BAHIR-37] Update Spark to release 2.0.0, MODIFY pom.xml
0,BAHIR-103,561291bfc17f8eae97318b39ea9cc2d80680d5ce,"Refactoring of files BahirUtils.scala & Logging.scala into bahir-common The files BahirUtils.scala & Logging.scala present under the package 

`org.apache.bahir.utils` 

in the streaming-sql connectors should be refactored into a

`bahir-common` project , which will be shared across extensions.","[BAHIR-101] Update sql-cloudant readme and python examples

Closes #40.", MODIFY README.md MODIFY CloudantApp.py MODIFY CloudantDF.py MODIFY CloudantDFOption.py
0,BAHIR-13,86ee8779c7980e10c1c246fff7a171d0118c0239,Update spark tags dependency Looks like the spark-test-tags component was merged into spark-tags in 8ad9f08c9. We need to replace the spark-test-tags in bahir modules dependencies to spark-tags.,"[BAHIR-15] Enable RAT on builds

Enable RAT to run automatically during builds
to verify license header policy.", MODIFY pom.xml
0,BAHIR-30,5e07303c65e2c88cd392691bdfe9f68391f51b5c,Add documentation for streaming-twitter connector  ,"[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
0,BAHIR-41,70539a35dc9bae9ee5d380351ffc32fa6e62567e,[WEBSITE] Fix logo display in small devices Reported by [~jreijn],"[BAHIR-42] Refactor sql-streaming-mqtt example

Move JavaMQTTStreamWordCount to examples root folder
which are processed by the build as test resources
and not built into the extension itself following
the pattern used by other examples.", RENAME JavaMQTTStreamWordCount.java main
1,BAHIR-42,70539a35dc9bae9ee5d380351ffc32fa6e62567e,Refactor sql-streaming-mqtt examples to follow other projects pattern  ,"[BAHIR-42] Refactor sql-streaming-mqtt example

Move JavaMQTTStreamWordCount to examples root folder
which are processed by the build as test resources
and not built into the extension itself following
the pattern used by other examples.", RENAME JavaMQTTStreamWordCount.java main
0,BAHIR-28,29d8c7622cf9663e295d7616ae1e1b089fe80da9,Add documentation for streaming-akka connector  ,[BAHIR-31] Add basic documentation for ZeroMQ connector, ADD README.md
1,BAHIR-162,adeb24ba86cc2a406b4305c0199ae7c451862fe8,Update release scripts to stop publishing MD5 hash (updated release policy)  ,[BAHIR-162] Stop publishing MD5 hash with releases, MODIFY release-build.sh
0,BAHIR-41,c98dd0feefa79c70be65de06a411a2f9c4fc42dc,[WEBSITE] Fix logo display in small devices Reported by [~jreijn],"[BAHIR-39] Add SQL Streaming MQTT support

This provides support for using MQTT sources for
the new Spark Structured Streaming. This uses
MQTT client persistence layer to provide minimal
fault tolerance.

Closes #13", MODIFY pom.xml ADD README.md ADD pom.xml ADD assembly.xml ADD JavaMQTTStreamWordCount.java main ADD org.apache.spark.sql.sources.DataSourceRegister ADD MQTTStreamSource.scala stop connectionLost messageArrived fetchLastProcessedOffset createSource initialize deliveryComplete getBatch shortName sourceSchema connectComplete e ADD MessageStore.scala get this ADD MQTTStreamWordCount.scala main ADD BahirUtils.scala postVisitDirectory visitFile recursiveDeleteDir ADD Logging.scala ADD log4j.properties ADD LocalMessageStoreSuite.scala ADD MQTTStreamSourceSuite.scala createStreamingDataframe writeStreamResults readBackStreamingResults ADD MQTTTestUtils.scala setup publishData teardown findFreePort
0,BAHIR-152,55c60e5dd25c7c696118d2f2c8760fe5a17c1354,"License header not enforced for Java sources In any Java source file add this line on top of the license header:

{code}
/*
 * Copyright (c) 2017 ACME. All rights reserved.
 * <p>
{code}

then run {{mvn clean verify}} and see the build succeed.

Neither of the following maven goals complains either:
 - {{mvn apache-rat:check}}
 - {{mvn checkstyle:check}}

Adding a similar copyright statement to any of the Scala sources will fail the {{HeaderMatchesChecker}} rule in our {{scalastyle}} checks.

I am not sure why the [RAT|http://creadur.apache.org/rat/apache-rat-plugin/usage.html] checks allow this to pass, but I will add a similar {{Header}} verification to our {{checkstyle}} configuration.","[BAHIR-148] Use consistent MQTT client dependency version

Create a property to use a consistent version of the MQTT
client across all extensions based on MQTT.

For now, use org.eclipse.paho.client.mqttv3:1.1.0", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-41,95633de6741ddf757cc4964425463a972e1b4cbe,[WEBSITE] Fix logo display in small devices Reported by [~jreijn],[BAHIR-43] Add Apache License header file, MODIFY pom.xml MODIFY org.apache.spark.sql.sources.DataSourceRegister
1,BAHIR-154,6ea42a8965d98773a342ad5cd31aab6b64e9d9bd,"Refactor sql-cloudant to use Cloudant's java-cloudant features Cloudant's java-cloudant library (which is currently used for testing) contains several features that sql-cloudant can benefit from:
- HTTP 429 backoff
- View builder API to potentially simplify loading for _all_docs/views
- Improved exception handling when executing HTTP requests
- Future support for IAM API key

Would need to replace current scala HTTP library with OkHttp library, and also replace play-json with GSON library.","[BAHIR-154] Refactor sql-cloudant to use cloudant-client library

- Use java-cloudant’s executeRequest for HTTP requests against
  _all_docs endpoint
- Added HTTP 429 backoff with default settings
- Simplified caught exception and message for schema size
- Replaced scala http library with okhttp library for changes receiver
- Updated streaming CloudantReceiver class to use improved
  ChangesRowScanner method
- Replaced Play JSON with GSON library
- Updated save operation to use java-cloudant bulk API
- Use _changes feed filter option for Cloudant/CouchDB 2.x and greater

Closes #61", MODIFY README.md MODIFY pom.xml MODIFY application.conf MODIFY CloudantChangesConfig.scala MODIFY CloudantConfig.scala executeRequest getRows getTotalRows getLastNum getUrl getBulkRows calculateCondition getLastNum calculate getRangeUrl getResultCount getRows getSubSetUrl allowPartition MODIFY CloudantReceiver.scala receive MODIFY DefaultSource.scala create MODIFY FilterUtil.scala filter filter apply apply MODIFY JsonStoreConfigManager.scala getInt getConfig MODIFY JsonStoreDataAccess.scala processAll getMany processIterator convert getTotalRows saveAll createDB getChanges getClRequest convert MODIFY JsonStoreRDD.scala convertToMangoJson convertAttrToMangoJson getLimitPerPartition compute MODIFY JsonUtil.scala getField toJson getField MODIFY ChangesReceiver.scala receive MODIFY CloudantOptionSuite.scala
0,BAHIR-123,5b76e629697d7f3e0094929ce3003e2425368fbc,"Fix errors to support the latest version of Play JSON library for sql-cloudant The latest version is 2.6.2.  Error during mvn install -pl sql-cloudant after updating play-json to 2.6.2 in sql-cloudant/pom.xml:

[ERROR] /Users/estebanmlaver/emlaver-bahir/sql-cloudant/src/main/scala/org/apache/bahir/cloudant/common/JsonStoreConfigManager.scala:19: object typesafe is not a member of package com
[ERROR] import com.typesafe.config.ConfigFactory
[ERROR]            ^
[ERROR] /Users/estebanmlaver/emlaver-bahir/sql-cloudant/src/main/scala/org/apache/bahir/cloudant/common/JsonStoreConfigManager.scala:52: not found: value ConfigFactory
[ERROR]   private val configFactory = ConfigFactory.load()
[ERROR]                               ^
[ERROR] two errors found

Maven compile dependencies need to be added to pom.xml that existed in play-json 2.5.9 but were removed in 2.6.2.

Additional info. from Patrick Titzler between play-json versions 2.5.x and 2.6.x:
Looks like the parameter data type has been changed from `Seq[JsValue]` (https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.libs.json.JsArray) to `IndexedSeq[JsValue]` https://playframework.com/documentation/2.6.x/api/scala/index.html#play.api.libs.json.JsArray",[BAHIR-124] Update Spark depedency to version 2.2.0, MODIFY pom.xml
0,BAHIR-31,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,Add documentation for streaming-zeromq connector  ,[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
0,BAHIR-124,e3d9e6960941696ba073735e9d039c85146c217a,Update Bahir to use Spark 2.2.0 dependency  ,"[BAHIR-100] Enhance MQTT connector to support byte arrays

Closes #47", MODIFY README.md MODIFY MQTTInputDStream.scala ADD MQTTPairedByteArrayInputDStream.scala onStart connectionLost getReceiver onStop messageArrived deliveryComplete MODIFY MQTTPairedInputDStream.scala MODIFY MQTTUtils.scala createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedStream createPairedStream createPairedStream MODIFY JavaMQTTStreamSuite.java testMQTTStream MODIFY MQTTStreamSuite.scala
1,BAHIR-166,be1effaaf7cfde28d19e032e038694e01fbf169b,Migrate akka sql streaming source to datasrouce v2 API  ,"[BAHIR-166] Migrate akka sql streaming source to DataSource v2 API

Migrate akka sql streaming source to DataSource v2 API.

Closes #67", MODIFY pom.xml MODIFY README.md MODIFY AkkaStreamSource.scala shortName commit createMicroBatchReader stop fetchLastProcessedOffset deserializeOffset createSource get setOffsetRange sourceSchema e getBatch close receive readSchema createDataReaderFactories next createDataReader ADD LongOffset.scala + apply - convert MODIFY MessageStore.scala get get MODIFY AkkaStreamSourceSuite.scala MODIFY AkkaTestUtils.scala shutdown MODIFY ActorWordCount.scala main MODIFY ActorReceiver.scala onStop MODIFY AkkaStreamSuite.scala MODIFY ZeroMQWordCount.scala main
0,BAHIR-28,619936d39d18b7af45b7acec9af02b599a43b056,Add documentation for streaming-akka connector  ,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
0,BAHIR-16,cad277e611388ad61e3c7fcb4e8a2e796d0e983d,"Build issues due to log4j properties not found log4j:ERROR Could not read configuration file from URL [file:src/test/resources/log4j.properties].
java.io.FileNotFoundException: src/test/resources/log4j.properties (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:273)
	at org.apache.hadoop.util.ShutdownHookManager.<clinit>(ShutdownHookManager.java:44)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:179)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:152)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:57)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.streaming.StreamingContext$.<init>(StreamingContext.scala:730)
	at org.apache.spark.streaming.StreamingContext$.<clinit>(StreamingContext.scala)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:100)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:63)
	at org.apache.spark.streaming.akka.JavaAkkaUtilsSuite.testAkkaUtils(JavaAkkaUtilsSuite.java:36)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
log4j:ERROR Ignoring configuration file [file:src/test/resources/log4j.properties].",[BAHIR-7] Update Apache Spark version to 2.0.0-preview, MODIFY pom.xml
0,BAHIR-116,2a43076579dd247db4afa01e2dc0d1176c3eb4a1,Add Spark streaming connector for Google cloud Pub/Sub A spark streaming connector for [Google Pub/Sub|https://cloud.google.com/pubsub/] ,"[BAHIR-120] Akka SQL Streaming build fails with Apache Spark 2.1.1

Closes #44.", MODIFY AkkaStreamSourceSuite.scala
0,BAHIR-102,561291bfc17f8eae97318b39ea9cc2d80680d5ce,"Support pushing down query predicates using Cloudant Query / CouchDB Mango Query The feature of Cloudant Query / ChoudDB Mango Query is stated here: https://blog.couchdb.org/2016/08/03/feature-mango-query/.

Will enable pushing down first level AND column filtering to _find selector to replace _all_docs for the database query; will continue using skip+limit to offer partition and parallel loading.","[BAHIR-101] Update sql-cloudant readme and python examples

Closes #40.", MODIFY README.md MODIFY CloudantApp.py MODIFY CloudantDF.py MODIFY CloudantDFOption.py
0,BAHIR-188,172d7096147cd0be70687af893a4d71380ce47bf,update flink to 1.7.0 Update Flink to last version (1.7.0),"[BAHIR-183] Using HDFS for saving message for mqtt source.

Closes #78", MODIFY pom.xml MODIFY org.apache.spark.sql.sources.DataSourceRegister MODIFY CachedMQTTClient.scala createMqttClient MODIFY MQTTStreamSink.scala initialize MODIFY MQTTStreamSource.scala createMicroBatchReader MODIFY MQTTUtils.scala parseConfigParams ADD HDFSMQTTSourceProvider.scala createSource shortName sourceSchema ADD HdfsBasedMQTTStreamSource.scala startWriteNewDataFile initialize deliveryComplete connectionLost isBatchFile commit getOffsetValue addReceivedMessageInfo accept connectComplete getBatch messageArrived stop hasNewMessageForCurrentBatch ADD HDFSBasedMQTTStreamSourceSuite.scala createStreamingDataFrame readBackStreamingResults shutdownHadoop prepareHadoop writeStreamResults
0,BAHIR-13,7bc3d6e91ce8bf2b159032cf781a09e90854a19f,Update spark tags dependency Looks like the spark-test-tags component was merged into spark-tags in 8ad9f08c9. We need to replace the spark-test-tags in bahir modules dependencies to spark-tags.,"[BAHIR-14] Cleanup Bahir parent pom

The Bahir parent pom was initially based on Spark parent pom
and was bringing a lot of unecessary dependencies. This commit
cleans most of the unused properties, dependencies, etc.

Closes #1", MODIFY pom.xml MODIFY ActorWordCount.scala MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
1,BAHIR-164,b3902bac67edc2134bcc2c755fadc5c60c8ae01c,"Spark Streaming with MQTT fails with Spark 2.3.0 Currently I’m not able to use latest Spark 2.3.0 release together with Bahir, but getting the following error:
 
_java.lang.AssertionError: assertion failed: DataFrame returned by getBatch from org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource@66847d45 did not have isStreaming=true_
 
org/apache/bahir/sql/streaming/mqtt/MQTTStreamSource.scala line 164 seems to be causing the issue, i.e. it’s returning a normal dataframe instead of streaming dataframe.
 
There seems to be discussion about the same topic, but on stream source for Kafka: https://lists.apache.org/thread.html/015f2fbe9fbeafb86214a969a2d883a4898621189a86f21a4c6fbb75@%3Cdev.spark.apache.org%3E","[BAHIR-164][BAHIR-165] Port Mqtt sql source to datasource v2 API

Migrating Mqtt spark structured streaming connector to DatasourceV2 API.

Closes #65", MODIFY pom.xml MODIFY AkkaStreamSourceSuite.scala MODIFY README.md MODIFY JavaMQTTStreamWordCount.java main MODIFY MQTTStreamWordCount.scala main ADD LongOffset.scala + apply - convert MODIFY MQTTStreamSource.scala e sourceSchema getEndOffset initialize readSchema createDataReaderFactories next setOffsetRange createMicroBatchReader fetchLastProcessedOffset createDataReader toString commit stop messageArrived getStartOffset deserializeOffset createSource get getBatch close MODIFY MessageStore.scala this get get this getInstance ADD test-BAHIR-83.sh MODIFY LocalMessageStoreSuite.scala MODIFY MQTTStreamSourceSuite.scala readBackStreamingResults writeStreamResults createStreamingDataframe createStreamingDataframe writeStreamResults MODIFY MQTTTestUtils.scala publishData teardown
0,BAHIR-43,9ad566815b8e2e654547d6022d20016025d49923,Add missing apache license header to sql-mqtt file  ,[BAHIR-44] Add new sql-streaming-mqtt to distribution profile, MODIFY pom.xml
1,BAHIR-186,a73ab48a2dfec866b2ffa0ccf0d2bfeaba6fc782,Support SSL connection in MQTT SQL Streaming Mailing list discussion: https://www.mail-archive.com/user@bahir.apache.org/msg00022.html.,"[BAHIR-186] SSL support in MQTT structured streaming

Closes #74", MODIFY README.md MODIFY MQTTUtils.scala parseConfigParams ADD keystore.jks ADD truststore.jks MODIFY MQTTStreamSinkSuite.scala sendToMQTT MODIFY MQTTTestUtils.scala connectToServer publishData subscribeData setup
0,BAHIR-43,29d8c7622cf9663e295d7616ae1e1b089fe80da9,Add missing apache license header to sql-mqtt file  ,[BAHIR-31] Add basic documentation for ZeroMQ connector, ADD README.md
0,BAHIR-40,c78af705f5697ab11d93f933d033d96cc48403a0,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
1,BAHIR-148,55c60e5dd25c7c696118d2f2c8760fe5a17c1354,"Use same version of MQTT client in all MQTT extensions Currently MQTT streaming connector is using org.eclipse.paho.client.mqttv3:1.0.2 while MQTT structured streaming data source is using org.eclipse.paho.client.mqttv3:1.1.0

They should all use org.eclipse.paho.client.mqttv3:1.1.0","[BAHIR-148] Use consistent MQTT client dependency version

Create a property to use a consistent version of the MQTT
client across all extensions based on MQTT.

For now, use org.eclipse.paho.client.mqttv3:1.1.0", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-217,68ac1be22ddccecf105aee355a8c2652868e9f7d,"Install of Oracle JDK 8 Failing in Travis CI Install of Oracle JDK 8 Failing in Travis CI. As a result, build is failing for new pull requests.

We need to make a small fix in _ __ .travis.yml_ file as mentioned in the issue here:
https://travis-ci.community/t/install-of-oracle-jdk-8-failing/3038
We just need to add 
{code:java}
dist: trusty{code}
in the .travis.yml file as mentioned in the issue above.

I can raise a PR for this fix if required.",[BAHIR-172 ] Replace FileInputStream with Files.newInputStream (#92), MODIFY MQTTTestUtils.scala setup MODIFY SparkGCPCredentials.scala
1,BAHIR-101,561291bfc17f8eae97318b39ea9cc2d80680d5ce,Add Spark SQL datasource for CounchDB/Cloudant  ,"[BAHIR-101] Update sql-cloudant readme and python examples

Closes #40.", MODIFY README.md MODIFY CloudantApp.py MODIFY CloudantDF.py MODIFY CloudantDFOption.py
1,BAHIR-7,cad277e611388ad61e3c7fcb4e8a2e796d0e983d,Create website build script  ,[BAHIR-7] Update Apache Spark version to 2.0.0-preview, MODIFY pom.xml
0,BAHIR-19,9d63f090d8e4d443a811187dd5fd40a3c615dd41,Create Bahir source distribution  ,"[BAHIR-21] Change scala version script

Script to change scala version in use between 2.10 and 2.11
to help during development builds and also when publishing
releases in both scala version", ADD change-scala-version.sh
0,BAHIR-183,0601698c3721fb3db58431683e556af28ffc0d6a,"Using HDFS for saving message for mqtt source Currently in spark-sql-streaming-mqtt, the received mqtt message is saved in a local file by driver, this will have the risks of losing data for cluster mode when application master failover occurs. So saving in-coming mqtt messages using a director in checkpoint will solve this problem.","[BAHIR-103] New module with common utilities and test classes

Closes #73", ADD pom.xml RENAME FileHelper.scala recursiveDeleteDir deleteFileQuietly RENAME Logging.scala ADD Retry.scala RENAME ConditionalSparkFunSuite.scala runIf testIf RENAME LocalJavaStreamingContext.java setUp MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY ClientSparkFunSuite.scala beforeAll runIfTestsEnabled testIfEnabled afterAll MODIFY CloudantAllDocsDFSuite.scala MODIFY CloudantChangesDFSuite.scala MODIFY CloudantOptionSuite.scala MODIFY CloudantSparkSQLSuite.scala MODIFY TestUtils.scala deleteRecursively shouldRunTest MODIFY pom.xml MODIFY AkkaStreamSourceSuite.scala MODIFY pom.xml MODIFY CachedMQTTClient.scala MODIFY MQTTStreamSink.scala MODIFY MQTTUtils.scala DELETE BahirUtils.scala visitFile recursiveDeleteDir postVisitDirectory DELETE Logging.scala MODIFY LocalMessageStoreSuite.scala MODIFY MQTTStreamSinkSuite.scala MODIFY MQTTStreamSourceSuite.scala MODIFY pom.xml MODIFY JavaAkkaUtilsSuite.java call testAkkaUtils onReceive MODIFY pom.xml DELETE LocalJavaStreamingContext.java setUp tearDown MODIFY MQTTStreamSuite.scala MODIFY pom.xml MODIFY pom.xml DELETE LocalJavaStreamingContext.java tearDown setUp DELETE PubsubFunSuite.scala testIfEnabled runIfTestsEnabled MODIFY PubsubStreamSuite.scala beforeAll MODIFY PubsubTestUtils.scala shouldRunTest MODIFY pom.xml DELETE LocalJavaStreamingContext.java tearDown setUp MODIFY pom.xml
0,BAHIR-27,5e07303c65e2c88cd392691bdfe9f68391f51b5c,Add documentation for existing streaming connectors  ,"[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
0,BAHIR-149,55c60e5dd25c7c696118d2f2c8760fe5a17c1354,Upgrade cloudant dependency to release 2.11.0  ,"[BAHIR-148] Use consistent MQTT client dependency version

Create a property to use a consistent version of the MQTT
client across all extensions based on MQTT.

For now, use org.eclipse.paho.client.mqttv3:1.1.0", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-18,2dfcd08d11e94b535a39c31c87cf690f99944357,"Include examples in Maven test build We need to find a way to include the examples in the Maven build but keep them excluded from the binary jar(s) and have IDEs like IntelliJ or Eclipse recognize the {{<module>/examples/src/\[java|scala\]}} as source files.

One way this can be achieved is by including the examples as ""additional test sources"".","[[BAHIR-14] More parent pom cleanup

Remove Spark assembly related configuration, and
stop producing source jars for non-jar projects.", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-20,c277d4902e7538609523eee0ded3950e0d14d260,"Create release script Create script to help with release process that perform:
release-prepare
release-perform
release-snapshot
","[BAHIR-18] Configure examples as maven test sources

This PR configure examples as maven test resources to be
recognized by maven builds. This acomplish the following :

- The examples get compiled
- IDEs like IntelliJ or Eclipse recognize the
  <module>/examples/src/[java|scala] as source folders
- Keep the examples along with their additional dependencies
  excluded from the generated binaries

Closes #2", MODIFY pom.xml MODIFY JavaActorWordCount.java main MODIFY ActorWordCount.scala main MODIFY JavaAkkaUtilsSuite.java onReceive MODIFY MQTTWordCount.scala main MODIFY package.scala ADD AFINN-111.txt MODIFY JavaTwitterHashTagJoinSentiments.java main MODIFY TwitterAlgebirdCMS.scala main MODIFY TwitterAlgebirdHLL.scala main MODIFY TwitterHashTagJoinSentiments.scala main MODIFY TwitterPopularTags.scala main MODIFY pom.xml MODIFY package.scala MODIFY ZeroMQWordCount.scala main MODIFY package.scala
0,BAHIR-42,c98dd0feefa79c70be65de06a411a2f9c4fc42dc,Refactor sql-streaming-mqtt examples to follow other projects pattern  ,"[BAHIR-39] Add SQL Streaming MQTT support

This provides support for using MQTT sources for
the new Spark Structured Streaming. This uses
MQTT client persistence layer to provide minimal
fault tolerance.

Closes #13", MODIFY pom.xml ADD README.md ADD pom.xml ADD assembly.xml ADD JavaMQTTStreamWordCount.java main ADD org.apache.spark.sql.sources.DataSourceRegister ADD MQTTStreamSource.scala stop connectionLost messageArrived fetchLastProcessedOffset createSource initialize deliveryComplete getBatch shortName sourceSchema connectComplete e ADD MessageStore.scala get this ADD MQTTStreamWordCount.scala main ADD BahirUtils.scala postVisitDirectory visitFile recursiveDeleteDir ADD Logging.scala ADD log4j.properties ADD LocalMessageStoreSuite.scala ADD MQTTStreamSourceSuite.scala createStreamingDataframe writeStreamResults readBackStreamingResults ADD MQTTTestUtils.scala setup publishData teardown findFreePort
1,BAHIR-19,13b127593e79debfbc97a9c1215198c780df50a4,Create Bahir source distribution  ,"[BAHIR-19] Create source distribution assembly

Add assemblie to create Bahir source release distribution", ADD pom.xml ADD src.xml MODIFY pom.xml
0,BAHIR-122,a70ff538ac48ac1576984304d273e7a1f25fc2a6,"[PubSub] Make ""ServiceAccountCredentials"" really broadcastable The origin implementation broadcast the key file path to Spark cluster, then the executor read key file with the broadcasted path. Which is absurd, if you are using a shared Spark cluster in a group/company, you certainly not want to (and have no right to) put your key file on each instance of the cluster.

If you store the key file on driver node and submit your job to a remote cluster. You would get the following warning:
{{WARN ReceiverTracker: Error reported by receiver for stream 0: Failed to pull messages - java.io.FileNotFoundException}}","[BAHIR-125] Update Bahir parent pom

- Default build using JAVA 8
- Update dependencies to align with Spark 2.2.0", MODIFY pom.xml
0,BAHIR-175,a45bd84210b8e68640f97dd328e7e7053c8276e6,"Recovering from Failures with Checkpointing Exception(Mqtt) Spark Version:2.2.0 spark-streaming-sql-mqtt version: 2.2.0

 
 # Reading checkpoints offsets error

{code:java}
org.apache.spark.sql.execution.streaming.SerializedOffset cannot be cast to org.apache.spark.sql.execution.streaming.LongOffset{code}
 

solution:

The MqttStreamSource.scala source file: 

Line 149, getBatch Method: 

 
{code:java}
val startIndex = start.getOrElse(LongOffset(0)) match
{ case offset: SerializedOffset => offset.json.toInt case offset: LongOffset => offset.offset.toInt }
val endIndex = end match
{ case offset: SerializedOffset => offset.json.toInt case offset: LongOffset => offset.offset.toInt }
{code}
 

2. The MqttStreamSource.scala source file

getBatch Method:

 
{code:java}
val data: ArrayBuffer[(String, Timestamp)] = ArrayBuffer.empty
 // Move consumed messages to persistent store.
 (startIndex + 1 to endIndex).foreach
{ id => val element = messages.getOrElse(id, store.retrieve(id)) data += element store.store(id, element) messages.remove(id, element) }
The following line：
val element = messages.getOrElse(id, store.retrieve(id)) throws error：
java.lang.ClassCastException: scala.Tuple2 cannot be cast to scala.runtime.Nothing$
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource$$anonfun$getBatch$1$$anonfun$3.apply(MQTTStreamSource.scala:160)
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource$$anonfun$getBatch$1$$anonfun$3.apply(MQTTStreamSource.scala:160)
 at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)
 at scala.collection.concurrent.TrieMap.getOrElse(TrieMap.scala:633)
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource$$anonfun$getBatch$1.apply$mcZI$sp(MQTTStreamSource.scala:160)
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource$$anonfun$getBatch$1.apply(MQTTStreamSource.scala:159)
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource$$anonfun$getBatch$1.apply(MQTTStreamSource.scala:159)
 at scala.collection.immutable.Range.foreach(Range.scala:160)
 at org.apache.bahir.sql.streaming.mqtt.MQTTTextStreamSource.getBatch(MQTTStreamSource.scala:159)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$3.apply(StreamExecution.scala:470)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$3.apply(StreamExecution.scala:466)
 at scala.collection.Iterator$class.foreach(Iterator.scala:891)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
 at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
 at org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:25)
 at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(StreamExecution.scala:466)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(StreamExecution.scala:297)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$apply$mcZ$sp$1.apply(StreamExecution.scala:294)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$apply$mcZ$sp$1.apply(StreamExecution.scala:294)
 at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:279)
 at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1.apply$mcZ$sp(StreamExecution.scala:294)
 at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)
 at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:290)
 at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:206)
{code}
 

 

solution:

 
{code:java}
val element: (String, Timestamp) = messages.getOrElse(id, store.retrieve[(String, Timestamp)](id))
 
{code}
 

 ","[BAHIR-65] Twitter integration test

Closes #80", MODIFY README.md MODIFY TwitterStreamSuite.scala shouldRunTest
0,BAHIR-31,5e07303c65e2c88cd392691bdfe9f68391f51b5c,Add documentation for streaming-zeromq connector  ,"[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
0,BAHIR-39,619936d39d18b7af45b7acec9af02b599a43b056,MQTT as a streaming source for SQL Streaming. MQTT compatible streaming source for Spark SQL Streaming.,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
0,BAHIR-187,a73ab48a2dfec866b2ffa0ccf0d2bfeaba6fc782,"Reduce size of sql-cloudant test database Reduce the number of documents from 1967 to 100 in the n_flight.json test file.
 ","[BAHIR-186] SSL support in MQTT structured streaming

Closes #74", MODIFY README.md MODIFY MQTTUtils.scala parseConfigParams ADD keystore.jks ADD truststore.jks MODIFY MQTTStreamSinkSuite.scala sendToMQTT MODIFY MQTTTestUtils.scala connectToServer publishData subscribeData setup
0,BAHIR-42,eab486427186cee3c0f7ed8e440971f67f7ed832,Refactor sql-streaming-mqtt examples to follow other projects pattern  ,"[BAHIR-52] Update README.md formatting for source code

Update source code paragraphs to use tabs instead of ```
which is the supported way in vanilla Jekyll.", MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
0,BAHIR-43,c78af705f5697ab11d93f933d033d96cc48403a0,Add missing apache license header to sql-mqtt file  ,[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
1,BAHIR-15,86ee8779c7980e10c1c246fff7a171d0118c0239,Enable RAT on Bahir builds RAT check for license headers compliance on source code,"[BAHIR-15] Enable RAT on builds

Enable RAT to run automatically during builds
to verify license header policy.", MODIFY pom.xml
0,BAHIR-50,c317def2a7575713d31353f87025ddacaf30e503,Add extension documentation to Bahir website Provide documentation for the available extensions based on each extension readme file.,[BAHIR-37] Start building against Spark Master -  2.1.0-SNAPSHOT, MODIFY pom.xml MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
0,BAHIR-18,9d63f090d8e4d443a811187dd5fd40a3c615dd41,"Include examples in Maven test build We need to find a way to include the examples in the Maven build but keep them excluded from the binary jar(s) and have IDEs like IntelliJ or Eclipse recognize the {{<module>/examples/src/\[java|scala\]}} as source files.

One way this can be achieved is by including the examples as ""additional test sources"".","[BAHIR-21] Change scala version script

Script to change scala version in use between 2.10 and 2.11
to help during development builds and also when publishing
releases in both scala version", ADD change-scala-version.sh
1,BAHIR-183,172d7096147cd0be70687af893a4d71380ce47bf,"Using HDFS for saving message for mqtt source Currently in spark-sql-streaming-mqtt, the received mqtt message is saved in a local file by driver, this will have the risks of losing data for cluster mode when application master failover occurs. So saving in-coming mqtt messages using a director in checkpoint will solve this problem.","[BAHIR-183] Using HDFS for saving message for mqtt source.

Closes #78", MODIFY pom.xml MODIFY org.apache.spark.sql.sources.DataSourceRegister MODIFY CachedMQTTClient.scala createMqttClient MODIFY MQTTStreamSink.scala initialize MODIFY MQTTStreamSource.scala createMicroBatchReader MODIFY MQTTUtils.scala parseConfigParams ADD HDFSMQTTSourceProvider.scala createSource shortName sourceSchema ADD HdfsBasedMQTTStreamSource.scala startWriteNewDataFile initialize deliveryComplete connectionLost isBatchFile commit getOffsetValue addReceivedMessageInfo accept connectComplete getBatch messageArrived stop hasNewMessageForCurrentBatch ADD HDFSBasedMQTTStreamSourceSuite.scala createStreamingDataFrame readBackStreamingResults shutdownHadoop prepareHadoop writeStreamResults
1,BAHIR-138,785ee1e1acfb129bb0524d79df3372968b9e95a7,"Fix sql-cloudant deprecation messages Deprecation warnings in {{DefaultSource}}:

{code}
[INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ spark-sql-cloudant_2.11 ---
[INFO] Compiling 11 Scala sources to sql-cloudant/target/scala-2.11/classes...
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:59: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]         val df = sqlContext.read.json(cloudantRDD)
[WARNING]                                  ^
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:115: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]             dataFrame = sqlContext.read.json(cloudantRDD)
[WARNING]                                         ^
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:121: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]             sqlContext.read.json(aRDD)
[WARNING]                             ^
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:152: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]               dataFrame = sqlContext.sparkSession.read.json(globalRDD)
[WARNING]                                                        ^
[WARNING] four warnings found
{code}


Deprecation warnings in {{CloudantStreaming}} and {{CloudantStreamingSelector}} examples:

{code}
[INFO] --- scala-maven-plugin:3.2.2:testCompile (scala-test-compile-first) @ spark-sql-cloudant_2.11 ---
[INFO] Compiling 11 Scala sources to sql-cloudant/target/scala-2.11/test-classes...
[WARNING] sql-cloudant/examples/src/main/scala/org/apache/spark/examples/sql/cloudant/CloudantStreaming.scala:46: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]       val changesDataFrame = spark.read.json(rdd)
[WARNING]                                         ^
[WARNING] sql-cloudant/examples/src/main/scala/org/apache/spark/examples/sql/cloudant/CloudantStreaming.scala:67: method registerTempTable in class Dataset is deprecated: Use createOrReplaceTempView(viewName) instead.
[WARNING]           changesDataFrame.registerTempTable(""airportcodemapping"")
[WARNING]                            ^
[WARNING] sql-cloudant/examples/src/main/scala/org/apache/spark/examples/sql/cloudant/CloudantStreamingSelector.scala:50: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]       val changesDataFrame = spark.read.json(rdd)
[WARNING]                                         ^
[WARNING] three warnings found
{code}","[BAHIR-138] Fix deprecation warning messages

- Imported ‘spark.implicits._’ to convert Spark RDD to Dataset
- Replaced deprecated `json(RDD[String])` with `json(Dataset[String])`

Closes #63", MODIFY DefaultSource.scala createRelation createRelation insert buildScan create createRelation
1,BAHIR-38,5e07303c65e2c88cd392691bdfe9f68391f51b5c,"Spark-submit does not use latest locally installed Bahir packages We use {{`spark-submit --packages <maven-coordinates> ...`}} to run Spark with any of the Bahir extensions. 

In order to perform a _manual integration test_ of a Bahir code change developers have to _build_ the respective Bahir module and then _install_ it into their *local Maven repository*. Then, when running {{`spark-submit --packages <maven-coordinates> ...`}} Spark will use *Ivy* to resolve the given _maven-coordinates_ in order add the necessary jar files to the classpath.

The first time Ivy encounters new maven coordinates, it will download them from the local or remote Maven repository. All consecutive times Ivy will just use the previously cached jar files based on group ID, artifact ID and version, but irrespective of creation time stamp. 

This behavior is fine when using spark-submit with released versions of Spark packages. For continuous development and integration-testing however that Ivy caching behavior poses a problem. 

To *work around* it developers have to *clear the local Ivy cache* each time they _install_ a new version of a Bahir package into their local Maven repository and before the run spark-submit.

For example, to test a code change in module streaming-mqtt, we would have to do ...
{code}
mvn clean install -pl streaming-mqtt

rm -rf ~/.ivy2/cache/org.apache.bahir/spark-streaming-mqtt_2.11/

${SPARK_HOME}/bin/spark-submit \
    --packages org.apache.bahir:spark-streaming-mqtt_2.11:2.0.0-SNAPSHOT \
    test.py
{code}
","[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
0,BAHIR-39,5e07303c65e2c88cd392691bdfe9f68391f51b5c,MQTT as a streaming source for SQL Streaming. MQTT compatible streaming source for Spark SQL Streaming.,"[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
0,BAHIR-189,5cfd7ac3154621b1780e2eb4719731030fc7d80a,Update Siddhi version to 4.3 for flink-library-siddhi. Siddhi library was released in version 4.3. We should update the dependency for flink-library-siddhi module in bahir-flink.,"[BAHIR-175] Fix MQTT recovery after checkpoint

Closes #79", MODIFY MQTTStreamSource.scala messageArrived commit createDataReaderFactories initialize MODIFY MessageStore.scala MODIFY MQTTStreamSourceSuite.scala
1,BAHIR-102,fd4c35fc9f7ebb57464d231cf5d66e7bc4096a1b,"Support pushing down query predicates using Cloudant Query / CouchDB Mango Query The feature of Cloudant Query / ChoudDB Mango Query is stated here: https://blog.couchdb.org/2016/08/03/feature-mango-query/.

Will enable pushing down first level AND column filtering to _find selector to replace _all_docs for the database query; will continue using skip+limit to offer partition and parallel loading.","[BAHIR-102] Initial support of Cloudant Query and examples

Add optimization to use query in particular scenarios.

Closes #41.", MODIFY README.md ADD CloudantQuery.py ADD CloudantQueryDF.py MODIFY application.conf MODIFY CloudantConfig.scala getTotalRows getRows getOneUrlExcludeDDoc1 getSubSetUrl allowPartition getLastNum calculateCondition getCreateDBonSave getLastUrl getSubSetUrl getChangesUrl calculate getOneUrl getAllDocsUrlExcludeDDoc getAllDocsUrl calculate getRangeUrl getRows getOneUrlExcludeDDoc2 getAllDocsUrl allowPartition queryEnabled getDbUrl getRangeUrl getDbname MODIFY DefaultSource.scala buildScan create MODIFY JsonStoreConfigManager.scala getConfig getConfig MODIFY JsonStoreDataAccess.scala getTotalRows getIterator processAll processIterator getChanges getMany getOne getTotalRows convertSkip MODIFY JsonStoreRDD.scala getLimitPerPartition convertAttrToMangoJson compute convertToMangoJson getTotalPartition
0,BAHIR-44,eab486427186cee3c0f7ed8e440971f67f7ed832,Add new sql-streaming-mqtt to distribution  ,"[BAHIR-52] Update README.md formatting for source code

Update source code paragraphs to use tabs instead of ```
which is the supported way in vanilla Jekyll.", MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
0,BAHIR-43,eab486427186cee3c0f7ed8e440971f67f7ed832,Add missing apache license header to sql-mqtt file  ,"[BAHIR-52] Update README.md formatting for source code

Update source code paragraphs to use tabs instead of ```
which is the supported way in vanilla Jekyll.", MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
0,BAHIR-24,416252915431f01bb7a4ef4f8b7b9ed9ab02c3f5,Fix MQTT Python code When the Bahir project was created from Spark revision {{8301fadd8}} the Python code (incl. examples) were not updated with respect to the modified project structure and test cases were left out from the import.,"[BAHIR-35] Add Python sources to binary jar

Add python sources to jar to enable `spark-submit --packages …`

This can be verified by the following steps :

 mvn clean install

 rm -rf ~/.ivy2/cache/org.apache.bahir/

 mosquitto -p 1883

 bin/run-example \
    org.apache.spark.examples.streaming.mqtt.MQTTPublisher \
    tcp://localhost:1883 \
    foo

 ${SPARK_HOME}/bin/spark-submit \
    --packages org.apache.bahir:spark-streaming-mqtt_2.11:2.0.0-SNAPSHOT \
    streaming-mqtt/examples/src/main/python/streaming/mqtt_wordcount.py \
    tcp://localhost:1883 \
    foo

Closes #11", MODIFY pom.xml
0,BAHIR-188,0601698c3721fb3db58431683e556af28ffc0d6a,update flink to 1.7.0 Update Flink to last version (1.7.0),"[BAHIR-103] New module with common utilities and test classes

Closes #73", ADD pom.xml RENAME FileHelper.scala recursiveDeleteDir deleteFileQuietly RENAME Logging.scala ADD Retry.scala RENAME ConditionalSparkFunSuite.scala runIf testIf RENAME LocalJavaStreamingContext.java setUp MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY ClientSparkFunSuite.scala beforeAll runIfTestsEnabled testIfEnabled afterAll MODIFY CloudantAllDocsDFSuite.scala MODIFY CloudantChangesDFSuite.scala MODIFY CloudantOptionSuite.scala MODIFY CloudantSparkSQLSuite.scala MODIFY TestUtils.scala deleteRecursively shouldRunTest MODIFY pom.xml MODIFY AkkaStreamSourceSuite.scala MODIFY pom.xml MODIFY CachedMQTTClient.scala MODIFY MQTTStreamSink.scala MODIFY MQTTUtils.scala DELETE BahirUtils.scala visitFile recursiveDeleteDir postVisitDirectory DELETE Logging.scala MODIFY LocalMessageStoreSuite.scala MODIFY MQTTStreamSinkSuite.scala MODIFY MQTTStreamSourceSuite.scala MODIFY pom.xml MODIFY JavaAkkaUtilsSuite.java call testAkkaUtils onReceive MODIFY pom.xml DELETE LocalJavaStreamingContext.java setUp tearDown MODIFY MQTTStreamSuite.scala MODIFY pom.xml MODIFY pom.xml DELETE LocalJavaStreamingContext.java tearDown setUp DELETE PubsubFunSuite.scala testIfEnabled runIfTestsEnabled MODIFY PubsubStreamSuite.scala beforeAll MODIFY PubsubTestUtils.scala shouldRunTest MODIFY pom.xml DELETE LocalJavaStreamingContext.java tearDown setUp MODIFY pom.xml
0,BAHIR-187,cc61a83a79d912f8eb842507ecca0b2d82f734e6,"Reduce size of sql-cloudant test database Reduce the number of documents from 1967 to 100 in the n_flight.json test file.
 ","[BAHIR-141] Support GCP JSON key type as binary array

Closes #82
Closes #53", MODIFY README.md MODIFY SparkGCPCredentials.scala this metadataServiceAccount httpTransport p12ServiceAccount jsonServiceAccount build p12ServiceAccount scopes jsonServiceAccount jacksonFactory this MODIFY SparkGCPCredentialsBuilderSuite.scala jsonAssumption p12Assumption
0,BAHIR-214,549c50be02f98b93c5f79890332e8de97332e8f5,"Improve KuduConnector speed kudu connector has some issues on kudu sink with some flush modes that kill sink over time

 

this is a refactor to resolve that issues and improve speed on eventual consistence","[BAHIR-217] Installation of Oracle JDK8 is Failing in Travis CI (#93)

Install of Oracle JDK 8 Failing in Travis CI and as a result, 
build is failing for new pull requests.

We just need to add `dist: trusty` in the .travis.yml file 
as mentioned in the issue below:
https://travis-ci.community/t/install-of-oracle-jdk-8-failing/3038", MODIFY .travis.yml
0,BAHIR-68,942b43dc428c7ade2789fb09df0fda360cf94024,Add documentation for existing Flink connectors  ,"[BAHIR-69] Clean build between different scala version

During release-publish, execute a mvn clean between different scala
version builds", MODIFY release-build.sh
0,BAHIR-40,70539a35dc9bae9ee5d380351ffc32fa6e62567e,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],"[BAHIR-42] Refactor sql-streaming-mqtt example

Move JavaMQTTStreamWordCount to examples root folder
which are processed by the build as test resources
and not built into the extension itself following
the pattern used by other examples.", RENAME JavaMQTTStreamWordCount.java main
0,BAHIR-50,31b3e96d4680c54ff05d4b09f944106d22b62760,Add extension documentation to Bahir website Provide documentation for the available extensions based on each extension readme file.,[BAHIR-52] Fix code paragraph formatting for Akka readme, MODIFY README.md
1,BAHIR-120,2a43076579dd247db4afa01e2dc0d1176c3eb4a1,Akka SQL Streaming build fails with Apache Spark 2.1.1  ,"[BAHIR-120] Akka SQL Streaming build fails with Apache Spark 2.1.1

Closes #44.", MODIFY AkkaStreamSourceSuite.scala
0,BAHIR-55,28f034f49d19034b596f7f04ca4fc2698a21ad6c,"Move Redis Sink from Flink to Bahir As per http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Move-Redis-and-Flume-connectors-to-Apache-Bahir-and-redirect-contributions-there-td13102.html, the Flink community agreed to move the Redis connector to Bahir.","[BAHIR-53] Add new configuration options to MQTTInputDStream

Add new configuration options to enable secured connections and
other quality of services.

Closes #23", MODIFY scalastyle-config.xml MODIFY README.md MODIFY MQTTInputDStream.scala onStart getReceiver MODIFY MQTTUtils.scala createStream createStream createStream createStream createStream createStream createStream MODIFY JavaMQTTStreamSuite.java testMQTTStream
0,BAHIR-38,91e82f42bac58fd8d912e892a5ebfca79f6b8268,"Spark-submit does not use latest locally installed Bahir packages We use {{`spark-submit --packages <maven-coordinates> ...`}} to run Spark with any of the Bahir extensions. 

In order to perform a _manual integration test_ of a Bahir code change developers have to _build_ the respective Bahir module and then _install_ it into their *local Maven repository*. Then, when running {{`spark-submit --packages <maven-coordinates> ...`}} Spark will use *Ivy* to resolve the given _maven-coordinates_ in order add the necessary jar files to the classpath.

The first time Ivy encounters new maven coordinates, it will download them from the local or remote Maven repository. All consecutive times Ivy will just use the previously cached jar files based on group ID, artifact ID and version, but irrespective of creation time stamp. 

This behavior is fine when using spark-submit with released versions of Spark packages. For continuous development and integration-testing however that Ivy caching behavior poses a problem. 

To *work around* it developers have to *clear the local Ivy cache* each time they _install_ a new version of a Bahir package into their local Maven repository and before the run spark-submit.

For example, to test a code change in module streaming-mqtt, we would have to do ...
{code}
mvn clean install -pl streaming-mqtt

rm -rf ~/.ivy2/cache/org.apache.bahir/spark-streaming-mqtt_2.11/

${SPARK_HOME}/bin/spark-submit \
    --packages org.apache.bahir:spark-streaming-mqtt_2.11:2.0.0-SNAPSHOT \
    test.py
{code}
",[BAHIR-37] Update Spark to release 2.0.0, MODIFY pom.xml
0,BAHIR-48,29d8c7622cf9663e295d7616ae1e1b089fe80da9,Documentation improvements for Bahir README.md  ,[BAHIR-31] Add basic documentation for ZeroMQ connector, ADD README.md
0,BAHIR-47,95633de6741ddf757cc4964425463a972e1b4cbe,Bring up release download for Apache Bahir website  ,[BAHIR-43] Add Apache License header file, MODIFY pom.xml MODIFY org.apache.spark.sql.sources.DataSourceRegister
0,BAHIR-42,c78af705f5697ab11d93f933d033d96cc48403a0,Refactor sql-streaming-mqtt examples to follow other projects pattern  ,[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
1,BAHIR-30,c78af705f5697ab11d93f933d033d96cc48403a0,Add documentation for streaming-twitter connector  ,[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
0,BAHIR-84,b7cb52bab5117910d7c69a41d738dd670447fd13,"Build log flooded with test log messages The maven build log/console gets flooded with INFO messages from {{org.apache.parquet.hadoop.*}} during the {{test}} phase of module {{sql-streaming-mqtt}} . This makes it hard to find actual problems and test results especially when the log messages intersect with build and test status messages throwing off line breaks etc.

*Excerpt of build log:*

{code:title=$ mvn clean package}
...
Discovery completed in 293 milliseconds.
Run starting. Expected test count is: 7
BasicMQTTSourceSuite:
- basic usage
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
...
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is o- Send and receive 100 messages.
- no server up
- params not provided.
- Recovering offset from the last processed offset. !!! IGNORED !!!
StressTestMQTTSource:
- Send and receive messages of size 250MB. !!! IGNORED !!!
LocalMessageStoreSuite:
- serialize and deserialize
- Store and retreive
- Max offset stored
MQTTStreamSourceSuite:
Run completed in 20 seconds, 622 milliseconds.
Total number of tests run: 7
Suites: completed 5, aborted 0
Tests: succeeded 7, failed 0, canceled 0, ignored 2, pending 0
All tests passed.
ff
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 48
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 48
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [value] BINARY: 1 values, 34B raw, 36B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [timestamp] INT96: 1 values, 8B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries,...
{code}","[BAHIR-83] ignore test case ""Send and receive 100 messages.""", MODIFY MQTTStreamSourceSuite.scala
0,BAHIR-19,2dfcd08d11e94b535a39c31c87cf690f99944357,Create Bahir source distribution  ,"[[BAHIR-14] More parent pom cleanup

Remove Spark assembly related configuration, and
stop producing source jars for non-jar projects.", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
1,BAHIR-139,30e9a1de8ede8a551df15a1e8d8c89ebf4568a96,"Scala-maven-plugin does not respect Java compile level While working on PR [#28|https://github.com/apache/bahir/pull/28] we found that the [scala-maven-plugin|https://mvnrepository.com/artifact/net.alchim31.maven/scala-maven-plugin/3.2.2] does not respect the Java version ({{<java.version>1.8</java.version>}}) as configured by Java compile arguments {{-source}} and {{-target}}.


*[Bahir Scala-Maven-Plugin Configuration|https://github.com/apache/bahir/blob/3204f34aae679dd95c7fa5bdc9071fb2f4e52c16/pom.xml#L532]:*
{code:xml|title=pom.xml}
        <plugin>
          <groupId>net.alchim31.maven</groupId>
          <artifactId>scala-maven-plugin</artifactId>
          <version>3.2.2</version>
          ...
          <configuration>
            <scalaVersion>${scala.version}</scalaVersion>
            <recompileMode>incremental</recompileMode>
            <useZincServer>true</useZincServer>
            ...
            <javacArgs>
              <javacArg>-source</javacArg>
              <javacArg>${java.version}</javacArg>
              <javacArg>-target</javacArg>
              <javacArg>${java.version}</javacArg>
              <javacArg>-Xlint:all,-serial,-path</javacArg>
            </javacArgs>
          </configuration>
{code}


*Compile Errors PR #28:*
{code}
[bahir_pr28]$ mvn scala:compile -pl datasource-webhdfs
{code}

{code}
...
[ERROR] warning: [options] bootstrap class path not set in conjunction with -source 1.6

[ERROR] datasource-webhdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java:286: error: diamond operator is not supported in -source 1.6
[ERROR]       this.restCsrfMethodsToIgnore = new HashSet<>();
[ERROR]                                                  ^
[ERROR]   (use -source 7 or higher to enable diamond operator)

[ERROR] datasource-webhdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java:773: error: multi-catch statement is not supported in -source 1.6
[ERROR]           } catch (NoSuchMethodException | SecurityException 
[ERROR]                                          ^
[ERROR]   (use -source 7 or higher to enable multi-catch statement)

[ERROR] 2 errors
[ERROR] 1 warning
...
{code}


*Maven DEBUG Output:*

{code:title=mvn -X compile -pl datasource-webhdfs}
...
[INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ spark-datasource-webhdfs_2.11 ---
[DEBUG] Configuring mojo net.alchim31.maven:scala-maven-plugin:3.2.2:compile from plugin realm ClassRealm[plugin>net.alchim31.maven:scala-maven-plugin:3.2.2, parent: sun.misc.Launcher$AppClassLoader@74a14482]
[DEBUG] Configuring mojo 'net.alchim31.maven:scala-maven-plugin:3.2.2:compile' with basic configurator -->
[DEBUG]   (f) analysisCacheFile = /projects/bahir_pr28/datasource-webhdfs/target/analysis/compile
[DEBUG]   (f) args = [-unchecked, -deprecation, -feature]
[DEBUG]   (f) checkMultipleScalaVersions = true
[DEBUG]   (f) compileOrder = mixed
[DEBUG]   (f) displayCmd = false
[DEBUG]   (f) encoding = UTF-8
[DEBUG]   (f) failOnMultipleScalaVersions = false
[DEBUG]   (f) forceUseArgFile = false
[DEBUG]   (f) fork = true
[DEBUG]   (f) javacArgs = [-source, 1.8, -target, 1.8, -Xlint:all,-serial,-path]
[DEBUG]   (f) javacGenerateDebugSymbols = true
[DEBUG]   (f) jvmArgs = [-Xms1024m, -Xmx1024m, -XX:ReservedCodeCacheSize=512m]
[DEBUG]   (f) notifyCompilation = true
[DEBUG]   (f) outputDir = /projects/bahir_pr28/datasource-webhdfs/target/scala-2.11/classes
[DEBUG]   (f) pluginArtifacts = [net.alchim31.maven:scala-maven-plugin:maven-plugin:3.2.2:,...
[DEBUG]   (f) recompileMode = incremental
[DEBUG]   (f) scalaClassName = scala.tools.nsc.Main
[DEBUG]   (f) scalaOrganization = org.scala-lang
[DEBUG]   (f) scalaVersion = 2.11.8
[DEBUG]   (f) sendJavaToScalac = true
[DEBUG]   (f) session = org.apache.maven.execution.MavenSession@7a6ebe1e
[DEBUG]   (f) source = 1.6
[DEBUG]   (f) sourceDir = /projects/bahir_pr28/datasource-webhdfs/src/main/java/../scala
[DEBUG]   (f) target = 1.6
[DEBUG]   (f) useCanonicalPath = true
[DEBUG]   (f) useZincServer = true
[DEBUG]   (f) zincPort = 3030
[DEBUG] -- end configuration --
...
{code}

*Notice:*
{code}
[DEBUG]   (f) javacArgs = [-source, 1.8, -target, 1.8, -Xlint:all,-serial,-path]
[DEBUG]   (f) source = 1.6
[DEBUG]   (f) target = 1.6
{code}
Apparently the compile version defaults to Java 1.6 when the [{{source}}|http://davidb.github.io/scala-maven-plugin/compile-mojo.html#source] and [{{target}}|http://davidb.github.io/scala-maven-plugin/compile-mojo.html#target] elements are not explicitly set as part of the scala-maven-plugin {{<configuration>}}.


*Proposed Fix:*

Add the {{<source>}} and {{<target>}} elements to the scala-maven-plugin {{<configuration>}}:
{code:xml}
            <source>${java.version}</source>
            <target>${java.version}</target>
{code}


{code:xml|title=pom.xml}
        <plugin>
          <groupId>net.alchim31.maven</groupId>
          <artifactId>scala-maven-plugin</artifactId>
          <version>3.2.2</version>
          ...
          <configuration>
            <scalaVersion>${scala.version}</scalaVersion>
            <recompileMode>incremental</recompileMode>
            <useZincServer>true</useZincServer>
            ...
            <source>${java.version}</source>
            <target>${java.version}</target>
            <javacArgs>
              <javacArg>-source</javacArg>
              <javacArg>${java.version}</javacArg>
              <javacArg>-target</javacArg>
              <javacArg>${java.version}</javacArg>
              <javacArg>-Xlint:all,-serial,-path</javacArg>
            </javacArgs>
          </configuration>
{code}
","[BAHIR-139] Force scala-maven-plugin to use java.version

Make sure the scala-maven-plugin uses java.version 1.8 instead of the
default source and target version which is Java 1.6.

Upgrade the scala-maven-plugin version from 3.2.2 to 3.3.1

Closes #51", MODIFY pom.xml
0,BAHIR-96,561291bfc17f8eae97318b39ea9cc2d80680d5ce,"Add a ""release-build.sh"" script for bahir-flink We need to adopt the {{release-build.sh}} script from the Bahir Spark repo, in order to kick off the first Bahir Flink extensions release.","[BAHIR-101] Update sql-cloudant readme and python examples

Closes #40.", MODIFY README.md MODIFY CloudantApp.py MODIFY CloudantDF.py MODIFY CloudantDFOption.py
0,BAHIR-45,c317def2a7575713d31353f87025ddacaf30e503,"Add cleanup support to SQL-STREAMING-MQTT Source once SPARK-16963 is fixed. Currently, source tries to persist all the incoming messages to allow for minimal level of fault tolerance. Once SPARK-16963 is fixed, we would have a way to perform cleanup of messages no longer required. This would allow us to make the connector more fault tolerant.",[BAHIR-37] Start building against Spark Master -  2.1.0-SNAPSHOT, MODIFY pom.xml MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
0,BAHIR-49,eab486427186cee3c0f7ed8e440971f67f7ed832,Add MQTTSink to SQL Streaming MQTT.  ,"[BAHIR-52] Update README.md formatting for source code

Update source code paragraphs to use tabs instead of ```
which is the supported way in vanilla Jekyll.", MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
0,BAHIR-187,0601698c3721fb3db58431683e556af28ffc0d6a,"Reduce size of sql-cloudant test database Reduce the number of documents from 1967 to 100 in the n_flight.json test file.
 ","[BAHIR-103] New module with common utilities and test classes

Closes #73", ADD pom.xml RENAME FileHelper.scala recursiveDeleteDir deleteFileQuietly RENAME Logging.scala ADD Retry.scala RENAME ConditionalSparkFunSuite.scala runIf testIf RENAME LocalJavaStreamingContext.java setUp MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY ClientSparkFunSuite.scala beforeAll runIfTestsEnabled testIfEnabled afterAll MODIFY CloudantAllDocsDFSuite.scala MODIFY CloudantChangesDFSuite.scala MODIFY CloudantOptionSuite.scala MODIFY CloudantSparkSQLSuite.scala MODIFY TestUtils.scala deleteRecursively shouldRunTest MODIFY pom.xml MODIFY AkkaStreamSourceSuite.scala MODIFY pom.xml MODIFY CachedMQTTClient.scala MODIFY MQTTStreamSink.scala MODIFY MQTTUtils.scala DELETE BahirUtils.scala visitFile recursiveDeleteDir postVisitDirectory DELETE Logging.scala MODIFY LocalMessageStoreSuite.scala MODIFY MQTTStreamSinkSuite.scala MODIFY MQTTStreamSourceSuite.scala MODIFY pom.xml MODIFY JavaAkkaUtilsSuite.java call testAkkaUtils onReceive MODIFY pom.xml DELETE LocalJavaStreamingContext.java setUp tearDown MODIFY MQTTStreamSuite.scala MODIFY pom.xml MODIFY pom.xml DELETE LocalJavaStreamingContext.java tearDown setUp DELETE PubsubFunSuite.scala testIfEnabled runIfTestsEnabled MODIFY PubsubStreamSuite.scala beforeAll MODIFY PubsubTestUtils.scala shouldRunTest MODIFY pom.xml DELETE LocalJavaStreamingContext.java tearDown setUp MODIFY pom.xml
0,BAHIR-37,eab486427186cee3c0f7ed8e440971f67f7ed832,Prepare release based on Apache Spark 2.0.0  ,"[BAHIR-52] Update README.md formatting for source code

Update source code paragraphs to use tabs instead of ```
which is the supported way in vanilla Jekyll.", MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
1,BAHIR-52,31b3e96d4680c54ff05d4b09f944106d22b62760,"Update extension documentation formats for code sections The ```md format is not working properly for pure jekyll html generation, and the tab seems to be the supported way in vanilla jekyll. We should update Bahir extension readme to use the supported format.",[BAHIR-52] Fix code paragraph formatting for Akka readme, MODIFY README.md
0,BAHIR-42,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,Refactor sql-streaming-mqtt examples to follow other projects pattern  ,[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
0,BAHIR-41,5e07303c65e2c88cd392691bdfe9f68391f51b5c,[WEBSITE] Fix logo display in small devices Reported by [~jreijn],"[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
0,BAHIR-66,aecd5fd9f00e40b64ebe81269396bfdc42f8ed00,"Add test that ZeroMQ streaming connector can receive data Add test cases that verify that the *ZeroMQ streaming connector* can receive streaming data.

See [BAHIR-63|https://issues.apache.org/jira/browse/BAHIR-63]","[BAHIR-49] Sink for SQL Streaming MQTT module

Closes #68", MODIFY README.md ADD JavaMQTTSinkWordCount.java main ADD MQTTSinkWordCount.scala main MODIFY org.apache.spark.sql.sources.DataSourceRegister ADD CachedMQTTClient.scala mapToSeq onRemoval deliveryComplete close connectComplete closeMqttClient getOrCreate createMqttClient messageArrived load clear connectionLost ADD MQTTStreamSink.scala commit abort abort shortName commit createRelation createStreamWriter createWriterFactory initialize MODIFY MQTTStreamSource.scala createMicroBatchReader ADD MQTTUtils.scala e parseConfigParams MODIFY LocalMessageStoreSuite.scala ADD MQTTStreamSinkSuite.scala sendToMQTT waitForMessages createContextAndDF MODIFY MQTTStreamSourceSuite.scala createStreamingDataFrame createStreamingDataframe readBackStreamingResults MODIFY MQTTTestUtils.scala deliveryComplete connectionLost connectComplete messageArrived subscribeData publishData sleepUntil
0,BAHIR-98,826545cb8db4b89bbdb3927e53f555c0fa15771e,"Jenkins does not run Scalatests Our Jenkins integration server currently only runs Java tests (JUnit) but no Scala tests.

i.e. [build log|http://169.45.79.58:8080/job/Apache%20Bahir%20-%20Pull%20Request%20Builder/35/console] for PR #[38|https://github.com/apache/bahir/pull/38#issuecomment-289624943]

...for *JUnit tests*:
{code}
17:20:37 [INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ spark-streaming-akka_2.11 ---
17:20:37 
17:20:37 -------------------------------------------------------
17:20:37  T E S T S
17:20:37 -------------------------------------------------------
17:20:37 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
17:20:37 Running org.apache.spark.streaming.akka.JavaAkkaUtilsSuite
17:20:39 Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.483 sec - in org.apache.spark.streaming.akka.JavaAkkaUtilsSuite
17:20:39 
17:20:39 Results :
17:20:39 
17:20:39 Tests run: 1, Failures: 0, Errors: 0, Skipped: 0
17:20:39 
{code}

...for *Scala tests*:
{code}
17:20:39 [INFO] --- maven-surefire-plugin:2.19.1:test (test) @ spark-streaming-akka_2.11 ---
17:20:39 [INFO] Skipping execution of surefire because it has already been run for this configuration
17:20:39 [INFO] 
17:20:39 [INFO] --- scalatest-maven-plugin:1.0:test (test) @ spark-streaming-akka_2.11 ---
17:20:39 OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
17:20:39 Discovery starting.
17:20:39 Discovery completed in 34 milliseconds.
17:20:39 Run starting. Expected test count is: 0
17:20:39 DiscoverySuite:
17:20:39 Run completed in 95 milliseconds.
17:20:39 Total number of tests run: 0
17:20:39 Suites: completed 1, aborted 0
17:20:39 Tests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0
17:20:39 No tests were executed.
{code}","[BAHIR-89] Multi topic API support for streaming MQTT

New API which accept array of MQTT topics as input
and return Tuple2<TopicName, Message> as output.

It helps consume from multiple MQTT topics with
efficient user of resources.

Closes #37.", DELETE .gitattributes DELETE .gitignore MODIFY README.md MODIFY mqtt.py createPairedStream ADD MQTTPairedInputDStream.scala messageArrived deliveryComplete onStart connectionLost getReceiver onStop MODIFY MQTTUtils.scala createStream createStream createPairedStream createPairedStream createPairedStream createPairedStream createPairedStream createPairedStream createPairedStream MODIFY JavaMQTTStreamSuite.java testMQTTStream MODIFY MQTTStreamSuite.scala
0,BAHIR-14,fc1ef7f990880cad8ce69300bb9bbbb8ad260050,Cleanup maven pom from Spark dependencies There are a lot of dependencies that came from Spark and are not necessary for these extensions. We should cleanup the current poms and make the dependencies as lean as possible.,"[BAHIR-13] Update dependencies on spark-tags

The spark-test-tags and spark-tags were merged in
revision 8ad9f08c9 and the modules in Bahir needs
to be updated to properly use spark-tags dependency.", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-104,eae02f29eb011f50bc313714e6cde62ce65804c4,"MQTT Dstream returned by the new multi topic support API is not a pairRDD The new multi topic support API added with [BAHIR-89], when used in pyspark, does not return a Dstream of <topic,message> tuples. 
Example: 
In pyspark, when creating a Dstream using the new API ( mqttstream = MQTTUtils.createPairedStream(ssc, brokerUrl, topics) ) the expected contents of mqttstream should be a collections of tuples:

(topic,message) , (topic,message) , (topic,message) , ...

Instead, the current content is a flattened list:

topic, message, topic, message, topic, message, ...

that is hard to use.


","[BAHIR-138] fix deprecated warnings in sql-cloudant

Fix warnings in DefaultSource class, and in CloudantStreaming
and CloudantStreamingSelector examples.

How

Imported spark.implicits._ to convert Spark RDD to Dataset
Replaced deprecated json(RDD[String]) with json(Dataset[String])
Improved streaming examples:

Replaced registerTempTable with preferred createOrReplaceTempView
Replaced !isEmpty with nonEmpty
Use an accessible sales database so users can run the example without any setup
Fixed error message when stopping tests by adding logic to streaming
receiver to not store documents in Spark memory when stream has stopped

Closes #59", MODIFY CloudantStreaming.scala getInstance main MODIFY CloudantStreamingSelector.scala main MODIFY CloudantReceiver.scala receive
0,BAHIR-157,6ea42a8965d98773a342ad5cd31aab6b64e9d9bd,"The current documentation for extensions contains bad version (2.2.0.1.0.0-SNAPSHOT) On  [https://bahir.apache.org/docs/spark/current/documentation/] where {{{\{site.SPARK_VERSION}}}} seem to be replaced with the version instead of expected 2.3.0-SNAPSHOT there is 2.2.0.1.0.0-SNAPSHOT

e.g. [https://bahir.apache.org/docs/spark/current/spark-sql-cloudant/]

 ","[BAHIR-154] Refactor sql-cloudant to use cloudant-client library

- Use java-cloudant’s executeRequest for HTTP requests against
  _all_docs endpoint
- Added HTTP 429 backoff with default settings
- Simplified caught exception and message for schema size
- Replaced scala http library with okhttp library for changes receiver
- Updated streaming CloudantReceiver class to use improved
  ChangesRowScanner method
- Replaced Play JSON with GSON library
- Updated save operation to use java-cloudant bulk API
- Use _changes feed filter option for Cloudant/CouchDB 2.x and greater

Closes #61", MODIFY README.md MODIFY pom.xml MODIFY application.conf MODIFY CloudantChangesConfig.scala MODIFY CloudantConfig.scala executeRequest getRows getTotalRows getLastNum getUrl getBulkRows calculateCondition getLastNum calculate getRangeUrl getResultCount getRows getSubSetUrl allowPartition MODIFY CloudantReceiver.scala receive MODIFY DefaultSource.scala create MODIFY FilterUtil.scala filter filter apply apply MODIFY JsonStoreConfigManager.scala getInt getConfig MODIFY JsonStoreDataAccess.scala processAll getMany processIterator convert getTotalRows saveAll createDB getChanges getClRequest convert MODIFY JsonStoreRDD.scala convertToMangoJson convertAttrToMangoJson getLimitPerPartition compute MODIFY JsonUtil.scala getField toJson getField MODIFY ChangesReceiver.scala receive MODIFY CloudantOptionSuite.scala
0,BAHIR-48,9ad566815b8e2e654547d6022d20016025d49923,Documentation improvements for Bahir README.md  ,[BAHIR-44] Add new sql-streaming-mqtt to distribution profile, MODIFY pom.xml
0,BAHIR-24,61f5592e4bd334a6c4c39cae2229dce53ee66535,Fix MQTT Python code When the Bahir project was created from Spark revision {{8301fadd8}} the Python code (incl. examples) were not updated with respect to the modified project structure and test cases were left out from the import.,"[BAHIR-22] add shell script to run examples

Apache Spark has a convenience script ./bin/run-example to allow users to
quickly run the pre-packaged examples without having to compose a long(ish)
spark-submit command.
The JavaDoc of most examples refers to that ./bin/run-example script
in their description of how to run that example.

This adds a similar convenience script to the Apache Bahir project in order
to keep consistent with existing (Apache Spark) documentation and to
(at least initially) hide additional complexities of the spark-submit command.

Example:

./bin/run-example \
  org.apache.spark.examples.streaming.akka.ActorWordCount localhost 9999

...translates to this spark-submit command:

${SPARK_HOME}/bin/spark-submit \
  --packages org.apache.bahir:spark-streaming-akka_2.11:2.0.0-SNAPSHOT \
  --class org.apache.spark.examples.streaming.akka.ActorWordCount \
    streaming-akka/target/spark-streaming-akka_2.11-2.0.0-SNAPSHOT-tests.jar \
  localhost 9999

Closes #4", ADD run-example
0,BAHIR-41,c78af705f5697ab11d93f933d033d96cc48403a0,[WEBSITE] Fix logo display in small devices Reported by [~jreijn],[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
0,BAHIR-11,d32542483d08d74cfc898454ca3b4f68e3d155bc,Enable analytics on Apache Bahir website  ,[BAHIR-2] Initial maven build for Bahir spark extensions, ADD checkstyle-suppressions.xml ADD checkstyle.xml ADD pom.xml ADD scalastyle-config.xml MODIFY pom.xml MODIFY ActorReceiver.scala MODIFY pom.xml MODIFY MQTTTestUtils.scala MODIFY pom.xml MODIFY TwitterInputDStream.scala MODIFY TwitterStreamSuite.scala MODIFY pom.xml MODIFY ZeroMQReceiver.scala
0,BAHIR-44,70539a35dc9bae9ee5d380351ffc32fa6e62567e,Add new sql-streaming-mqtt to distribution  ,"[BAHIR-42] Refactor sql-streaming-mqtt example

Move JavaMQTTStreamWordCount to examples root folder
which are processed by the build as test resources
and not built into the extension itself following
the pattern used by other examples.", RENAME JavaMQTTStreamWordCount.java main
1,BAHIR-62,cc9cf1ba2097fad00f8ff173434bfdabf4e3f10c,Prepare release based on Apache Spark 2.0.1  ,[BAHIR-62] Prepare release based on Apache Spark 2.0.1, MODIFY pom.xml
1,BAHIR-49,aecd5fd9f00e40b64ebe81269396bfdc42f8ed00,Add MQTTSink to SQL Streaming MQTT.  ,"[BAHIR-49] Sink for SQL Streaming MQTT module

Closes #68", MODIFY README.md ADD JavaMQTTSinkWordCount.java main ADD MQTTSinkWordCount.scala main MODIFY org.apache.spark.sql.sources.DataSourceRegister ADD CachedMQTTClient.scala mapToSeq onRemoval deliveryComplete close connectComplete closeMqttClient getOrCreate createMqttClient messageArrived load clear connectionLost ADD MQTTStreamSink.scala commit abort abort shortName commit createRelation createStreamWriter createWriterFactory initialize MODIFY MQTTStreamSource.scala createMicroBatchReader ADD MQTTUtils.scala e parseConfigParams MODIFY LocalMessageStoreSuite.scala ADD MQTTStreamSinkSuite.scala sendToMQTT waitForMessages createContextAndDF MODIFY MQTTStreamSourceSuite.scala createStreamingDataFrame createStreamingDataframe readBackStreamingResults MODIFY MQTTTestUtils.scala deliveryComplete connectionLost connectComplete messageArrived subscribeData publishData sleepUntil
0,BAHIR-156,770b2916f0a7603b62ef997a0ea98b38c6da15c0,Improve integration test stability for flink-library-siddhi  ,"[BAHIR-104] Multi-topic MQTT DStream in Python is now a PairRDD.

Closes #55", MODIFY README.md MODIFY tests.py _start_context_with_paired_stream test_mqtt_pair_stream test_mqtt_pair_stream.retry _start_context_with_paired_stream.getOutput MODIFY mqtt.py createPairedStream _list_to_java_string_array
0,BAHIR-84,826545cb8db4b89bbdb3927e53f555c0fa15771e,"Build log flooded with test log messages The maven build log/console gets flooded with INFO messages from {{org.apache.parquet.hadoop.*}} during the {{test}} phase of module {{sql-streaming-mqtt}} . This makes it hard to find actual problems and test results especially when the log messages intersect with build and test status messages throwing off line breaks etc.

*Excerpt of build log:*

{code:title=$ mvn clean package}
...
Discovery completed in 293 milliseconds.
Run starting. Expected test count is: 7
BasicMQTTSourceSuite:
- basic usage
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
Jan 11, 2017 11:05:54 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
...
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is o- Send and receive 100 messages.
- no server up
- params not provided.
- Recovering offset from the last processed offset. !!! IGNORED !!!
StressTestMQTTSource:
- Send and receive messages of size 250MB. !!! IGNORED !!!
LocalMessageStoreSuite:
- serialize and deserialize
- Store and retreive
- Max offset stored
MQTTStreamSourceSuite:
Run completed in 20 seconds, 622 milliseconds.
Total number of tests run: 7
Suites: completed 5, aborted 0
Tests: succeeded 7, failed 0, canceled 0, ignored 2, pending 0
All tests passed.
ff
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 48
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 48
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 109B for [value] BINARY: 1 values, 34B raw, 36B comp, 1 pages, encodings: [RLE, PLAIN, BIT_PACKED]
Jan 11, 2017 11:06:03 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [timestamp] INT96: 1 values, 8B raw, 10B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries,...
{code}","[BAHIR-89] Multi topic API support for streaming MQTT

New API which accept array of MQTT topics as input
and return Tuple2<TopicName, Message> as output.

It helps consume from multiple MQTT topics with
efficient user of resources.

Closes #37.", DELETE .gitattributes DELETE .gitignore MODIFY README.md MODIFY mqtt.py createPairedStream ADD MQTTPairedInputDStream.scala messageArrived deliveryComplete onStart connectionLost getReceiver onStop MODIFY MQTTUtils.scala createStream createStream createPairedStream createPairedStream createPairedStream createPairedStream createPairedStream createPairedStream createPairedStream MODIFY JavaMQTTStreamSuite.java testMQTTStream MODIFY MQTTStreamSuite.scala
0,BAHIR-36,416252915431f01bb7a4ef4f8b7b9ed9ab02c3f5,Update readme.md with build instructions  ,"[BAHIR-35] Add Python sources to binary jar

Add python sources to jar to enable `spark-submit --packages …`

This can be verified by the following steps :

 mvn clean install

 rm -rf ~/.ivy2/cache/org.apache.bahir/

 mosquitto -p 1883

 bin/run-example \
    org.apache.spark.examples.streaming.mqtt.MQTTPublisher \
    tcp://localhost:1883 \
    foo

 ${SPARK_HOME}/bin/spark-submit \
    --packages org.apache.bahir:spark-streaming-mqtt_2.11:2.0.0-SNAPSHOT \
    streaming-mqtt/examples/src/main/python/streaming/mqtt_wordcount.py \
    tcp://localhost:1883 \
    foo

Closes #11", MODIFY pom.xml
0,BAHIR-179,a45bd84210b8e68640f97dd328e7e7053c8276e6,fail silently when tests need docker image to be running  ,"[BAHIR-65] Twitter integration test

Closes #80", MODIFY README.md MODIFY TwitterStreamSuite.scala shouldRunTest
0,BAHIR-182,fb752570c7ac817b414c738e05b751dd5864feb6,Create PubNub extension for Apache Spark Streaming Implement new connector for PubNub ([https://www.pubnub.com/)] which is increasing in popularity cloud messaging infrastructure.,"[BAHIR-66] Switch to Java binding for ZeroMQ

Initially, I just wanted to implement integration test for BAHIR-66.
Google pointed me to JeroMQ, which provides official ZeroMQ binding
for Java and does not require native libraries. I have decided to give
it a try, but quickly realized that akka-zeromq module (transient
dependency from current Bahir master) is not compatible with JeroMQ.
Actually Akka team also wanted to move to JeroMQ (akka/akka#13856),
but in the end decided to remove akka-zeromq project completely
(akka/akka#15864, https://www.lightbend.com/blog/akka-roadmap-update-2014).

Having in mind that akka-zeromq does not support latest version of ZeroMQ
protocol and further development may come delayed, I have decided to refactor
streaming-zeromq implementation and leverage JeroMQ. With the change we receive
various benefits, such as support for PUB-SUB and PUSH-PULL messaging patterns
and the ability to bind the socket on whatever end of communication channel
(see test cases), subscription to multiple channels, etc. JeroMQ seems pretty
reliable and reconnection is handled out-of-the-box. Actually, we could even
start the ZeroMQ subscriber trying to connect to remote socket before other
end created and bound the socket. While I tried to preserve backward compatibility
of method signatures, there was no easy way to support Akka API and business
logic that users could put there (e.g. akka.actor.ActorSystem).

Closes #71", MODIFY README.md MODIFY ZeroMQWordCount.scala run bytesToStringIterator stringToByteString main MODIFY pom.xml ADD ZeroMQInputDStream.scala run onStop subscribe getReceiver receiveLoop onStart DELETE ZeroMQReceiver.scala preStart MODIFY ZeroMQUtils.scala createTextStream createTextJavaStream MODIFY LocalJavaStreamingContext.java setUp MODIFY JavaZeroMQStreamSuite.java testZeroMQStream call call testZeroMQAPICompatibility MODIFY log4j.properties MODIFY ZeroMQStreamSuite.scala checkAllReceived
0,BAHIR-16,2dfcd08d11e94b535a39c31c87cf690f99944357,"Build issues due to log4j properties not found log4j:ERROR Could not read configuration file from URL [file:src/test/resources/log4j.properties].
java.io.FileNotFoundException: src/test/resources/log4j.properties (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:273)
	at org.apache.hadoop.util.ShutdownHookManager.<clinit>(ShutdownHookManager.java:44)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:179)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:152)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:57)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.streaming.StreamingContext$.<init>(StreamingContext.scala:730)
	at org.apache.spark.streaming.StreamingContext$.<clinit>(StreamingContext.scala)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:100)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:63)
	at org.apache.spark.streaming.akka.JavaAkkaUtilsSuite.testAkkaUtils(JavaAkkaUtilsSuite.java:36)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
log4j:ERROR Ignoring configuration file [file:src/test/resources/log4j.properties].","[[BAHIR-14] More parent pom cleanup

Remove Spark assembly related configuration, and
stop producing source jars for non-jar projects.", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-123,c7f158d86634d602a19a4abfd873809f8ece9d03,"Fix errors to support the latest version of Play JSON library for sql-cloudant The latest version is 2.6.2.  Error during mvn install -pl sql-cloudant after updating play-json to 2.6.2 in sql-cloudant/pom.xml:

[ERROR] /Users/estebanmlaver/emlaver-bahir/sql-cloudant/src/main/scala/org/apache/bahir/cloudant/common/JsonStoreConfigManager.scala:19: object typesafe is not a member of package com
[ERROR] import com.typesafe.config.ConfigFactory
[ERROR]            ^
[ERROR] /Users/estebanmlaver/emlaver-bahir/sql-cloudant/src/main/scala/org/apache/bahir/cloudant/common/JsonStoreConfigManager.scala:52: not found: value ConfigFactory
[ERROR]   private val configFactory = ConfigFactory.load()
[ERROR]                               ^
[ERROR] two errors found

Maven compile dependencies need to be added to pom.xml that existed in play-json 2.5.9 but were removed in 2.6.2.

Additional info. from Patrick Titzler between play-json versions 2.5.x and 2.6.x:
Looks like the parameter data type has been changed from `Seq[JsValue]` (https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.libs.json.JsArray) to `IndexedSeq[JsValue]` https://playframework.com/documentation/2.6.x/api/scala/index.html#play.api.libs.json.JsArray","[BAHIR-110] Implement _changes API for sql-cloudant

 - support loading Cloudant data into Spark DataFrames and SQL tables
   using '_changes' endpoint
 - update README to explain the new config options and differences
   between '_all_docs' and '_changes' endpoints when loading data
 - Add test suite to test Spark DataFrames using the '_all_docs' and
   '_changes' endpoint, assert Cloudant config options, and test Spark
   SQL temporary views

Closes #45", MODIFY pom.xml MODIFY README.md MODIFY CloudantApp.scala main MODIFY CloudantDF.scala main MODIFY CloudantDFOption.scala MODIFY pom.xml MODIFY application.conf ADD CloudantChangesConfig.scala MODIFY CloudantConfig.scala getTotalUrl getBulkRows getContinuousChangesUrl calculateCondition getCreateDBonSave getSchemaSampleSize getSelector calculate getRangeUrl getAllDocsUrl getSubSetUrl allowPartition getBulkPostUrl queryEnabled getDbUrl getDbname getConflictErrStr getTotalRows getForbiddenErrStr getRows getLastNum getUrl MODIFY CloudantReceiver.scala receive onStop MODIFY DefaultSource.scala buildScan create insert ADD CloudantException.scala this this this MODIFY FilterUtil.scala filter MODIFY JsonStoreConfigManager.scala getString getStorageLevel getInt getLong getConfig getBool MODIFY JsonStoreDataAccess.scala saveAll getClPostRequest createDB processAll getMany getClRequest convert MODIFY JsonStoreRDD.scala compute getTotalPartition MODIFY JsonUtil.scala ADD ChangesReceiver.scala receive run onStart onStop ADD n_airportcodemapping.json ADD n_booking.json ADD n_customer.json ADD n_customersession.json ADD n_flight.json ADD n_flightsegment.json ADD log4j.properties ADD ClientSparkFunSuite.scala deleteTestDb runIfTestsEnabled setupClient testIfEnabled createTestDbs afterAll beforeAll teardownClient deleteTestDbs ADD CloudantAllDocsDFSuite.scala beforeAll ADD CloudantChangesDFSuite.scala ADD CloudantOptionSuite.scala ADD CloudantSparkSQLSuite.scala beforeAll ADD TestUtils.scala deleteRecursively
1,BAHIR-20,76bfd8b2509128ae2a4ed7f59bcf4dac75c0296a,"Create release script Create script to help with release process that perform:
release-prepare
release-perform
release-snapshot
","[BAHIR-20] Create release helper scripts

Release script to automate :
- Preparing release artifacts
- Publishing maven artifacts in Scala 2.10 and 2.11
- Publishing snapshots", ADD release-build.sh
1,BAHIR-213,d036820c0efa1b2e9b8021506164b67582352dff,"Faster S3 file Source for Structured Streaming with SQS Using FileStreamSource to read files from a S3 bucket has problems both in terms of costs and latency:
 * *Latency:* Listing all the files in S3 buckets every microbatch can be both slow and resource intensive.
 * *Costs:* Making List API requests to S3 every microbatch can be costly.

The solution is to use Amazon Simple Queue Service (SQS) which lets you find new files written to S3 bucket without the need to list all the files every microbatch.

S3 buckets can be configured to send notification to an Amazon SQS Queue on Object Create / Object Delete events. For details see AWS documentation here [Configuring S3 Event Notifications|https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html] 

Spark can leverage this to find new files written to S3 bucket by reading notifications from SQS queue instead of listing files every microbatch.

I hope to contribute changes proposed in [this pull request|https://github.com/apache/spark/pull/24934] to Apache Bahir as suggested by [gaborgsomogyi|https://github.com/gaborgsomogyi]  [here|https://github.com/apache/spark/pull/24934#issuecomment-511389130]","[BAHIR-213] Faster S3 file Source for Structured Streaming with SQS (#91)

Using FileStreamSource to read files from a S3 bucket has problems 
both in terms of costs and latency:

Latency: Listing all the files in S3 buckets every micro-batch can be both
slow and resource-intensive.

Costs: Making List API requests to S3 every micro-batch can be costly.

The solution is to use Amazon Simple Queue Service (SQS) which lets 
you find new files written to S3 bucket without the need to list all the 
files every micro-batch.

S3 buckets can be configured to send a notification to an Amazon SQS Queue
on Object Create / Object Delete events. For details see AWS documentation
here Configuring S3 Event Notifications

Spark can leverage this to find new files written to S3 bucket by reading 
notifications from SQS queue instead of listing files every micro-batch.

This PR adds a new SQSSource which uses Amazon SQS queue to find 
new files every micro-batch.

Usage

val inputDf = spark .readStream
   .format(""s3-sqs"")
   .schema(schema)
   .option(""fileFormat"", ""json"")
   .option(""sqsUrl"", ""https://QUEUE_URL"")
   .option(""region"", ""us-east-1"")
   .load()", MODIFY pom.xml ADD README.md ADD SqsSourceExample.scala main ADD pom.xml ADD BasicAWSCredentialsProvider.java refresh BasicAWSCredentialsProvider getCredentials toString ADD InstanceProfileCredentialsProviderWithRetries.java getCredentials ADD org.apache.spark.sql.sources.DataSourceRegister ADD log4j.properties ADD SqsClient.scala deleteMessagesFromQueue run createSqsClient evaluateRetries assertSqsIsWorking parseSqsMessages convertTimestampToMills sqsFetchMessages addToDeleteMessageQueue ADD SqsFileCache.scala filterAllUncommittedFiles getUncommittedFiles add purge markCommitted isNewFile filterTopUncommittedFiles stripPathIfNecessary ADD SqsSource.scala fetchMaxOffset stop commit getBatch ADD SqsSourceOptions.scala this withBooleanParameter ADD SqsSourceProvider.scala sourceSchema createSource shortName ADD log4j.properties ADD SqsSourceOptionsSuite.scala testBadOptions testMissingMandatoryOptions
0,BAHIR-42,29d8c7622cf9663e295d7616ae1e1b089fe80da9,Refactor sql-streaming-mqtt examples to follow other projects pattern  ,[BAHIR-31] Add basic documentation for ZeroMQ connector, ADD README.md
0,BAHIR-156,0e1505a8960bfe40ea025267bbf36ec5c4cf5c79,Improve integration test stability for flink-library-siddhi  ,"[BAHIR-128] Improve sql-cloudant _changes receiver

This change improves the stability of _changes receiver and
fix the intermitent failing test in sql-cloudant's
CloudantChangesDFSuite.

How

Improve performance and decrease testing time by setting batch size
to 8 seconds and using seq_interval _changes feed option.
Use getResource to load json files path
Added Mike Rhodes's ChangesRowScanner for reading each _changes line
and transforming to GSON's JSON object
Added Mike Rhodes's ChangesRow representing a row in the changes feed

Closes #57", ADD ChangesRow.java getDoc Rev getRev getSeq getId getChanges ADD ChangesRowScanner.java readRowFromReader MODIFY CloudantChangesConfig.scala MODIFY DefaultSource.scala create MODIFY JsonStoreDataAccess.scala MODIFY ChangesReceiver.scala receive MODIFY ClientSparkFunSuite.scala createTestDbs MODIFY CloudantChangesDFSuite.scala
0,BAHIR-166,b3902bac67edc2134bcc2c755fadc5c60c8ae01c,Migrate akka sql streaming source to datasrouce v2 API  ,"[BAHIR-164][BAHIR-165] Port Mqtt sql source to datasource v2 API

Migrating Mqtt spark structured streaming connector to DatasourceV2 API.

Closes #65", MODIFY pom.xml MODIFY AkkaStreamSourceSuite.scala MODIFY README.md MODIFY JavaMQTTStreamWordCount.java main MODIFY MQTTStreamWordCount.scala main ADD LongOffset.scala + apply - convert MODIFY MQTTStreamSource.scala e sourceSchema getEndOffset initialize readSchema createDataReaderFactories next setOffsetRange createMicroBatchReader fetchLastProcessedOffset createDataReader toString commit stop messageArrived getStartOffset deserializeOffset createSource get getBatch close MODIFY MessageStore.scala this get get this getInstance ADD test-BAHIR-83.sh MODIFY LocalMessageStoreSuite.scala MODIFY MQTTStreamSourceSuite.scala readBackStreamingResults writeStreamResults createStreamingDataframe createStreamingDataframe writeStreamResults MODIFY MQTTTestUtils.scala publishData teardown
0,BAHIR-40,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
0,BAHIR-41,619936d39d18b7af45b7acec9af02b599a43b056,[WEBSITE] Fix logo display in small devices Reported by [~jreijn],[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
0,BAHIR-47,1abeab29c8a5e884f4603ef12abd85971a9105b0,Bring up release download for Apache Bahir website  ,[BAHIR-42] Refactor sql-streaming-mqtt scala example, RENAME MQTTStreamWordCount.scala
0,BAHIR-16,7bc3d6e91ce8bf2b159032cf781a09e90854a19f,"Build issues due to log4j properties not found log4j:ERROR Could not read configuration file from URL [file:src/test/resources/log4j.properties].
java.io.FileNotFoundException: src/test/resources/log4j.properties (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:273)
	at org.apache.hadoop.util.ShutdownHookManager.<clinit>(ShutdownHookManager.java:44)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:179)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:152)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:57)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.streaming.StreamingContext$.<init>(StreamingContext.scala:730)
	at org.apache.spark.streaming.StreamingContext$.<clinit>(StreamingContext.scala)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:100)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:63)
	at org.apache.spark.streaming.akka.JavaAkkaUtilsSuite.testAkkaUtils(JavaAkkaUtilsSuite.java:36)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
log4j:ERROR Ignoring configuration file [file:src/test/resources/log4j.properties].","[BAHIR-14] Cleanup Bahir parent pom

The Bahir parent pom was initially based on Spark parent pom
and was bringing a lot of unecessary dependencies. This commit
cleans most of the unused properties, dependencies, etc.

Closes #1", MODIFY pom.xml MODIFY ActorWordCount.scala MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-27,29d8c7622cf9663e295d7616ae1e1b089fe80da9,Add documentation for existing streaming connectors  ,[BAHIR-31] Add basic documentation for ZeroMQ connector, ADD README.md
0,BAHIR-40,95633de6741ddf757cc4964425463a972e1b4cbe,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],[BAHIR-43] Add Apache License header file, MODIFY pom.xml MODIFY org.apache.spark.sql.sources.DataSourceRegister
1,BAHIR-97,889de659c33dd56bad7193a4b69e6d05d061a2fd,"Akka as a streaming source for SQL Streaming. Hello,

This issue is created to propose the addition of Akka compatible streaming source for Spark SQL Streaming.","[BAHIR-97] Akka as SQL Streaming datasource.

Closes #38.", MODIFY pom.xml ADD README.md ADD JavaAkkaStreamWordCount.java main ADD AkkaStreamWordCount.scala main ADD pom.xml ADD assembly.xml ADD AkkaStreamSource.scala store stop store store preStart fetchLastProcessedOffset createSource postStop getBatch e shortName close sourceSchema receive getOrCreatePersistenceInstance ADD MessageStore.scala get this ADD BahirUtils.scala visitFile postVisitDirectory recursiveDeleteDir ADD Logging.scala ADD feeder_actor.conf ADD log4j.properties ADD AkkaStreamSourceSuite.scala readBackSreamingResults writeStreamResults createStreamingDataframe ADD AkkaTestUtils.scala setup setCountOfMessages run getFeederActorUri getFeederActorConfig setMessage shutdown
1,BAHIR-137,ebdc8b257d32ff64a88657cc3e7dc838564a1d01,"Load performance improvements for _changes API in sql-cloudant Items for improving _changes feed load:
- Make Spark streaming batch interval visible to the user for tuning based on type/size of document and number of docs in database
- Merge BAHIR-128: Improve stability of _changes receiver
- Merge BAHIR-154: refactor sql-cloudant to use java-cloudant library","[BAHIR-137] CouchDB/Cloudant _changes feed receiver improvements

Adds batchInterval option for tuning _changes receiver’s streaming batch interval
Throw a CloudantException if the final schema for the _changes receiver is empty
Call stop method in streaming receiver when there’s an error

Closes #60", MODIFY README.md MODIFY application.conf MODIFY CloudantChangesConfig.scala MODIFY DefaultSource.scala create MODIFY JsonStoreConfigManager.scala getConfig MODIFY ChangesReceiver.scala receive MODIFY CloudantOptionSuite.scala
0,BAHIR-44,29d8c7622cf9663e295d7616ae1e1b089fe80da9,Add new sql-streaming-mqtt to distribution  ,[BAHIR-31] Add basic documentation for ZeroMQ connector, ADD README.md
0,BAHIR-48,1abeab29c8a5e884f4603ef12abd85971a9105b0,Documentation improvements for Bahir README.md  ,[BAHIR-42] Refactor sql-streaming-mqtt scala example, RENAME MQTTStreamWordCount.scala
1,BAHIR-22,61f5592e4bd334a6c4c39cae2229dce53ee66535,"Add script to run examples Apache Spark has a convenience script {{./bin/run-example}} to allow users to quickly run the pre-packaged examples without having to compose a long(ish) spark-submit command. The JavaDoc of most examples refers to that  {{./bin/run-example}} script in their description of how to run that example.

The Apache Bahir project should have a similar convenience script to be consistent with Apache Spark, existing documentation and to (at least initially) hide additional complexities of the spark-submit command.

Example:
{code}
./bin/run-example \
  org.apache.spark.examples.streaming.akka.ActorWordCount localhost 9999
{code}
...translates to this {{spark-submit}} command:
{code}
${SPARK_HOME}/bin/spark-submit \
  --packages org.apache.bahir:spark-streaming-akka_2.11:2.0.0-SNAPSHOT \
  --class org.apache.spark.examples.streaming.akka.ActorWordCount \
    streaming-akka/target/spark-streaming-akka_2.11-2.0.0-SNAPSHOT-tests.jar \
  localhost 9999
{code}","[BAHIR-22] add shell script to run examples

Apache Spark has a convenience script ./bin/run-example to allow users to
quickly run the pre-packaged examples without having to compose a long(ish)
spark-submit command.
The JavaDoc of most examples refers to that ./bin/run-example script
in their description of how to run that example.

This adds a similar convenience script to the Apache Bahir project in order
to keep consistent with existing (Apache Spark) documentation and to
(at least initially) hide additional complexities of the spark-submit command.

Example:

./bin/run-example \
  org.apache.spark.examples.streaming.akka.ActorWordCount localhost 9999

...translates to this spark-submit command:

${SPARK_HOME}/bin/spark-submit \
  --packages org.apache.bahir:spark-streaming-akka_2.11:2.0.0-SNAPSHOT \
  --class org.apache.spark.examples.streaming.akka.ActorWordCount \
    streaming-akka/target/spark-streaming-akka_2.11-2.0.0-SNAPSHOT-tests.jar \
  localhost 9999

Closes #4", ADD run-example
0,BAHIR-53,a351549cf634adf5249862599166ef9ed9073725,"Add additional MQTT options/parameters to MQTTInputDStream I propose that the MQTTInputDStream be extended to optionally allow setting the following mqtt options / parameters (like MqttStreamSource in BAHIR-51):
* username
* password
* clientId
* cleanSession
* QoS
* connectionTimeout
* keepAliveInterval
* mqttVersion","[BAHIR-51] Add new configuration options to MqttStreamSource.

Add new configuration options to enable secured connections and
other quality of services.

Closes #22", MODIFY README.md MODIFY MQTTStreamSource.scala initialize createSource MODIFY MQTTStreamSourceSuite.scala createStreamingDataframe
0,BAHIR-30,619936d39d18b7af45b7acec9af02b599a43b056,Add documentation for streaming-twitter connector  ,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
1,BAHIR-37,91e82f42bac58fd8d912e892a5ebfca79f6b8268,Prepare release based on Apache Spark 2.0.0  ,[BAHIR-37] Update Spark to release 2.0.0, MODIFY pom.xml
1,BAHIR-23,d6770f833ec723f24ea452c11fd102624d12c3f2,"Build should fail on Checkstyle violation Currently the maven build will fail for code style violations in Scala files but the build will succeed regardless of code style violations in Java files.

{code:xml}
      <plugin>
        ...
        <artifactId>scalastyle-maven-plugin</artifactId>
        ...
        <configuration>
          ...
          <failOnViolation>true</failOnViolation>
          ...
        </configuration>
        ...
      </plugin>
      <plugin>
        ...
        <artifactId>maven-checkstyle-plugin</artifactId>
        ...
        <configuration>
          ...
          <failOnViolation>false</failOnViolation>
          ...
        </configuration>
        ...
      </plugin>
{code}

As a consequence potential problems in the Java code may not get noticed.","[BAHIR-23] Build should fail on Checkstyle violations

Currently the maven build is configured to:

- fail for code style violations in Scala files
- succeed despite code style violations in Java files
- exclude Scala test sources (and examples) from code style checks
- include Java test sources (and examples) in code style checks

This changes the maven build configuration to

- fail for code style violations in both Scala and Java sources
- include test sources (and examples) in style checks for both
  Scala and Java sources

Additionally cleaning up unsupported checkstyle configuration
elements (apparently copy-and-pasted from scalastyle configuration)", MODIFY checkstyle-suppressions.xml MODIFY pom.xml MODIFY package-info.java MODIFY JavaTwitterHashTagJoinSentiments.java main MODIFY package-info.java MODIFY package-info.java MODIFY JavaZeroMQStreamSuite.java testZeroMQStream
0,BAHIR-49,31b3e96d4680c54ff05d4b09f944106d22b62760,Add MQTTSink to SQL Streaming MQTT.  ,[BAHIR-52] Fix code paragraph formatting for Akka readme, MODIFY README.md
0,BAHIR-47,c98dd0feefa79c70be65de06a411a2f9c4fc42dc,Bring up release download for Apache Bahir website  ,"[BAHIR-39] Add SQL Streaming MQTT support

This provides support for using MQTT sources for
the new Spark Structured Streaming. This uses
MQTT client persistence layer to provide minimal
fault tolerance.

Closes #13", MODIFY pom.xml ADD README.md ADD pom.xml ADD assembly.xml ADD JavaMQTTStreamWordCount.java main ADD org.apache.spark.sql.sources.DataSourceRegister ADD MQTTStreamSource.scala stop connectionLost messageArrived fetchLastProcessedOffset createSource initialize deliveryComplete getBatch shortName sourceSchema connectComplete e ADD MessageStore.scala get this ADD MQTTStreamWordCount.scala main ADD BahirUtils.scala postVisitDirectory visitFile recursiveDeleteDir ADD Logging.scala ADD log4j.properties ADD LocalMessageStoreSuite.scala ADD MQTTStreamSourceSuite.scala createStreamingDataframe writeStreamResults readBackStreamingResults ADD MQTTTestUtils.scala setup publishData teardown findFreePort
0,BAHIR-5,d32542483d08d74cfc898454ca3b4f68e3d155bc,"Document guidelines for proposing new extensions for Bahir Project Document the guidelines for contributing new extensions, as well as describe the life cycle of an extension (e.g. what happens if it becomes stale, etc)",[BAHIR-2] Initial maven build for Bahir spark extensions, ADD checkstyle-suppressions.xml ADD checkstyle.xml ADD pom.xml ADD scalastyle-config.xml MODIFY pom.xml MODIFY ActorReceiver.scala MODIFY pom.xml MODIFY MQTTTestUtils.scala MODIFY pom.xml MODIFY TwitterInputDStream.scala MODIFY TwitterStreamSuite.scala MODIFY pom.xml MODIFY ZeroMQReceiver.scala
1,BAHIR-110,c7f158d86634d602a19a4abfd873809f8ece9d03,"Implement _changes API for non-streaming receiver Today we use the _changes API for Spark streaming receiver and _all_docs API for non-streaming receiver. _all_docs API supports parallel reads (using offset and range) but performance of _changes API is still better in most cases (even with single threaded support).

With this ticket we want to:
a) implement _changes API for non-streaming receivers
b) allow customers to pick either _all_docs (default) or _changes API endpoint, with documentation about pros and cons

_changes performance details:
Successfully loaded Cloudant (using local cloudant-developer docker image) docs into Spark (local standalone) with the following database sizes: 15GB (time: 8 1/2 mins), 20GB (17 mins), 46GB (25 mins), and 75GB (48 1/2 mins).","[BAHIR-110] Implement _changes API for sql-cloudant

 - support loading Cloudant data into Spark DataFrames and SQL tables
   using '_changes' endpoint
 - update README to explain the new config options and differences
   between '_all_docs' and '_changes' endpoints when loading data
 - Add test suite to test Spark DataFrames using the '_all_docs' and
   '_changes' endpoint, assert Cloudant config options, and test Spark
   SQL temporary views

Closes #45", MODIFY pom.xml MODIFY README.md MODIFY CloudantApp.scala main MODIFY CloudantDF.scala main MODIFY CloudantDFOption.scala MODIFY pom.xml MODIFY application.conf ADD CloudantChangesConfig.scala MODIFY CloudantConfig.scala getTotalUrl getBulkRows getContinuousChangesUrl calculateCondition getCreateDBonSave getSchemaSampleSize getSelector calculate getRangeUrl getAllDocsUrl getSubSetUrl allowPartition getBulkPostUrl queryEnabled getDbUrl getDbname getConflictErrStr getTotalRows getForbiddenErrStr getRows getLastNum getUrl MODIFY CloudantReceiver.scala receive onStop MODIFY DefaultSource.scala buildScan create insert ADD CloudantException.scala this this this MODIFY FilterUtil.scala filter MODIFY JsonStoreConfigManager.scala getString getStorageLevel getInt getLong getConfig getBool MODIFY JsonStoreDataAccess.scala saveAll getClPostRequest createDB processAll getMany getClRequest convert MODIFY JsonStoreRDD.scala compute getTotalPartition MODIFY JsonUtil.scala ADD ChangesReceiver.scala receive run onStart onStop ADD n_airportcodemapping.json ADD n_booking.json ADD n_customer.json ADD n_customersession.json ADD n_flight.json ADD n_flightsegment.json ADD log4j.properties ADD ClientSparkFunSuite.scala deleteTestDb runIfTestsEnabled setupClient testIfEnabled createTestDbs afterAll beforeAll teardownClient deleteTestDbs ADD CloudantAllDocsDFSuite.scala beforeAll ADD CloudantChangesDFSuite.scala ADD CloudantOptionSuite.scala ADD CloudantSparkSQLSuite.scala beforeAll ADD TestUtils.scala deleteRecursively
1,BAHIR-217,549c50be02f98b93c5f79890332e8de97332e8f5,"Install of Oracle JDK 8 Failing in Travis CI Install of Oracle JDK 8 Failing in Travis CI. As a result, build is failing for new pull requests.

We need to make a small fix in _ __ .travis.yml_ file as mentioned in the issue here:
https://travis-ci.community/t/install-of-oracle-jdk-8-failing/3038
We just need to add 
{code:java}
dist: trusty{code}
in the .travis.yml file as mentioned in the issue above.

I can raise a PR for this fix if required.","[BAHIR-217] Installation of Oracle JDK8 is Failing in Travis CI (#93)

Install of Oracle JDK 8 Failing in Travis CI and as a result, 
build is failing for new pull requests.

We just need to add `dist: trusty` in the .travis.yml file 
as mentioned in the issue below:
https://travis-ci.community/t/install-of-oracle-jdk-8-failing/3038", MODIFY .travis.yml
0,BAHIR-21,335f605e75d426f62052378920880ab22729083e,Create script to change build between scala 2.10 and 2.11  ,[BAHIR-17] Update Apache Spark version back to 2.0.0-SNAPSHOT, MODIFY pom.xml
0,BAHIR-43,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,Add missing apache license header to sql-mqtt file  ,[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
1,BAHIR-222,4f22586d527d1beb0225deb6b8ff7d75e07573e7,Update Readme with details of SQL Streaming SQS connector Adding link to SQL Streaming SQS connector in BAHIR Readme.,[BAHIR-222] Add SQL Streaming SQS connector to Readme (#96), MODIFY README.md
0,BAHIR-52,415576ba702206ba9cfc5c8bdbdee4869a1e52ac,"Update extension documentation formats for code sections The ```md format is not working properly for pure jekyll html generation, and the tab seems to be the supported way in vanilla jekyll. We should update Bahir extension readme to use the supported format.","[BAHIR-61] Enable publishing release artifacts from a tag

Enable a --gitTag parameter to identify an RC tag to be used
when publishing artifacts to maven.", MODIFY release-build.sh
0,BAHIR-40,1abeab29c8a5e884f4603ef12abd85971a9105b0,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],[BAHIR-42] Refactor sql-streaming-mqtt scala example, RENAME MQTTStreamWordCount.scala
0,BAHIR-11,fc1ef7f990880cad8ce69300bb9bbbb8ad260050,Enable analytics on Apache Bahir website  ,"[BAHIR-13] Update dependencies on spark-tags

The spark-test-tags and spark-tags were merged in
revision 8ad9f08c9 and the modules in Bahir needs
to be updated to properly use spark-tags dependency.", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-51,c317def2a7575713d31353f87025ddacaf30e503,"Add additional MQTT options/parameters to MQTTInputDStream and MqttStreamSource We are using Spark Streaming in the automotive IOT environment with MQTT as the data source.
For security reasons our MQTT broker is protected by username and password (as is default for these kind of environments). At the moment it is not possible to set username/password when creating an MQTT Receiver (MQTTInputDStream or MqttStreamSource).

I propose that the MQTTInputDStream and MqttStreamSource be extended to optionally allow setting the following mqtt options / parameters:
* username
* password
* clientId
* cleanSession
* QoS
* connectionTimeout
* keepAliveInterval
* mqttVersion

If this proposal meets your approval I am willing to create a pull request with these changes implemented.


*Note*: The part for MqttInputDStream has been split off into BAHIR-53.",[BAHIR-37] Start building against Spark Master -  2.1.0-SNAPSHOT, MODIFY pom.xml MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
1,BAHIR-89,826545cb8db4b89bbdb3927e53f555c0fa15771e,"New API for subscribing from a list of MQTT topics and Return tuple of <Topic,Message> as output I am working in IoT Project. As part of MQTT-Kafka bridge program development I used Bahir. I feel that it will be a good feature to prove a new API to support a list of MQTT topic as input and output as a tuple of <Topic, Message>. This will be useful to reduce resource usage in case of multiple topic subscription. I had developed this feature a like to integrate","[BAHIR-89] Multi topic API support for streaming MQTT

New API which accept array of MQTT topics as input
and return Tuple2<TopicName, Message> as output.

It helps consume from multiple MQTT topics with
efficient user of resources.

Closes #37.", DELETE .gitattributes DELETE .gitignore MODIFY README.md MODIFY mqtt.py createPairedStream ADD MQTTPairedInputDStream.scala messageArrived deliveryComplete onStart connectionLost getReceiver onStop MODIFY MQTTUtils.scala createStream createStream createPairedStream createPairedStream createPairedStream createPairedStream createPairedStream createPairedStream createPairedStream MODIFY JavaMQTTStreamSuite.java testMQTTStream MODIFY MQTTStreamSuite.scala
1,BAHIR-172,68ac1be22ddccecf105aee355a8c2652868e9f7d,"Avoid FileInputStream/FileOutputStream They rely on finalizers (before Java 11), which create unnecessary GC load.


The alternatives, {{Files.newInputStream}}, are as easy to use and don't have this issue.",[BAHIR-172 ] Replace FileInputStream with Files.newInputStream (#92), MODIFY MQTTTestUtils.scala setup MODIFY SparkGCPCredentials.scala
0,BAHIR-27,619936d39d18b7af45b7acec9af02b599a43b056,Add documentation for existing streaming connectors  ,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
0,BAHIR-44,31b3e96d4680c54ff05d4b09f944106d22b62760,Add new sql-streaming-mqtt to distribution  ,[BAHIR-52] Fix code paragraph formatting for Akka readme, MODIFY README.md
0,BAHIR-18,1028736bb79a6b7c66789bc77ca97c44020e62af,"Include examples in Maven test build We need to find a way to include the examples in the Maven build but keep them excluded from the binary jar(s) and have IDEs like IntelliJ or Eclipse recognize the {{<module>/examples/src/\[java|scala\]}} as source files.

One way this can be achieved is by including the examples as ""additional test sources"".","[BAHIR-19] Update source distribution assembly name

Update final assembly name and extraction directory
to use apache best practice pattern :

apache-bahir-${project.version}-src", MODIFY pom.xml MODIFY src.xml
0,BAHIR-35,91e82f42bac58fd8d912e892a5ebfca79f6b8268,"Include Python code in the binary jars for use with ""--packages ..."" Currently, to make use the PySpark code (i.e streaming-mqtt/python) a user will have to download the jar from Maven central or clone the code from GitHub and then have to find individual *.py files, create a zip and add that to the {{spark-submit}} command with the {{--py-files}} option, or, add them to the {{PYTHONPATH}} when running locally.

If we include the Python code in the binary build (to the jar that gets uploaded to Maven central), then users need not do any acrobatics besides using the {{--packages ...}} option.

An example where the Python code is part of the binary jar is the [GraphFrames|https://spark-packages.org/package/graphframes/graphframes] package.",[BAHIR-37] Update Spark to release 2.0.0, MODIFY pom.xml
0,BAHIR-48,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,Documentation improvements for Bahir README.md  ,[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
0,BAHIR-88,e3d9e6960941696ba073735e9d039c85146c217a,"Source distribution contains release build artifacts There are 'releaseBackup' files and 'release.properties' in the 'source distribution' which are artifacts of the release build process.
We appear to be missing to run the mvn {{release:clean}} target _after_
the {{release:prepare}} target. We should fix that for the next release. All prior
Apache Bahir source distributions contained these release build artifacts.

[Mailing list post|https://www.mail-archive.com/dev@bahir.apache.org/msg00735.html]

Fixed by Luciano on June 7 with commits: [ba68b35|https://github.com/apache/bahir/commit/ba68b3587ad4011a093bcaad921035f26907967c], [6d9a4d7|https://github.com/apache/bahir/commit/6d9a4d7ab0c1eff0bf63e91cec32b601c263f790], [dcb4bbd|https://github.com/apache/bahir/commit/dcb4bbd2e4d75bc0872ce32c159b03a1d0f90047]","[BAHIR-100] Enhance MQTT connector to support byte arrays

Closes #47", MODIFY README.md MODIFY MQTTInputDStream.scala ADD MQTTPairedByteArrayInputDStream.scala onStart connectionLost getReceiver onStop messageArrived deliveryComplete MODIFY MQTTPairedInputDStream.scala MODIFY MQTTUtils.scala createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedStream createPairedStream createPairedStream MODIFY JavaMQTTStreamSuite.java testMQTTStream MODIFY MQTTStreamSuite.scala
0,BAHIR-69,cc9cf1ba2097fad00f8ff173434bfdabf4e3f10c,Release script fails intermitently when publishing to staging maven repo  ,[BAHIR-62] Prepare release based on Apache Spark 2.0.1, MODIFY pom.xml
0,BAHIR-97,f0d9a84f76cb34a432e1d2db053d2471a8ab2ba4,"Akka as a streaming source for SQL Streaming. Hello,

This issue is created to propose the addition of Akka compatible streaming source for Spark SQL Streaming.","[BAHIR-101] Spark SQL datasource for CounchDB/Cloudant

Initial code supporting CounchDB/Cloudant as an Spark SQL
data source. The initial source contains the core connector,
examples, and basic documentation on the README.

Closes #39.", MODIFY README.md MODIFY pom.xml ADD README.md ADD CloudantApp.py ADD CloudantDF.py ADD CloudantDFOption.py ADD CloudantApp.scala main ADD CloudantDF.scala main ADD CloudantDFOption.scala main ADD CloudantStreaming.scala getInstance main ADD CloudantStreamingSelector.scala main ADD pom.xml ADD application.conf ADD reference.conf ADD CloudantConfig.scala getRangeUrl getDbname getTotalRows getForbiddenErrStr getConflictErrStr getOneUrlExcludeDDoc1 getSubSetUrl allowPartition getLastNum getTotalUrl getBulkRows calculateCondition getCreateDBonSave getLastUrl getContinuousChangesUrl getChangesUrl calculate getAllDocsUrlExcludeDDoc getOneUrl getSelector getAllDocsUrl getSchemaSampleSize getRows getBulkPostUrl getDbUrl getOneUrlExcludeDDoc2 ADD CloudantReceiver.scala onStart onStop receive run ADD DefaultSource.scala buildScan createRelation create insert createRelation createRelation ADD FilterUtil.scala filter containsFiltersFor apply analyze evaluate getFiltersForPostProcess getFilterAttribute getInfo ADD JsonStoreConfigManager.scala getString getConfig getConfig getBool getLong getInt ADD JsonStoreDataAccess.scala getChanges convert convertSkip getOne getClPostRequest saveAll getTotalRows createDB processAll getMany getIterator processIterator ADD JsonStoreRDD.scala compute ADD JsonUtil.scala getField
0,BAHIR-57,a351549cf634adf5249862599166ef9ed9073725,Add Flume sink from Flink to Bahir  ,"[BAHIR-51] Add new configuration options to MqttStreamSource.

Add new configuration options to enable secured connections and
other quality of services.

Closes #22", MODIFY README.md MODIFY MQTTStreamSource.scala initialize createSource MODIFY MQTTStreamSourceSuite.scala createStreamingDataframe
0,BAHIR-49,c317def2a7575713d31353f87025ddacaf30e503,Add MQTTSink to SQL Streaming MQTT.  ,[BAHIR-37] Start building against Spark Master -  2.1.0-SNAPSHOT, MODIFY pom.xml MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
0,BAHIR-40,29d8c7622cf9663e295d7616ae1e1b089fe80da9,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],[BAHIR-31] Add basic documentation for ZeroMQ connector, ADD README.md
0,BAHIR-48,619936d39d18b7af45b7acec9af02b599a43b056,Documentation improvements for Bahir README.md  ,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
0,BAHIR-20,335f605e75d426f62052378920880ab22729083e,"Create release script Create script to help with release process that perform:
release-prepare
release-perform
release-snapshot
",[BAHIR-17] Update Apache Spark version back to 2.0.0-SNAPSHOT, MODIFY pom.xml
0,BAHIR-124,c7f158d86634d602a19a4abfd873809f8ece9d03,Update Bahir to use Spark 2.2.0 dependency  ,"[BAHIR-110] Implement _changes API for sql-cloudant

 - support loading Cloudant data into Spark DataFrames and SQL tables
   using '_changes' endpoint
 - update README to explain the new config options and differences
   between '_all_docs' and '_changes' endpoints when loading data
 - Add test suite to test Spark DataFrames using the '_all_docs' and
   '_changes' endpoint, assert Cloudant config options, and test Spark
   SQL temporary views

Closes #45", MODIFY pom.xml MODIFY README.md MODIFY CloudantApp.scala main MODIFY CloudantDF.scala main MODIFY CloudantDFOption.scala MODIFY pom.xml MODIFY application.conf ADD CloudantChangesConfig.scala MODIFY CloudantConfig.scala getTotalUrl getBulkRows getContinuousChangesUrl calculateCondition getCreateDBonSave getSchemaSampleSize getSelector calculate getRangeUrl getAllDocsUrl getSubSetUrl allowPartition getBulkPostUrl queryEnabled getDbUrl getDbname getConflictErrStr getTotalRows getForbiddenErrStr getRows getLastNum getUrl MODIFY CloudantReceiver.scala receive onStop MODIFY DefaultSource.scala buildScan create insert ADD CloudantException.scala this this this MODIFY FilterUtil.scala filter MODIFY JsonStoreConfigManager.scala getString getStorageLevel getInt getLong getConfig getBool MODIFY JsonStoreDataAccess.scala saveAll getClPostRequest createDB processAll getMany getClRequest convert MODIFY JsonStoreRDD.scala compute getTotalPartition MODIFY JsonUtil.scala ADD ChangesReceiver.scala receive run onStart onStop ADD n_airportcodemapping.json ADD n_booking.json ADD n_customer.json ADD n_customersession.json ADD n_flight.json ADD n_flightsegment.json ADD log4j.properties ADD ClientSparkFunSuite.scala deleteTestDb runIfTestsEnabled setupClient testIfEnabled createTestDbs afterAll beforeAll teardownClient deleteTestDbs ADD CloudantAllDocsDFSuite.scala beforeAll ADD CloudantChangesDFSuite.scala ADD CloudantOptionSuite.scala ADD CloudantSparkSQLSuite.scala beforeAll ADD TestUtils.scala deleteRecursively
0,BAHIR-54,a351549cf634adf5249862599166ef9ed9073725,"Create initial directory structure in bahir-flink.git As per INFRA-12440, the bahir-flink repository has been created.

We need to set up the initial directory structure in that repository (license file, maven pom, ...)","[BAHIR-51] Add new configuration options to MqttStreamSource.

Add new configuration options to enable secured connections and
other quality of services.

Closes #22", MODIFY README.md MODIFY MQTTStreamSource.scala initialize createSource MODIFY MQTTStreamSourceSuite.scala createStreamingDataframe
1,BAHIR-181,e79a960fa289ee6caefce43c37355a73d44b5220,"username and password should be available for pyspark when using mqtt streaming When using spark-streaming-mqtt with pyspark to access rabbitmq, I  found there are no username and password provied for python api;

These two params are important and necessary for rabbitmq especially when using rabbitmq virtual hosts, so I added a group of functions here;","[BAHIR-181] Add username and password in MQTTUtils for pyspark

Closes #69", MODIFY mqtt.py createStream MODIFY MQTTUtils.scala createPairedByteArrayStream createStream createStream
0,BAHIR-122,5b76e629697d7f3e0094929ce3003e2425368fbc,"[PubSub] Make ""ServiceAccountCredentials"" really broadcastable The origin implementation broadcast the key file path to Spark cluster, then the executor read key file with the broadcasted path. Which is absurd, if you are using a shared Spark cluster in a group/company, you certainly not want to (and have no right to) put your key file on each instance of the cluster.

If you store the key file on driver node and submit your job to a remote cluster. You would get the following warning:
{{WARN ReceiverTracker: Error reported by receiver for stream 0: Failed to pull messages - java.io.FileNotFoundException}}",[BAHIR-124] Update Spark depedency to version 2.2.0, MODIFY pom.xml
0,BAHIR-184,0601698c3721fb3db58431683e556af28ffc0d6a,"Please delete old releases from mirroring system To reduce the load on the volunteer 3rd party mirrors, projects must remove non-current releases from the mirroring system.

The following releases appear to be obsolete, as they do not appear on the Bahir download page:

2.1.1
2.1.2
2.1.3
2.2.0
2.2.1
2.2.2
2.3.0
2.3.1

Furthermore, many of the underlying Spark releases are no longer current according to the Spark release notes, e.g.
http://spark.apache.org/releases/spark-release-2-3-2.html
says that it replaces earlier 2.3.x releases, so 2.3.0 and 2.3.1 are not current Spark releases.

It's unfair to expect the mirrors to carry old releases.","[BAHIR-103] New module with common utilities and test classes

Closes #73", ADD pom.xml RENAME FileHelper.scala recursiveDeleteDir deleteFileQuietly RENAME Logging.scala ADD Retry.scala RENAME ConditionalSparkFunSuite.scala runIf testIf RENAME LocalJavaStreamingContext.java setUp MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY ClientSparkFunSuite.scala beforeAll runIfTestsEnabled testIfEnabled afterAll MODIFY CloudantAllDocsDFSuite.scala MODIFY CloudantChangesDFSuite.scala MODIFY CloudantOptionSuite.scala MODIFY CloudantSparkSQLSuite.scala MODIFY TestUtils.scala deleteRecursively shouldRunTest MODIFY pom.xml MODIFY AkkaStreamSourceSuite.scala MODIFY pom.xml MODIFY CachedMQTTClient.scala MODIFY MQTTStreamSink.scala MODIFY MQTTUtils.scala DELETE BahirUtils.scala visitFile recursiveDeleteDir postVisitDirectory DELETE Logging.scala MODIFY LocalMessageStoreSuite.scala MODIFY MQTTStreamSinkSuite.scala MODIFY MQTTStreamSourceSuite.scala MODIFY pom.xml MODIFY JavaAkkaUtilsSuite.java call testAkkaUtils onReceive MODIFY pom.xml DELETE LocalJavaStreamingContext.java setUp tearDown MODIFY MQTTStreamSuite.scala MODIFY pom.xml MODIFY pom.xml DELETE LocalJavaStreamingContext.java tearDown setUp DELETE PubsubFunSuite.scala testIfEnabled runIfTestsEnabled MODIFY PubsubStreamSuite.scala beforeAll MODIFY PubsubTestUtils.scala shouldRunTest MODIFY pom.xml DELETE LocalJavaStreamingContext.java tearDown setUp MODIFY pom.xml
0,BAHIR-57,28f034f49d19034b596f7f04ca4fc2698a21ad6c,Add Flume sink from Flink to Bahir  ,"[BAHIR-53] Add new configuration options to MQTTInputDStream

Add new configuration options to enable secured connections and
other quality of services.

Closes #23", MODIFY scalastyle-config.xml MODIFY README.md MODIFY MQTTInputDStream.scala onStart getReceiver MODIFY MQTTUtils.scala createStream createStream createStream createStream createStream createStream createStream MODIFY JavaMQTTStreamSuite.java testMQTTStream
0,BAHIR-104,561291bfc17f8eae97318b39ea9cc2d80680d5ce,"MQTT Dstream returned by the new multi topic support API is not a pairRDD The new multi topic support API added with [BAHIR-89], when used in pyspark, does not return a Dstream of <topic,message> tuples. 
Example: 
In pyspark, when creating a Dstream using the new API ( mqttstream = MQTTUtils.createPairedStream(ssc, brokerUrl, topics) ) the expected contents of mqttstream should be a collections of tuples:

(topic,message) , (topic,message) , (topic,message) , ...

Instead, the current content is a flattened list:

topic, message, topic, message, topic, message, ...

that is hard to use.


","[BAHIR-101] Update sql-cloudant readme and python examples

Closes #40.", MODIFY README.md MODIFY CloudantApp.py MODIFY CloudantDF.py MODIFY CloudantDFOption.py
0,BAHIR-182,aecd5fd9f00e40b64ebe81269396bfdc42f8ed00,Create PubNub extension for Apache Spark Streaming Implement new connector for PubNub ([https://www.pubnub.com/)] which is increasing in popularity cloud messaging infrastructure.,"[BAHIR-49] Sink for SQL Streaming MQTT module

Closes #68", MODIFY README.md ADD JavaMQTTSinkWordCount.java main ADD MQTTSinkWordCount.scala main MODIFY org.apache.spark.sql.sources.DataSourceRegister ADD CachedMQTTClient.scala mapToSeq onRemoval deliveryComplete close connectComplete closeMqttClient getOrCreate createMqttClient messageArrived load clear connectionLost ADD MQTTStreamSink.scala commit abort abort shortName commit createRelation createStreamWriter createWriterFactory initialize MODIFY MQTTStreamSource.scala createMicroBatchReader ADD MQTTUtils.scala e parseConfigParams MODIFY LocalMessageStoreSuite.scala ADD MQTTStreamSinkSuite.scala sendToMQTT waitForMessages createContextAndDF MODIFY MQTTStreamSourceSuite.scala createStreamingDataFrame createStreamingDataframe readBackStreamingResults MODIFY MQTTTestUtils.scala deliveryComplete connectionLost connectComplete messageArrived subscribeData publishData sleepUntil
0,BAHIR-103,889de659c33dd56bad7193a4b69e6d05d061a2fd,"Refactoring of files BahirUtils.scala & Logging.scala into bahir-common The files BahirUtils.scala & Logging.scala present under the package 

`org.apache.bahir.utils` 

in the streaming-sql connectors should be refactored into a

`bahir-common` project , which will be shared across extensions.","[BAHIR-97] Akka as SQL Streaming datasource.

Closes #38.", MODIFY pom.xml ADD README.md ADD JavaAkkaStreamWordCount.java main ADD AkkaStreamWordCount.scala main ADD pom.xml ADD assembly.xml ADD AkkaStreamSource.scala store stop store store preStart fetchLastProcessedOffset createSource postStop getBatch e shortName close sourceSchema receive getOrCreatePersistenceInstance ADD MessageStore.scala get this ADD BahirUtils.scala visitFile postVisitDirectory recursiveDeleteDir ADD Logging.scala ADD feeder_actor.conf ADD log4j.properties ADD AkkaStreamSourceSuite.scala readBackSreamingResults writeStreamResults createStreamingDataframe ADD AkkaTestUtils.scala setup setCountOfMessages run getFeederActorUri getFeederActorConfig setMessage shutdown
0,BAHIR-23,61f5592e4bd334a6c4c39cae2229dce53ee66535,"Build should fail on Checkstyle violation Currently the maven build will fail for code style violations in Scala files but the build will succeed regardless of code style violations in Java files.

{code:xml}
      <plugin>
        ...
        <artifactId>scalastyle-maven-plugin</artifactId>
        ...
        <configuration>
          ...
          <failOnViolation>true</failOnViolation>
          ...
        </configuration>
        ...
      </plugin>
      <plugin>
        ...
        <artifactId>maven-checkstyle-plugin</artifactId>
        ...
        <configuration>
          ...
          <failOnViolation>false</failOnViolation>
          ...
        </configuration>
        ...
      </plugin>
{code}

As a consequence potential problems in the Java code may not get noticed.","[BAHIR-22] add shell script to run examples

Apache Spark has a convenience script ./bin/run-example to allow users to
quickly run the pre-packaged examples without having to compose a long(ish)
spark-submit command.
The JavaDoc of most examples refers to that ./bin/run-example script
in their description of how to run that example.

This adds a similar convenience script to the Apache Bahir project in order
to keep consistent with existing (Apache Spark) documentation and to
(at least initially) hide additional complexities of the spark-submit command.

Example:

./bin/run-example \
  org.apache.spark.examples.streaming.akka.ActorWordCount localhost 9999

...translates to this spark-submit command:

${SPARK_HOME}/bin/spark-submit \
  --packages org.apache.bahir:spark-streaming-akka_2.11:2.0.0-SNAPSHOT \
  --class org.apache.spark.examples.streaming.akka.ActorWordCount \
    streaming-akka/target/spark-streaming-akka_2.11-2.0.0-SNAPSHOT-tests.jar \
  localhost 9999

Closes #4", ADD run-example
1,BAHIR-128,0e1505a8960bfe40ea025267bbf36ec5c4cf5c79,"Test failing sporadically in sql-cloudant's CloudantChangesDFSuite This failure happened during pre-release testing for Bahir RC 2.2.0:
CloudantChangesDFSuite:
- load and save data from Cloudant database *** FAILED ***
  0 did not equal 1967 (CloudantChangesDFSuite.scala:49)

Partial stack trace:
{code:java}
Exception in thread ""Cloudant Receiver"" org.apache.spark.SparkException: Cannot add data as BlockGenerator has not been started or has been stopped
    at org.apache.spark.streaming.receiver.BlockGenerator.addData(BlockGenerator.scala:173)
    at org.apache.spark.streaming.receiver.ReceiverSupervisorImpl.pushSingle(ReceiverSupervisorImpl.scala:120)
    at org.apache.spark.streaming.receiver.Receiver.store(Receiver.scala:119)
    at org.apache.bahir.cloudant.internal.ChangesReceiver$$anonfun$org$apache$bahir$cloudant$internal$ChangesReceiver$$receive$1$$anonfun$apply$1.apply(ChangesReceiver.scala:82)
{code}","[BAHIR-128] Improve sql-cloudant _changes receiver

This change improves the stability of _changes receiver and
fix the intermitent failing test in sql-cloudant's
CloudantChangesDFSuite.

How

Improve performance and decrease testing time by setting batch size
to 8 seconds and using seq_interval _changes feed option.
Use getResource to load json files path
Added Mike Rhodes's ChangesRowScanner for reading each _changes line
and transforming to GSON's JSON object
Added Mike Rhodes's ChangesRow representing a row in the changes feed

Closes #57", ADD ChangesRow.java getDoc Rev getRev getSeq getId getChanges ADD ChangesRowScanner.java readRowFromReader MODIFY CloudantChangesConfig.scala MODIFY DefaultSource.scala create MODIFY JsonStoreDataAccess.scala MODIFY ChangesReceiver.scala receive MODIFY ClientSparkFunSuite.scala createTestDbs MODIFY CloudantChangesDFSuite.scala
0,BAHIR-105,f0d9a84f76cb34a432e1d2db053d2471a8ab2ba4,Add distribution module for Flink extensions  ,"[BAHIR-101] Spark SQL datasource for CounchDB/Cloudant

Initial code supporting CounchDB/Cloudant as an Spark SQL
data source. The initial source contains the core connector,
examples, and basic documentation on the README.

Closes #39.", MODIFY README.md MODIFY pom.xml ADD README.md ADD CloudantApp.py ADD CloudantDF.py ADD CloudantDFOption.py ADD CloudantApp.scala main ADD CloudantDF.scala main ADD CloudantDFOption.scala main ADD CloudantStreaming.scala getInstance main ADD CloudantStreamingSelector.scala main ADD pom.xml ADD application.conf ADD reference.conf ADD CloudantConfig.scala getRangeUrl getDbname getTotalRows getForbiddenErrStr getConflictErrStr getOneUrlExcludeDDoc1 getSubSetUrl allowPartition getLastNum getTotalUrl getBulkRows calculateCondition getCreateDBonSave getLastUrl getContinuousChangesUrl getChangesUrl calculate getAllDocsUrlExcludeDDoc getOneUrl getSelector getAllDocsUrl getSchemaSampleSize getRows getBulkPostUrl getDbUrl getOneUrlExcludeDDoc2 ADD CloudantReceiver.scala onStart onStop receive run ADD DefaultSource.scala buildScan createRelation create insert createRelation createRelation ADD FilterUtil.scala filter containsFiltersFor apply analyze evaluate getFiltersForPostProcess getFilterAttribute getInfo ADD JsonStoreConfigManager.scala getString getConfig getConfig getBool getLong getInt ADD JsonStoreDataAccess.scala getChanges convert convertSkip getOne getClPostRequest saveAll getTotalRows createDB processAll getMany getIterator processIterator ADD JsonStoreRDD.scala compute ADD JsonUtil.scala getField
0,BAHIR-183,a73ab48a2dfec866b2ffa0ccf0d2bfeaba6fc782,"Using HDFS for saving message for mqtt source Currently in spark-sql-streaming-mqtt, the received mqtt message is saved in a local file by driver, this will have the risks of losing data for cluster mode when application master failover occurs. So saving in-coming mqtt messages using a director in checkpoint will solve this problem.","[BAHIR-186] SSL support in MQTT structured streaming

Closes #74", MODIFY README.md MODIFY MQTTUtils.scala parseConfigParams ADD keystore.jks ADD truststore.jks MODIFY MQTTStreamSinkSuite.scala sendToMQTT MODIFY MQTTTestUtils.scala connectToServer publishData subscribeData setup
0,BAHIR-14,86ee8779c7980e10c1c246fff7a171d0118c0239,Cleanup maven pom from Spark dependencies There are a lot of dependencies that came from Spark and are not necessary for these extensions. We should cleanup the current poms and make the dependencies as lean as possible.,"[BAHIR-15] Enable RAT on builds

Enable RAT to run automatically during builds
to verify license header policy.", MODIFY pom.xml
1,BAHIR-28,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,Add documentation for streaming-akka connector  ,[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
0,BAHIR-105,889de659c33dd56bad7193a4b69e6d05d061a2fd,Add distribution module for Flink extensions  ,"[BAHIR-97] Akka as SQL Streaming datasource.

Closes #38.", MODIFY pom.xml ADD README.md ADD JavaAkkaStreamWordCount.java main ADD AkkaStreamWordCount.scala main ADD pom.xml ADD assembly.xml ADD AkkaStreamSource.scala store stop store store preStart fetchLastProcessedOffset createSource postStop getBatch e shortName close sourceSchema receive getOrCreatePersistenceInstance ADD MessageStore.scala get this ADD BahirUtils.scala visitFile postVisitDirectory recursiveDeleteDir ADD Logging.scala ADD feeder_actor.conf ADD log4j.properties ADD AkkaStreamSourceSuite.scala readBackSreamingResults writeStreamResults createStreamingDataframe ADD AkkaTestUtils.scala setup setCountOfMessages run getFeederActorUri getFeederActorConfig setMessage shutdown
0,BAHIR-181,be1effaaf7cfde28d19e032e038694e01fbf169b,"username and password should be available for pyspark when using mqtt streaming When using spark-streaming-mqtt with pyspark to access rabbitmq, I  found there are no username and password provied for python api;

These two params are important and necessary for rabbitmq especially when using rabbitmq virtual hosts, so I added a group of functions here;","[BAHIR-166] Migrate akka sql streaming source to DataSource v2 API

Migrate akka sql streaming source to DataSource v2 API.

Closes #67", MODIFY pom.xml MODIFY README.md MODIFY AkkaStreamSource.scala shortName commit createMicroBatchReader stop fetchLastProcessedOffset deserializeOffset createSource get setOffsetRange sourceSchema e getBatch close receive readSchema createDataReaderFactories next createDataReader ADD LongOffset.scala + apply - convert MODIFY MessageStore.scala get get MODIFY AkkaStreamSourceSuite.scala MODIFY AkkaTestUtils.scala shutdown MODIFY ActorWordCount.scala main MODIFY ActorReceiver.scala onStop MODIFY AkkaStreamSuite.scala MODIFY ZeroMQWordCount.scala main
0,BAHIR-47,c317def2a7575713d31353f87025ddacaf30e503,Bring up release download for Apache Bahir website  ,[BAHIR-37] Start building against Spark Master -  2.1.0-SNAPSHOT, MODIFY pom.xml MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
1,BAHIR-18,c277d4902e7538609523eee0ded3950e0d14d260,"Include examples in Maven test build We need to find a way to include the examples in the Maven build but keep them excluded from the binary jar(s) and have IDEs like IntelliJ or Eclipse recognize the {{<module>/examples/src/\[java|scala\]}} as source files.

One way this can be achieved is by including the examples as ""additional test sources"".","[BAHIR-18] Configure examples as maven test sources

This PR configure examples as maven test resources to be
recognized by maven builds. This acomplish the following :

- The examples get compiled
- IDEs like IntelliJ or Eclipse recognize the
  <module>/examples/src/[java|scala] as source folders
- Keep the examples along with their additional dependencies
  excluded from the generated binaries

Closes #2", MODIFY pom.xml MODIFY JavaActorWordCount.java main MODIFY ActorWordCount.scala main MODIFY JavaAkkaUtilsSuite.java onReceive MODIFY MQTTWordCount.scala main MODIFY package.scala ADD AFINN-111.txt MODIFY JavaTwitterHashTagJoinSentiments.java main MODIFY TwitterAlgebirdCMS.scala main MODIFY TwitterAlgebirdHLL.scala main MODIFY TwitterHashTagJoinSentiments.scala main MODIFY TwitterPopularTags.scala main MODIFY pom.xml MODIFY package.scala MODIFY ZeroMQWordCount.scala main MODIFY package.scala
0,BAHIR-16,fc1ef7f990880cad8ce69300bb9bbbb8ad260050,"Build issues due to log4j properties not found log4j:ERROR Could not read configuration file from URL [file:src/test/resources/log4j.properties].
java.io.FileNotFoundException: src/test/resources/log4j.properties (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:273)
	at org.apache.hadoop.util.ShutdownHookManager.<clinit>(ShutdownHookManager.java:44)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:179)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:152)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:57)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.streaming.StreamingContext$.<init>(StreamingContext.scala:730)
	at org.apache.spark.streaming.StreamingContext$.<clinit>(StreamingContext.scala)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:100)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:63)
	at org.apache.spark.streaming.akka.JavaAkkaUtilsSuite.testAkkaUtils(JavaAkkaUtilsSuite.java:36)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
log4j:ERROR Ignoring configuration file [file:src/test/resources/log4j.properties].","[BAHIR-13] Update dependencies on spark-tags

The spark-test-tags and spark-tags were merged in
revision 8ad9f08c9 and the modules in Bahir needs
to be updated to properly use spark-tags dependency.", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-48,31b3e96d4680c54ff05d4b09f944106d22b62760,Documentation improvements for Bahir README.md  ,[BAHIR-52] Fix code paragraph formatting for Akka readme, MODIFY README.md
1,BAHIR-53,28f034f49d19034b596f7f04ca4fc2698a21ad6c,"Add additional MQTT options/parameters to MQTTInputDStream I propose that the MQTTInputDStream be extended to optionally allow setting the following mqtt options / parameters (like MqttStreamSource in BAHIR-51):
* username
* password
* clientId
* cleanSession
* QoS
* connectionTimeout
* keepAliveInterval
* mqttVersion","[BAHIR-53] Add new configuration options to MQTTInputDStream

Add new configuration options to enable secured connections and
other quality of services.

Closes #23", MODIFY scalastyle-config.xml MODIFY README.md MODIFY MQTTInputDStream.scala onStart getReceiver MODIFY MQTTUtils.scala createStream createStream createStream createStream createStream createStream createStream MODIFY JavaMQTTStreamSuite.java testMQTTStream
0,BAHIR-35,48e91fca54f7fab6f6171be4c05747a985876483,"Include Python code in the binary jars for use with ""--packages ..."" Currently, to make use the PySpark code (i.e streaming-mqtt/python) a user will have to download the jar from Maven central or clone the code from GitHub and then have to find individual *.py files, create a zip and add that to the {{spark-submit}} command with the {{--py-files}} option, or, add them to the {{PYTHONPATH}} when running locally.

If we include the Python code in the binary build (to the jar that gets uploaded to Maven central), then users need not do any acrobatics besides using the {{--packages ...}} option.

An example where the Python code is part of the binary jar is the [GraphFrames|https://spark-packages.org/package/graphframes/graphframes] package.","[BAHIR-36] Update Readme.md

- Add how to build and test project

Closes #12", MODIFY README.md
1,BAHIR-31,29d8c7622cf9663e295d7616ae1e1b089fe80da9,Add documentation for streaming-zeromq connector  ,[BAHIR-31] Add basic documentation for ZeroMQ connector, ADD README.md
1,BAHIR-104,770b2916f0a7603b62ef997a0ea98b38c6da15c0,"MQTT Dstream returned by the new multi topic support API is not a pairRDD The new multi topic support API added with [BAHIR-89], when used in pyspark, does not return a Dstream of <topic,message> tuples. 
Example: 
In pyspark, when creating a Dstream using the new API ( mqttstream = MQTTUtils.createPairedStream(ssc, brokerUrl, topics) ) the expected contents of mqttstream should be a collections of tuples:

(topic,message) , (topic,message) , (topic,message) , ...

Instead, the current content is a flattened list:

topic, message, topic, message, topic, message, ...

that is hard to use.


","[BAHIR-104] Multi-topic MQTT DStream in Python is now a PairRDD.

Closes #55", MODIFY README.md MODIFY tests.py _start_context_with_paired_stream test_mqtt_pair_stream test_mqtt_pair_stream.retry _start_context_with_paired_stream.getOutput MODIFY mqtt.py createPairedStream _list_to_java_string_array
0,BAHIR-222,d036820c0efa1b2e9b8021506164b67582352dff,Update Readme with details of SQL Streaming SQS connector Adding link to SQL Streaming SQS connector in BAHIR Readme.,"[BAHIR-213] Faster S3 file Source for Structured Streaming with SQS (#91)

Using FileStreamSource to read files from a S3 bucket has problems 
both in terms of costs and latency:

Latency: Listing all the files in S3 buckets every micro-batch can be both
slow and resource-intensive.

Costs: Making List API requests to S3 every micro-batch can be costly.

The solution is to use Amazon Simple Queue Service (SQS) which lets 
you find new files written to S3 bucket without the need to list all the 
files every micro-batch.

S3 buckets can be configured to send a notification to an Amazon SQS Queue
on Object Create / Object Delete events. For details see AWS documentation
here Configuring S3 Event Notifications

Spark can leverage this to find new files written to S3 bucket by reading 
notifications from SQS queue instead of listing files every micro-batch.

This PR adds a new SQSSource which uses Amazon SQS queue to find 
new files every micro-batch.

Usage

val inputDf = spark .readStream
   .format(""s3-sqs"")
   .schema(schema)
   .option(""fileFormat"", ""json"")
   .option(""sqsUrl"", ""https://QUEUE_URL"")
   .option(""region"", ""us-east-1"")
   .load()", MODIFY pom.xml ADD README.md ADD SqsSourceExample.scala main ADD pom.xml ADD BasicAWSCredentialsProvider.java refresh BasicAWSCredentialsProvider getCredentials toString ADD InstanceProfileCredentialsProviderWithRetries.java getCredentials ADD org.apache.spark.sql.sources.DataSourceRegister ADD log4j.properties ADD SqsClient.scala deleteMessagesFromQueue run createSqsClient evaluateRetries assertSqsIsWorking parseSqsMessages convertTimestampToMills sqsFetchMessages addToDeleteMessageQueue ADD SqsFileCache.scala filterAllUncommittedFiles getUncommittedFiles add purge markCommitted isNewFile filterTopUncommittedFiles stripPathIfNecessary ADD SqsSource.scala fetchMaxOffset stop commit getBatch ADD SqsSourceOptions.scala this withBooleanParameter ADD SqsSourceProvider.scala sourceSchema createSource shortName ADD log4j.properties ADD SqsSourceOptionsSuite.scala testBadOptions testMissingMandatoryOptions
0,BAHIR-44,95633de6741ddf757cc4964425463a972e1b4cbe,Add new sql-streaming-mqtt to distribution  ,[BAHIR-43] Add Apache License header file, MODIFY pom.xml MODIFY org.apache.spark.sql.sources.DataSourceRegister
0,BAHIR-122,c7f158d86634d602a19a4abfd873809f8ece9d03,"[PubSub] Make ""ServiceAccountCredentials"" really broadcastable The origin implementation broadcast the key file path to Spark cluster, then the executor read key file with the broadcasted path. Which is absurd, if you are using a shared Spark cluster in a group/company, you certainly not want to (and have no right to) put your key file on each instance of the cluster.

If you store the key file on driver node and submit your job to a remote cluster. You would get the following warning:
{{WARN ReceiverTracker: Error reported by receiver for stream 0: Failed to pull messages - java.io.FileNotFoundException}}","[BAHIR-110] Implement _changes API for sql-cloudant

 - support loading Cloudant data into Spark DataFrames and SQL tables
   using '_changes' endpoint
 - update README to explain the new config options and differences
   between '_all_docs' and '_changes' endpoints when loading data
 - Add test suite to test Spark DataFrames using the '_all_docs' and
   '_changes' endpoint, assert Cloudant config options, and test Spark
   SQL temporary views

Closes #45", MODIFY pom.xml MODIFY README.md MODIFY CloudantApp.scala main MODIFY CloudantDF.scala main MODIFY CloudantDFOption.scala MODIFY pom.xml MODIFY application.conf ADD CloudantChangesConfig.scala MODIFY CloudantConfig.scala getTotalUrl getBulkRows getContinuousChangesUrl calculateCondition getCreateDBonSave getSchemaSampleSize getSelector calculate getRangeUrl getAllDocsUrl getSubSetUrl allowPartition getBulkPostUrl queryEnabled getDbUrl getDbname getConflictErrStr getTotalRows getForbiddenErrStr getRows getLastNum getUrl MODIFY CloudantReceiver.scala receive onStop MODIFY DefaultSource.scala buildScan create insert ADD CloudantException.scala this this this MODIFY FilterUtil.scala filter MODIFY JsonStoreConfigManager.scala getString getStorageLevel getInt getLong getConfig getBool MODIFY JsonStoreDataAccess.scala saveAll getClPostRequest createDB processAll getMany getClRequest convert MODIFY JsonStoreRDD.scala compute getTotalPartition MODIFY JsonUtil.scala ADD ChangesReceiver.scala receive run onStart onStop ADD n_airportcodemapping.json ADD n_booking.json ADD n_customer.json ADD n_customersession.json ADD n_flight.json ADD n_flightsegment.json ADD log4j.properties ADD ClientSparkFunSuite.scala deleteTestDb runIfTestsEnabled setupClient testIfEnabled createTestDbs afterAll beforeAll teardownClient deleteTestDbs ADD CloudantAllDocsDFSuite.scala beforeAll ADD CloudantChangesDFSuite.scala ADD CloudantOptionSuite.scala ADD CloudantSparkSQLSuite.scala beforeAll ADD TestUtils.scala deleteRecursively
0,BAHIR-124,e491610d8a7a9cff1ed5087c1c0cbe6b2c29eb39,Update Bahir to use Spark 2.2.0 dependency  ,"[BAHIR-122][PubSub] Make ""ServiceAccountCredentials"" really broadcastable

Instead of requiring key files on each instance of the cluster, we read
the key file content on the driver node and store the binary in the
ServiceAccountCredentials. When the provider is called, it retrieves the
credential with the in-memory key file.

Closes #48", MODIFY README.md MODIFY pom.xml MODIFY SparkGCPCredentials.scala MODIFY SparkGCPCredentialsBuilderSuite.scala
0,BAHIR-42,31b3e96d4680c54ff05d4b09f944106d22b62760,Refactor sql-streaming-mqtt examples to follow other projects pattern  ,[BAHIR-52] Fix code paragraph formatting for Akka readme, MODIFY README.md
1,BAHIR-124,5b76e629697d7f3e0094929ce3003e2425368fbc,Update Bahir to use Spark 2.2.0 dependency  ,[BAHIR-124] Update Spark depedency to version 2.2.0, MODIFY pom.xml
0,BAHIR-125,5b76e629697d7f3e0094929ce3003e2425368fbc,Update Bahir pom to use JAVA 8 and to align with Spark 2.2.0 dependencies  ,[BAHIR-124] Update Spark depedency to version 2.2.0, MODIFY pom.xml
0,BAHIR-43,619936d39d18b7af45b7acec9af02b599a43b056,Add missing apache license header to sql-mqtt file  ,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
1,BAHIR-69,942b43dc428c7ade2789fb09df0fda360cf94024,Release script fails intermitently when publishing to staging maven repo  ,"[BAHIR-69] Clean build between different scala version

During release-publish, execute a mvn clean between different scala
version builds", MODIFY release-build.sh
0,BAHIR-31,619936d39d18b7af45b7acec9af02b599a43b056,Add documentation for streaming-zeromq connector  ,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
0,BAHIR-30,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,Add documentation for streaming-twitter connector  ,[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
0,BAHIR-39,70539a35dc9bae9ee5d380351ffc32fa6e62567e,MQTT as a streaming source for SQL Streaming. MQTT compatible streaming source for Spark SQL Streaming.,"[BAHIR-42] Refactor sql-streaming-mqtt example

Move JavaMQTTStreamWordCount to examples root folder
which are processed by the build as test resources
and not built into the extension itself following
the pattern used by other examples.", RENAME JavaMQTTStreamWordCount.java main
0,BAHIR-20,9d63f090d8e4d443a811187dd5fd40a3c615dd41,"Create release script Create script to help with release process that perform:
release-prepare
release-perform
release-snapshot
","[BAHIR-21] Change scala version script

Script to change scala version in use between 2.10 and 2.11
to help during development builds and also when publishing
releases in both scala version", ADD change-scala-version.sh
0,BAHIR-53,415576ba702206ba9cfc5c8bdbdee4869a1e52ac,"Add additional MQTT options/parameters to MQTTInputDStream I propose that the MQTTInputDStream be extended to optionally allow setting the following mqtt options / parameters (like MqttStreamSource in BAHIR-51):
* username
* password
* clientId
* cleanSession
* QoS
* connectionTimeout
* keepAliveInterval
* mqttVersion","[BAHIR-61] Enable publishing release artifacts from a tag

Enable a --gitTag parameter to identify an RC tag to be used
when publishing artifacts to maven.", MODIFY release-build.sh
1,BAHIR-117,86ded930e4af769e8191c8f415fe48193dd4914b,"Expand filtering options for TwitterInputDStream Currently, the TwitterInputDStream only supports filtering by keywords [1] which corresponds to the ""track"" option in the Twitter API [2]. The Twitter API supports many more ways to receive a filtered stream (e.g. get Tweets in a particular location [3]). It would be very useful to expose these additional filtering options in this library.

Proposal: add a new public method to TwitterUtils which follows the same interface as createStream [4] but which takes a FilterQuery [5] object as argument. In this way, we give full filtering flexibility to our users.

I'm currently working on Project Fortis, a social data analysis platform for the United Nations [6]. The extra filtering options would be very useful for my project so I'm happy to implement this and create a pull request.

[1] https://github.com/apache/bahir/blob/fd4c35fc9f7ebb57464d231cf5d66e7bc4096a1b/streaming-twitter/src/main/scala/org/apache/spark/streaming/twitter/TwitterInputDStream.scala#L44
[2] https://dev.twitter.com/streaming/overview/request-parameters#track
[3] https://dev.twitter.com/streaming/overview/request-parameters#locations
[4] https://github.com/apache/bahir/blob/fd4c35fc9f7ebb57464d231cf5d66e7bc4096a1b/streaming-twitter/src/main/scala/org/apache/spark/streaming/twitter/TwitterUtils.scala#L39
[5] http://twitter4j.org/javadoc/twitter4j/FilterQuery.html
[6] https://fortis-web.azurewebsites.net/#/site/ocha/","[BAHIR-117] Expand filtering options for TwitterInputDStream

Adds a new method to TwitterUtils that enables users to pass
an arbitrary FilterQuery down to the TwitterReceiver.

This enables use-cases like receiving Tweets based on location,
based on handle, etc. Previously users were only able to receive
Tweets based on disjunctive keyword queries.

Closes #43.", MODIFY JavaActorWordCount.java JavaSampleActorReceiver ADD TwitterLocations.scala main MODIFY TwitterInputDStream.scala getReceiver onStart MODIFY TwitterUtils.scala createStream createStream createFilteredStream createFilteredStream MODIFY JavaTwitterStreamSuite.java testTwitterStream MODIFY TwitterStreamSuite.scala
1,BAHIR-103,0601698c3721fb3db58431683e556af28ffc0d6a,"Refactoring of files BahirUtils.scala & Logging.scala into bahir-common The files BahirUtils.scala & Logging.scala present under the package 

`org.apache.bahir.utils` 

in the streaming-sql connectors should be refactored into a

`bahir-common` project , which will be shared across extensions.","[BAHIR-103] New module with common utilities and test classes

Closes #73", ADD pom.xml RENAME FileHelper.scala recursiveDeleteDir deleteFileQuietly RENAME Logging.scala ADD Retry.scala RENAME ConditionalSparkFunSuite.scala runIf testIf RENAME LocalJavaStreamingContext.java setUp MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY ClientSparkFunSuite.scala beforeAll runIfTestsEnabled testIfEnabled afterAll MODIFY CloudantAllDocsDFSuite.scala MODIFY CloudantChangesDFSuite.scala MODIFY CloudantOptionSuite.scala MODIFY CloudantSparkSQLSuite.scala MODIFY TestUtils.scala deleteRecursively shouldRunTest MODIFY pom.xml MODIFY AkkaStreamSourceSuite.scala MODIFY pom.xml MODIFY CachedMQTTClient.scala MODIFY MQTTStreamSink.scala MODIFY MQTTUtils.scala DELETE BahirUtils.scala visitFile recursiveDeleteDir postVisitDirectory DELETE Logging.scala MODIFY LocalMessageStoreSuite.scala MODIFY MQTTStreamSinkSuite.scala MODIFY MQTTStreamSourceSuite.scala MODIFY pom.xml MODIFY JavaAkkaUtilsSuite.java call testAkkaUtils onReceive MODIFY pom.xml DELETE LocalJavaStreamingContext.java setUp tearDown MODIFY MQTTStreamSuite.scala MODIFY pom.xml MODIFY pom.xml DELETE LocalJavaStreamingContext.java tearDown setUp DELETE PubsubFunSuite.scala testIfEnabled runIfTestsEnabled MODIFY PubsubStreamSuite.scala beforeAll MODIFY PubsubTestUtils.scala shouldRunTest MODIFY pom.xml DELETE LocalJavaStreamingContext.java tearDown setUp MODIFY pom.xml
0,BAHIR-44,c98dd0feefa79c70be65de06a411a2f9c4fc42dc,Add new sql-streaming-mqtt to distribution  ,"[BAHIR-39] Add SQL Streaming MQTT support

This provides support for using MQTT sources for
the new Spark Structured Streaming. This uses
MQTT client persistence layer to provide minimal
fault tolerance.

Closes #13", MODIFY pom.xml ADD README.md ADD pom.xml ADD assembly.xml ADD JavaMQTTStreamWordCount.java main ADD org.apache.spark.sql.sources.DataSourceRegister ADD MQTTStreamSource.scala stop connectionLost messageArrived fetchLastProcessedOffset createSource initialize deliveryComplete getBatch shortName sourceSchema connectComplete e ADD MessageStore.scala get this ADD MQTTStreamWordCount.scala main ADD BahirUtils.scala postVisitDirectory visitFile recursiveDeleteDir ADD Logging.scala ADD log4j.properties ADD LocalMessageStoreSuite.scala ADD MQTTStreamSourceSuite.scala createStreamingDataframe writeStreamResults readBackStreamingResults ADD MQTTTestUtils.scala setup publishData teardown findFreePort
0,BAHIR-40,c98dd0feefa79c70be65de06a411a2f9c4fc42dc,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],"[BAHIR-39] Add SQL Streaming MQTT support

This provides support for using MQTT sources for
the new Spark Structured Streaming. This uses
MQTT client persistence layer to provide minimal
fault tolerance.

Closes #13", MODIFY pom.xml ADD README.md ADD pom.xml ADD assembly.xml ADD JavaMQTTStreamWordCount.java main ADD org.apache.spark.sql.sources.DataSourceRegister ADD MQTTStreamSource.scala stop connectionLost messageArrived fetchLastProcessedOffset createSource initialize deliveryComplete getBatch shortName sourceSchema connectComplete e ADD MessageStore.scala get this ADD MQTTStreamWordCount.scala main ADD BahirUtils.scala postVisitDirectory visitFile recursiveDeleteDir ADD Logging.scala ADD log4j.properties ADD LocalMessageStoreSuite.scala ADD MQTTStreamSourceSuite.scala createStreamingDataframe writeStreamResults readBackStreamingResults ADD MQTTTestUtils.scala setup publishData teardown findFreePort
0,BAHIR-56,a351549cf634adf5249862599166ef9ed9073725,Add ActiveMQ streaming connector for Flink  ,"[BAHIR-51] Add new configuration options to MqttStreamSource.

Add new configuration options to enable secured connections and
other quality of services.

Closes #22", MODIFY README.md MODIFY MQTTStreamSource.scala initialize createSource MODIFY MQTTStreamSourceSuite.scala createStreamingDataframe
0,BAHIR-42,95633de6741ddf757cc4964425463a972e1b4cbe,Refactor sql-streaming-mqtt examples to follow other projects pattern  ,[BAHIR-43] Add Apache License header file, MODIFY pom.xml MODIFY org.apache.spark.sql.sources.DataSourceRegister
0,BAHIR-150,f9a67de735fee8c89518cf37a513766c9e9e6b15,"Jenkins PR builder should not abort build after first failed module Currently our Jenkins build skips building any further modules after the first build or test failure (see [PR #55|https://github.com/apache/bahir/pull/55#issuecomment-349487768])

We should change the Jenking PR build configuration to continue running the Maven build and report any/all failures after building and testing all modules.",[BAHIR-149] Update Cloudant dependency to release 2.11.0, MODIFY pom.xml
1,BAHIR-21,9d63f090d8e4d443a811187dd5fd40a3c615dd41,Create script to change build between scala 2.10 and 2.11  ,"[BAHIR-21] Change scala version script

Script to change scala version in use between 2.10 and 2.11
to help during development builds and also when publishing
releases in both scala version", ADD change-scala-version.sh
0,BAHIR-110,e3d9e6960941696ba073735e9d039c85146c217a,"Implement _changes API for non-streaming receiver Today we use the _changes API for Spark streaming receiver and _all_docs API for non-streaming receiver. _all_docs API supports parallel reads (using offset and range) but performance of _changes API is still better in most cases (even with single threaded support).

With this ticket we want to:
a) implement _changes API for non-streaming receivers
b) allow customers to pick either _all_docs (default) or _changes API endpoint, with documentation about pros and cons

_changes performance details:
Successfully loaded Cloudant (using local cloudant-developer docker image) docs into Spark (local standalone) with the following database sizes: 15GB (time: 8 1/2 mins), 20GB (17 mins), 46GB (25 mins), and 75GB (48 1/2 mins).","[BAHIR-100] Enhance MQTT connector to support byte arrays

Closes #47", MODIFY README.md MODIFY MQTTInputDStream.scala ADD MQTTPairedByteArrayInputDStream.scala onStart connectionLost getReceiver onStop messageArrived deliveryComplete MODIFY MQTTPairedInputDStream.scala MODIFY MQTTUtils.scala createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedByteArrayStream createPairedByteArrayStream createPairedStream createPairedStream createPairedStream createPairedStream MODIFY JavaMQTTStreamSuite.java testMQTTStream MODIFY MQTTStreamSuite.scala
0,BAHIR-138,c5263df233b53a603883c1a5c4aa6c652f0e7fab,"Fix sql-cloudant deprecation messages Deprecation warnings in {{DefaultSource}}:

{code}
[INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ spark-sql-cloudant_2.11 ---
[INFO] Compiling 11 Scala sources to sql-cloudant/target/scala-2.11/classes...
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:59: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]         val df = sqlContext.read.json(cloudantRDD)
[WARNING]                                  ^
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:115: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]             dataFrame = sqlContext.read.json(cloudantRDD)
[WARNING]                                         ^
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:121: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]             sqlContext.read.json(aRDD)
[WARNING]                             ^
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:152: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]               dataFrame = sqlContext.sparkSession.read.json(globalRDD)
[WARNING]                                                        ^
[WARNING] four warnings found
{code}


Deprecation warnings in {{CloudantStreaming}} and {{CloudantStreamingSelector}} examples:

{code}
[INFO] --- scala-maven-plugin:3.2.2:testCompile (scala-test-compile-first) @ spark-sql-cloudant_2.11 ---
[INFO] Compiling 11 Scala sources to sql-cloudant/target/scala-2.11/test-classes...
[WARNING] sql-cloudant/examples/src/main/scala/org/apache/spark/examples/sql/cloudant/CloudantStreaming.scala:46: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]       val changesDataFrame = spark.read.json(rdd)
[WARNING]                                         ^
[WARNING] sql-cloudant/examples/src/main/scala/org/apache/spark/examples/sql/cloudant/CloudantStreaming.scala:67: method registerTempTable in class Dataset is deprecated: Use createOrReplaceTempView(viewName) instead.
[WARNING]           changesDataFrame.registerTempTable(""airportcodemapping"")
[WARNING]                            ^
[WARNING] sql-cloudant/examples/src/main/scala/org/apache/spark/examples/sql/cloudant/CloudantStreamingSelector.scala:50: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]       val changesDataFrame = spark.read.json(rdd)
[WARNING]                                         ^
[WARNING] three warnings found
{code}","[BAHIR-123] Upgrade to play-json 2.6.6

Fixed breaking API changes between play-json 2.5.x and 2.6.x
in sql-cloudant by replacing deprecated methods.

Closes #50", MODIFY pom.xml MODIFY pom.xml MODIFY ClientSparkFunSuite.scala deleteTestDbs MODIFY CloudantAllDocsDFSuite.scala MODIFY CloudantChangesDFSuite.scala
0,BAHIR-100,f0d9a84f76cb34a432e1d2db053d2471a8ab2ba4,"Providing MQTT Spark Streaming to return encoded Byte[] message without corruption Now a days Network bandwidth is becoming a serious resource that need to be conserver in IoT ecosystem, For this puropse we are using different byte[] based encoding such as Protocol Buffer and flat Buffer, Once this encoded message is converted into string the data becomes corrupted, So same byte[] format need to be preserved when forwarded.","[BAHIR-101] Spark SQL datasource for CounchDB/Cloudant

Initial code supporting CounchDB/Cloudant as an Spark SQL
data source. The initial source contains the core connector,
examples, and basic documentation on the README.

Closes #39.", MODIFY README.md MODIFY pom.xml ADD README.md ADD CloudantApp.py ADD CloudantDF.py ADD CloudantDFOption.py ADD CloudantApp.scala main ADD CloudantDF.scala main ADD CloudantDFOption.scala main ADD CloudantStreaming.scala getInstance main ADD CloudantStreamingSelector.scala main ADD pom.xml ADD application.conf ADD reference.conf ADD CloudantConfig.scala getRangeUrl getDbname getTotalRows getForbiddenErrStr getConflictErrStr getOneUrlExcludeDDoc1 getSubSetUrl allowPartition getLastNum getTotalUrl getBulkRows calculateCondition getCreateDBonSave getLastUrl getContinuousChangesUrl getChangesUrl calculate getAllDocsUrlExcludeDDoc getOneUrl getSelector getAllDocsUrl getSchemaSampleSize getRows getBulkPostUrl getDbUrl getOneUrlExcludeDDoc2 ADD CloudantReceiver.scala onStart onStop receive run ADD DefaultSource.scala buildScan createRelation create insert createRelation createRelation ADD FilterUtil.scala filter containsFiltersFor apply analyze evaluate getFiltersForPostProcess getFilterAttribute getInfo ADD JsonStoreConfigManager.scala getString getConfig getConfig getBool getLong getInt ADD JsonStoreDataAccess.scala getChanges convert convertSkip getOne getClPostRequest saveAll getTotalRows createDB processAll getMany getIterator processIterator ADD JsonStoreRDD.scala compute ADD JsonUtil.scala getField
0,BAHIR-105,561291bfc17f8eae97318b39ea9cc2d80680d5ce,Add distribution module for Flink extensions  ,"[BAHIR-101] Update sql-cloudant readme and python examples

Closes #40.", MODIFY README.md MODIFY CloudantApp.py MODIFY CloudantDF.py MODIFY CloudantDFOption.py
0,BAHIR-15,cad277e611388ad61e3c7fcb4e8a2e796d0e983d,Enable RAT on Bahir builds RAT check for license headers compliance on source code,[BAHIR-7] Update Apache Spark version to 2.0.0-preview, MODIFY pom.xml
0,BAHIR-152,f9a67de735fee8c89518cf37a513766c9e9e6b15,"License header not enforced for Java sources In any Java source file add this line on top of the license header:

{code}
/*
 * Copyright (c) 2017 ACME. All rights reserved.
 * <p>
{code}

then run {{mvn clean verify}} and see the build succeed.

Neither of the following maven goals complains either:
 - {{mvn apache-rat:check}}
 - {{mvn checkstyle:check}}

Adding a similar copyright statement to any of the Scala sources will fail the {{HeaderMatchesChecker}} rule in our {{scalastyle}} checks.

I am not sure why the [RAT|http://creadur.apache.org/rat/apache-rat-plugin/usage.html] checks allow this to pass, but I will add a similar {{Header}} verification to our {{checkstyle}} configuration.",[BAHIR-149] Update Cloudant dependency to release 2.11.0, MODIFY pom.xml
0,BAHIR-215,549c50be02f98b93c5f79890332e8de97332e8f5,bump flink to 1.9.0  ,"[BAHIR-217] Installation of Oracle JDK8 is Failing in Travis CI (#93)

Install of Oracle JDK 8 Failing in Travis CI and as a result, 
build is failing for new pull requests.

We just need to add `dist: trusty` in the .travis.yml file 
as mentioned in the issue below:
https://travis-ci.community/t/install-of-oracle-jdk-8-failing/3038", MODIFY .travis.yml
0,BAHIR-1,d32542483d08d74cfc898454ca3b4f68e3d155bc,"Import deleted Apache Spark Streaming connectors The following streaming connectors were deleted from Spark 2.0 code stream at revision 8301fadd8 :

Streaming connector for Akka
Streaming connector for MQTT
Streaming connector for Twitter
Streaming connector for ZeroMQ
",[BAHIR-2] Initial maven build for Bahir spark extensions, ADD checkstyle-suppressions.xml ADD checkstyle.xml ADD pom.xml ADD scalastyle-config.xml MODIFY pom.xml MODIFY ActorReceiver.scala MODIFY pom.xml MODIFY MQTTTestUtils.scala MODIFY pom.xml MODIFY TwitterInputDStream.scala MODIFY TwitterStreamSuite.scala MODIFY pom.xml MODIFY ZeroMQReceiver.scala
0,BAHIR-48,c78af705f5697ab11d93f933d033d96cc48403a0,Documentation improvements for Bahir README.md  ,[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
0,BAHIR-154,785ee1e1acfb129bb0524d79df3372968b9e95a7,"Refactor sql-cloudant to use Cloudant's java-cloudant features Cloudant's java-cloudant library (which is currently used for testing) contains several features that sql-cloudant can benefit from:
- HTTP 429 backoff
- View builder API to potentially simplify loading for _all_docs/views
- Improved exception handling when executing HTTP requests
- Future support for IAM API key

Would need to replace current scala HTTP library with OkHttp library, and also replace play-json with GSON library.","[BAHIR-138] Fix deprecation warning messages

- Imported ‘spark.implicits._’ to convert Spark RDD to Dataset
- Replaced deprecated `json(RDD[String])` with `json(Dataset[String])`

Closes #63", MODIFY DefaultSource.scala createRelation createRelation insert buildScan create createRelation
0,BAHIR-44,619936d39d18b7af45b7acec9af02b599a43b056,Add new sql-streaming-mqtt to distribution  ,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
0,BAHIR-40,9ad566815b8e2e654547d6022d20016025d49923,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],[BAHIR-44] Add new sql-streaming-mqtt to distribution profile, MODIFY pom.xml
1,BAHIR-64,d43dad21963d2ba338acc44d6233ff020cef7d38,"Add test that Akka streaming connector can receive data Add test cases that verify that the *Akka streaming connector* can receive streaming data.

See [BAHIR-63|https://issues.apache.org/jira/browse/BAHIR-63]","[BAHIR-64] add Akka streaming test (send/receive)

This PR adds the test suite AkkaStreamSuite.scala to
the streaming connector streaming-akka to test data
being sent and received.

Closes #24", MODIFY NOTICE MODIFY ActorReceiver.scala ADD AkkaStreamSuite.scala preStart
0,BAHIR-28,5e07303c65e2c88cd392691bdfe9f68391f51b5c,Add documentation for streaming-akka connector  ,"[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
1,BAHIR-14,7bc3d6e91ce8bf2b159032cf781a09e90854a19f,Cleanup maven pom from Spark dependencies There are a lot of dependencies that came from Spark and are not necessary for these extensions. We should cleanup the current poms and make the dependencies as lean as possible.,"[BAHIR-14] Cleanup Bahir parent pom

The Bahir parent pom was initially based on Spark parent pom
and was bringing a lot of unecessary dependencies. This commit
cleans most of the unused properties, dependencies, etc.

Closes #1", MODIFY pom.xml MODIFY ActorWordCount.scala MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-48,eab486427186cee3c0f7ed8e440971f67f7ed832,Documentation improvements for Bahir README.md  ,"[BAHIR-52] Update README.md formatting for source code

Update source code paragraphs to use tabs instead of ```
which is the supported way in vanilla Jekyll.", MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
0,BAHIR-125,c7f158d86634d602a19a4abfd873809f8ece9d03,Update Bahir pom to use JAVA 8 and to align with Spark 2.2.0 dependencies  ,"[BAHIR-110] Implement _changes API for sql-cloudant

 - support loading Cloudant data into Spark DataFrames and SQL tables
   using '_changes' endpoint
 - update README to explain the new config options and differences
   between '_all_docs' and '_changes' endpoints when loading data
 - Add test suite to test Spark DataFrames using the '_all_docs' and
   '_changes' endpoint, assert Cloudant config options, and test Spark
   SQL temporary views

Closes #45", MODIFY pom.xml MODIFY README.md MODIFY CloudantApp.scala main MODIFY CloudantDF.scala main MODIFY CloudantDFOption.scala MODIFY pom.xml MODIFY application.conf ADD CloudantChangesConfig.scala MODIFY CloudantConfig.scala getTotalUrl getBulkRows getContinuousChangesUrl calculateCondition getCreateDBonSave getSchemaSampleSize getSelector calculate getRangeUrl getAllDocsUrl getSubSetUrl allowPartition getBulkPostUrl queryEnabled getDbUrl getDbname getConflictErrStr getTotalRows getForbiddenErrStr getRows getLastNum getUrl MODIFY CloudantReceiver.scala receive onStop MODIFY DefaultSource.scala buildScan create insert ADD CloudantException.scala this this this MODIFY FilterUtil.scala filter MODIFY JsonStoreConfigManager.scala getString getStorageLevel getInt getLong getConfig getBool MODIFY JsonStoreDataAccess.scala saveAll getClPostRequest createDB processAll getMany getClRequest convert MODIFY JsonStoreRDD.scala compute getTotalPartition MODIFY JsonUtil.scala ADD ChangesReceiver.scala receive run onStart onStop ADD n_airportcodemapping.json ADD n_booking.json ADD n_customer.json ADD n_customersession.json ADD n_flight.json ADD n_flightsegment.json ADD log4j.properties ADD ClientSparkFunSuite.scala deleteTestDb runIfTestsEnabled setupClient testIfEnabled createTestDbs afterAll beforeAll teardownClient deleteTestDbs ADD CloudantAllDocsDFSuite.scala beforeAll ADD CloudantChangesDFSuite.scala ADD CloudantOptionSuite.scala ADD CloudantSparkSQLSuite.scala beforeAll ADD TestUtils.scala deleteRecursively
0,BAHIR-96,f0d9a84f76cb34a432e1d2db053d2471a8ab2ba4,"Add a ""release-build.sh"" script for bahir-flink We need to adopt the {{release-build.sh}} script from the Bahir Spark repo, in order to kick off the first Bahir Flink extensions release.","[BAHIR-101] Spark SQL datasource for CounchDB/Cloudant

Initial code supporting CounchDB/Cloudant as an Spark SQL
data source. The initial source contains the core connector,
examples, and basic documentation on the README.

Closes #39.", MODIFY README.md MODIFY pom.xml ADD README.md ADD CloudantApp.py ADD CloudantDF.py ADD CloudantDFOption.py ADD CloudantApp.scala main ADD CloudantDF.scala main ADD CloudantDFOption.scala main ADD CloudantStreaming.scala getInstance main ADD CloudantStreamingSelector.scala main ADD pom.xml ADD application.conf ADD reference.conf ADD CloudantConfig.scala getRangeUrl getDbname getTotalRows getForbiddenErrStr getConflictErrStr getOneUrlExcludeDDoc1 getSubSetUrl allowPartition getLastNum getTotalUrl getBulkRows calculateCondition getCreateDBonSave getLastUrl getContinuousChangesUrl getChangesUrl calculate getAllDocsUrlExcludeDDoc getOneUrl getSelector getAllDocsUrl getSchemaSampleSize getRows getBulkPostUrl getDbUrl getOneUrlExcludeDDoc2 ADD CloudantReceiver.scala onStart onStop receive run ADD DefaultSource.scala buildScan createRelation create insert createRelation createRelation ADD FilterUtil.scala filter containsFiltersFor apply analyze evaluate getFiltersForPostProcess getFilterAttribute getInfo ADD JsonStoreConfigManager.scala getString getConfig getConfig getBool getLong getInt ADD JsonStoreDataAccess.scala getChanges convert convertSkip getOne getClPostRequest saveAll getTotalRows createDB processAll getMany getIterator processIterator ADD JsonStoreRDD.scala compute ADD JsonUtil.scala getField
1,BAHIR-88,6d9a4d7ab0c1eff0bf63e91cec32b601c263f790,"Source distribution contains release build artifacts There are 'releaseBackup' files and 'release.properties' in the 'source distribution' which are artifacts of the release build process.
We appear to be missing to run the mvn {{release:clean}} target _after_
the {{release:prepare}} target. We should fix that for the next release. All prior
Apache Bahir source distributions contained these release build artifacts.

[Mailing list post|https://www.mail-archive.com/dev@bahir.apache.org/msg00735.html]

Fixed by Luciano on June 7 with commits: [ba68b35|https://github.com/apache/bahir/commit/ba68b3587ad4011a093bcaad921035f26907967c], [6d9a4d7|https://github.com/apache/bahir/commit/6d9a4d7ab0c1eff0bf63e91cec32b601c263f790], [dcb4bbd|https://github.com/apache/bahir/commit/dcb4bbd2e4d75bc0872ce32c159b03a1d0f90047]",[BAHIR-88] Additional fixes to produce proper rc distribution, MODIFY release-build.sh
0,BAHIR-41,91e82f42bac58fd8d912e892a5ebfca79f6b8268,[WEBSITE] Fix logo display in small devices Reported by [~jreijn],[BAHIR-37] Update Spark to release 2.0.0, MODIFY pom.xml
0,BAHIR-43,70539a35dc9bae9ee5d380351ffc32fa6e62567e,Add missing apache license header to sql-mqtt file  ,"[BAHIR-42] Refactor sql-streaming-mqtt example

Move JavaMQTTStreamWordCount to examples root folder
which are processed by the build as test resources
and not built into the extension itself following
the pattern used by other examples.", RENAME JavaMQTTStreamWordCount.java main
0,BAHIR-47,c78af705f5697ab11d93f933d033d96cc48403a0,Bring up release download for Apache Bahir website  ,[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
0,BAHIR-138,0e1505a8960bfe40ea025267bbf36ec5c4cf5c79,"Fix sql-cloudant deprecation messages Deprecation warnings in {{DefaultSource}}:

{code}
[INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ spark-sql-cloudant_2.11 ---
[INFO] Compiling 11 Scala sources to sql-cloudant/target/scala-2.11/classes...
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:59: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]         val df = sqlContext.read.json(cloudantRDD)
[WARNING]                                  ^
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:115: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]             dataFrame = sqlContext.read.json(cloudantRDD)
[WARNING]                                         ^
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:121: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]             sqlContext.read.json(aRDD)
[WARNING]                             ^
[WARNING] sql-cloudant/src/main/scala/org/apache/bahir/cloudant/DefaultSource.scala:152: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]               dataFrame = sqlContext.sparkSession.read.json(globalRDD)
[WARNING]                                                        ^
[WARNING] four warnings found
{code}


Deprecation warnings in {{CloudantStreaming}} and {{CloudantStreamingSelector}} examples:

{code}
[INFO] --- scala-maven-plugin:3.2.2:testCompile (scala-test-compile-first) @ spark-sql-cloudant_2.11 ---
[INFO] Compiling 11 Scala sources to sql-cloudant/target/scala-2.11/test-classes...
[WARNING] sql-cloudant/examples/src/main/scala/org/apache/spark/examples/sql/cloudant/CloudantStreaming.scala:46: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]       val changesDataFrame = spark.read.json(rdd)
[WARNING]                                         ^
[WARNING] sql-cloudant/examples/src/main/scala/org/apache/spark/examples/sql/cloudant/CloudantStreaming.scala:67: method registerTempTable in class Dataset is deprecated: Use createOrReplaceTempView(viewName) instead.
[WARNING]           changesDataFrame.registerTempTable(""airportcodemapping"")
[WARNING]                            ^
[WARNING] sql-cloudant/examples/src/main/scala/org/apache/spark/examples/sql/cloudant/CloudantStreamingSelector.scala:50: method json in class DataFrameReader is deprecated: Use json(Dataset[String]) instead.
[WARNING]       val changesDataFrame = spark.read.json(rdd)
[WARNING]                                         ^
[WARNING] three warnings found
{code}","[BAHIR-128] Improve sql-cloudant _changes receiver

This change improves the stability of _changes receiver and
fix the intermitent failing test in sql-cloudant's
CloudantChangesDFSuite.

How

Improve performance and decrease testing time by setting batch size
to 8 seconds and using seq_interval _changes feed option.
Use getResource to load json files path
Added Mike Rhodes's ChangesRowScanner for reading each _changes line
and transforming to GSON's JSON object
Added Mike Rhodes's ChangesRow representing a row in the changes feed

Closes #57", ADD ChangesRow.java getDoc Rev getRev getSeq getId getChanges ADD ChangesRowScanner.java readRowFromReader MODIFY CloudantChangesConfig.scala MODIFY DefaultSource.scala create MODIFY JsonStoreDataAccess.scala MODIFY ChangesReceiver.scala receive MODIFY ClientSparkFunSuite.scala createTestDbs MODIFY CloudantChangesDFSuite.scala
0,BAHIR-44,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,Add new sql-streaming-mqtt to distribution  ,[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
0,BAHIR-39,95633de6741ddf757cc4964425463a972e1b4cbe,MQTT as a streaming source for SQL Streaming. MQTT compatible streaming source for Spark SQL Streaming.,[BAHIR-43] Add Apache License header file, MODIFY pom.xml MODIFY org.apache.spark.sql.sources.DataSourceRegister
0,BAHIR-16,86ee8779c7980e10c1c246fff7a171d0118c0239,"Build issues due to log4j properties not found log4j:ERROR Could not read configuration file from URL [file:src/test/resources/log4j.properties].
java.io.FileNotFoundException: src/test/resources/log4j.properties (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:81)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:155)
	at org.apache.commons.logging.impl.SLF4JLogFactory.getInstance(SLF4JLogFactory.java:132)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:273)
	at org.apache.hadoop.util.ShutdownHookManager.<clinit>(ShutdownHookManager.java:44)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:179)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:152)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:57)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.streaming.StreamingContext$.<init>(StreamingContext.scala:730)
	at org.apache.spark.streaming.StreamingContext$.<clinit>(StreamingContext.scala)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:100)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:63)
	at org.apache.spark.streaming.akka.JavaAkkaUtilsSuite.testAkkaUtils(JavaAkkaUtilsSuite.java:36)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
log4j:ERROR Ignoring configuration file [file:src/test/resources/log4j.properties].","[BAHIR-15] Enable RAT on builds

Enable RAT to run automatically during builds
to verify license header policy.", MODIFY pom.xml
0,BAHIR-44,1abeab29c8a5e884f4603ef12abd85971a9105b0,Add new sql-streaming-mqtt to distribution  ,[BAHIR-42] Refactor sql-streaming-mqtt scala example, RENAME MQTTStreamWordCount.scala
0,BAHIR-21,c277d4902e7538609523eee0ded3950e0d14d260,Create script to change build between scala 2.10 and 2.11  ,"[BAHIR-18] Configure examples as maven test sources

This PR configure examples as maven test resources to be
recognized by maven builds. This acomplish the following :

- The examples get compiled
- IDEs like IntelliJ or Eclipse recognize the
  <module>/examples/src/[java|scala] as source folders
- Keep the examples along with their additional dependencies
  excluded from the generated binaries

Closes #2", MODIFY pom.xml MODIFY JavaActorWordCount.java main MODIFY ActorWordCount.scala main MODIFY JavaAkkaUtilsSuite.java onReceive MODIFY MQTTWordCount.scala main MODIFY package.scala ADD AFINN-111.txt MODIFY JavaTwitterHashTagJoinSentiments.java main MODIFY TwitterAlgebirdCMS.scala main MODIFY TwitterAlgebirdHLL.scala main MODIFY TwitterHashTagJoinSentiments.scala main MODIFY TwitterPopularTags.scala main MODIFY pom.xml MODIFY package.scala MODIFY ZeroMQWordCount.scala main MODIFY package.scala
1,BAHIR-13,fc1ef7f990880cad8ce69300bb9bbbb8ad260050,Update spark tags dependency Looks like the spark-test-tags component was merged into spark-tags in 8ad9f08c9. We need to replace the spark-test-tags in bahir modules dependencies to spark-tags.,"[BAHIR-13] Update dependencies on spark-tags

The spark-test-tags and spark-tags were merged in
revision 8ad9f08c9 and the modules in Bahir needs
to be updated to properly use spark-tags dependency.", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-21,13b127593e79debfbc97a9c1215198c780df50a4,Create script to change build between scala 2.10 and 2.11  ,"[BAHIR-19] Create source distribution assembly

Add assemblie to create Bahir source release distribution", ADD pom.xml ADD src.xml MODIFY pom.xml
0,BAHIR-162,70f6ba0e4430d0e0773b815c52f4a256e30e0234,Update release scripts to stop publishing MD5 hash (updated release policy)  ,[BAHIR-163] Enable builds using Travis CI, ADD .travis.yml
0,BAHIR-42,619936d39d18b7af45b7acec9af02b599a43b056,Refactor sql-streaming-mqtt examples to follow other projects pattern  ,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
0,BAHIR-47,31b3e96d4680c54ff05d4b09f944106d22b62760,Bring up release download for Apache Bahir website  ,[BAHIR-52] Fix code paragraph formatting for Akka readme, MODIFY README.md
0,BAHIR-154,0e1505a8960bfe40ea025267bbf36ec5c4cf5c79,"Refactor sql-cloudant to use Cloudant's java-cloudant features Cloudant's java-cloudant library (which is currently used for testing) contains several features that sql-cloudant can benefit from:
- HTTP 429 backoff
- View builder API to potentially simplify loading for _all_docs/views
- Improved exception handling when executing HTTP requests
- Future support for IAM API key

Would need to replace current scala HTTP library with OkHttp library, and also replace play-json with GSON library.","[BAHIR-128] Improve sql-cloudant _changes receiver

This change improves the stability of _changes receiver and
fix the intermitent failing test in sql-cloudant's
CloudantChangesDFSuite.

How

Improve performance and decrease testing time by setting batch size
to 8 seconds and using seq_interval _changes feed option.
Use getResource to load json files path
Added Mike Rhodes's ChangesRowScanner for reading each _changes line
and transforming to GSON's JSON object
Added Mike Rhodes's ChangesRow representing a row in the changes feed

Closes #57", ADD ChangesRow.java getDoc Rev getRev getSeq getId getChanges ADD ChangesRowScanner.java readRowFromReader MODIFY CloudantChangesConfig.scala MODIFY DefaultSource.scala create MODIFY JsonStoreDataAccess.scala MODIFY ChangesReceiver.scala receive MODIFY ClientSparkFunSuite.scala createTestDbs MODIFY CloudantChangesDFSuite.scala
0,BAHIR-44,c78af705f5697ab11d93f933d033d96cc48403a0,Add new sql-streaming-mqtt to distribution  ,[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
0,BAHIR-39,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,MQTT as a streaming source for SQL Streaming. MQTT compatible streaming source for Spark SQL Streaming.,[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
0,BAHIR-54,28f034f49d19034b596f7f04ca4fc2698a21ad6c,"Create initial directory structure in bahir-flink.git As per INFRA-12440, the bahir-flink repository has been created.

We need to set up the initial directory structure in that repository (license file, maven pom, ...)","[BAHIR-53] Add new configuration options to MQTTInputDStream

Add new configuration options to enable secured connections and
other quality of services.

Closes #23", MODIFY scalastyle-config.xml MODIFY README.md MODIFY MQTTInputDStream.scala onStart getReceiver MODIFY MQTTUtils.scala createStream createStream createStream createStream createStream createStream createStream MODIFY JavaMQTTStreamSuite.java testMQTTStream
0,BAHIR-28,c78af705f5697ab11d93f933d033d96cc48403a0,Add documentation for streaming-akka connector  ,[BAHIR-30] Add basic documentation for Twitter connector, ADD README.md
1,BAHIR-2,d32542483d08d74cfc898454ca3b4f68e3d155bc,"Create initial build for Bahir components After importing the deleted spark components, we need to create the initial components build.",[BAHIR-2] Initial maven build for Bahir spark extensions, ADD checkstyle-suppressions.xml ADD checkstyle.xml ADD pom.xml ADD scalastyle-config.xml MODIFY pom.xml MODIFY ActorReceiver.scala MODIFY pom.xml MODIFY MQTTTestUtils.scala MODIFY pom.xml MODIFY TwitterInputDStream.scala MODIFY TwitterStreamSuite.scala MODIFY pom.xml MODIFY ZeroMQReceiver.scala
1,BAHIR-122,e491610d8a7a9cff1ed5087c1c0cbe6b2c29eb39,"[PubSub] Make ""ServiceAccountCredentials"" really broadcastable The origin implementation broadcast the key file path to Spark cluster, then the executor read key file with the broadcasted path. Which is absurd, if you are using a shared Spark cluster in a group/company, you certainly not want to (and have no right to) put your key file on each instance of the cluster.

If you store the key file on driver node and submit your job to a remote cluster. You would get the following warning:
{{WARN ReceiverTracker: Error reported by receiver for stream 0: Failed to pull messages - java.io.FileNotFoundException}}","[BAHIR-122][PubSub] Make ""ServiceAccountCredentials"" really broadcastable

Instead of requiring key files on each instance of the cluster, we read
the key file content on the driver node and store the binary in the
ServiceAccountCredentials. When the provider is called, it retrieves the
credential with the in-memory key file.

Closes #48", MODIFY README.md MODIFY pom.xml MODIFY SparkGCPCredentials.scala MODIFY SparkGCPCredentialsBuilderSuite.scala
1,BAHIR-52,eab486427186cee3c0f7ed8e440971f67f7ed832,"Update extension documentation formats for code sections The ```md format is not working properly for pure jekyll html generation, and the tab seems to be the supported way in vanilla jekyll. We should update Bahir extension readme to use the supported format.","[BAHIR-52] Update README.md formatting for source code

Update source code paragraphs to use tabs instead of ```
which is the supported way in vanilla Jekyll.", MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md MODIFY README.md
0,BAHIR-48,c98dd0feefa79c70be65de06a411a2f9c4fc42dc,Documentation improvements for Bahir README.md  ,"[BAHIR-39] Add SQL Streaming MQTT support

This provides support for using MQTT sources for
the new Spark Structured Streaming. This uses
MQTT client persistence layer to provide minimal
fault tolerance.

Closes #13", MODIFY pom.xml ADD README.md ADD pom.xml ADD assembly.xml ADD JavaMQTTStreamWordCount.java main ADD org.apache.spark.sql.sources.DataSourceRegister ADD MQTTStreamSource.scala stop connectionLost messageArrived fetchLastProcessedOffset createSource initialize deliveryComplete getBatch shortName sourceSchema connectComplete e ADD MessageStore.scala get this ADD MQTTStreamWordCount.scala main ADD BahirUtils.scala postVisitDirectory visitFile recursiveDeleteDir ADD Logging.scala ADD log4j.properties ADD LocalMessageStoreSuite.scala ADD MQTTStreamSourceSuite.scala createStreamingDataframe writeStreamResults readBackStreamingResults ADD MQTTTestUtils.scala setup publishData teardown findFreePort
1,BAHIR-88,dcb4bbd2e4d75bc0872ce32c159b03a1d0f90047,"Source distribution contains release build artifacts There are 'releaseBackup' files and 'release.properties' in the 'source distribution' which are artifacts of the release build process.
We appear to be missing to run the mvn {{release:clean}} target _after_
the {{release:prepare}} target. We should fix that for the next release. All prior
Apache Bahir source distributions contained these release build artifacts.

[Mailing list post|https://www.mail-archive.com/dev@bahir.apache.org/msg00735.html]

Fixed by Luciano on June 7 with commits: [ba68b35|https://github.com/apache/bahir/commit/ba68b3587ad4011a093bcaad921035f26907967c], [6d9a4d7|https://github.com/apache/bahir/commit/6d9a4d7ab0c1eff0bf63e91cec32b601c263f790], [dcb4bbd|https://github.com/apache/bahir/commit/dcb4bbd2e4d75bc0872ce32c159b03a1d0f90047]",[BAHIR-88] Add release:prepare statement back to script, MODIFY release-build.sh
1,BAHIR-29,619936d39d18b7af45b7acec9af02b599a43b056,Add documentation for streaming-mqtt connector  ,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
0,BAHIR-102,889de659c33dd56bad7193a4b69e6d05d061a2fd,"Support pushing down query predicates using Cloudant Query / CouchDB Mango Query The feature of Cloudant Query / ChoudDB Mango Query is stated here: https://blog.couchdb.org/2016/08/03/feature-mango-query/.

Will enable pushing down first level AND column filtering to _find selector to replace _all_docs for the database query; will continue using skip+limit to offer partition and parallel loading.","[BAHIR-97] Akka as SQL Streaming datasource.

Closes #38.", MODIFY pom.xml ADD README.md ADD JavaAkkaStreamWordCount.java main ADD AkkaStreamWordCount.scala main ADD pom.xml ADD assembly.xml ADD AkkaStreamSource.scala store stop store store preStart fetchLastProcessedOffset createSource postStop getBatch e shortName close sourceSchema receive getOrCreatePersistenceInstance ADD MessageStore.scala get this ADD BahirUtils.scala visitFile postVisitDirectory recursiveDeleteDir ADD Logging.scala ADD feeder_actor.conf ADD log4j.properties ADD AkkaStreamSourceSuite.scala readBackSreamingResults writeStreamResults createStreamingDataframe ADD AkkaTestUtils.scala setup setCountOfMessages run getFeederActorUri getFeederActorConfig setMessage shutdown
0,BAHIR-40,619936d39d18b7af45b7acec9af02b599a43b056,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
0,BAHIR-156,eae02f29eb011f50bc313714e6cde62ce65804c4,Improve integration test stability for flink-library-siddhi  ,"[BAHIR-138] fix deprecated warnings in sql-cloudant

Fix warnings in DefaultSource class, and in CloudantStreaming
and CloudantStreamingSelector examples.

How

Imported spark.implicits._ to convert Spark RDD to Dataset
Replaced deprecated json(RDD[String]) with json(Dataset[String])
Improved streaming examples:

Replaced registerTempTable with preferred createOrReplaceTempView
Replaced !isEmpty with nonEmpty
Use an accessible sales database so users can run the example without any setup
Fixed error message when stopping tests by adding logic to streaming
receiver to not store documents in Spark memory when stream has stopped

Closes #59", MODIFY CloudantStreaming.scala getInstance main MODIFY CloudantStreamingSelector.scala main MODIFY CloudantReceiver.scala receive
1,BAHIR-61,415576ba702206ba9cfc5c8bdbdee4869a1e52ac,"Enable release script to publish release by a specific rc tag Enable calling the release script like :
release-build.sh --release-publish --gitTag=""v2.0.0rc1""

Which will then checkout the specific rc tag and publish maven artifacts from that tag to the maven staging repo.","[BAHIR-61] Enable publishing release artifacts from a tag

Enable a --gitTag parameter to identify an RC tag to be used
when publishing artifacts to maven.", MODIFY release-build.sh
0,BAHIR-47,619936d39d18b7af45b7acec9af02b599a43b056,Bring up release download for Apache Bahir website  ,[BAHIR-29] Add basic documentation for MQTT Connector, ADD README.md
1,BAHIR-19,1028736bb79a6b7c66789bc77ca97c44020e62af,Create Bahir source distribution  ,"[BAHIR-19] Update source distribution assembly name

Update final assembly name and extraction directory
to use apache best practice pattern :

apache-bahir-${project.version}-src", MODIFY pom.xml MODIFY src.xml
0,BAHIR-125,e491610d8a7a9cff1ed5087c1c0cbe6b2c29eb39,Update Bahir pom to use JAVA 8 and to align with Spark 2.2.0 dependencies  ,"[BAHIR-122][PubSub] Make ""ServiceAccountCredentials"" really broadcastable

Instead of requiring key files on each instance of the cluster, we read
the key file content on the driver node and store the binary in the
ServiceAccountCredentials. When the provider is called, it retrieves the
credential with the in-memory key file.

Closes #48", MODIFY README.md MODIFY pom.xml MODIFY SparkGCPCredentials.scala MODIFY SparkGCPCredentialsBuilderSuite.scala
0,BAHIR-214,68ac1be22ddccecf105aee355a8c2652868e9f7d,"Improve KuduConnector speed kudu connector has some issues on kudu sink with some flush modes that kill sink over time

 

this is a refactor to resolve that issues and improve speed on eventual consistence",[BAHIR-172 ] Replace FileInputStream with Files.newInputStream (#92), MODIFY MQTTTestUtils.scala setup MODIFY SparkGCPCredentials.scala
0,BAHIR-13,cad277e611388ad61e3c7fcb4e8a2e796d0e983d,Update spark tags dependency Looks like the spark-test-tags component was merged into spark-tags in 8ad9f08c9. We need to replace the spark-test-tags in bahir modules dependencies to spark-tags.,[BAHIR-7] Update Apache Spark version to 2.0.0-preview, MODIFY pom.xml
0,BAHIR-15,7bc3d6e91ce8bf2b159032cf781a09e90854a19f,Enable RAT on Bahir builds RAT check for license headers compliance on source code,"[BAHIR-14] Cleanup Bahir parent pom

The Bahir parent pom was initially based on Spark parent pom
and was bringing a lot of unecessary dependencies. This commit
cleans most of the unused properties, dependencies, etc.

Closes #1", MODIFY pom.xml MODIFY ActorWordCount.scala MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-17,2dfcd08d11e94b535a39c31c87cf690f99944357,Prepare release based on Apache Spark 2.0.0-preview  ,"[[BAHIR-14] More parent pom cleanup

Remove Spark assembly related configuration, and
stop producing source jars for non-jar projects.", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
1,BAHIR-165,b3902bac67edc2134bcc2c755fadc5c60c8ae01c,The avro messages to streaming-mqtt gives negative value.  Sending an Avro message with fields id and name to streaming-mqtt. The messages gets parsed but the value of id is returned as some random negative number. The result is negative only when the id input is greater than 63.,"[BAHIR-164][BAHIR-165] Port Mqtt sql source to datasource v2 API

Migrating Mqtt spark structured streaming connector to DatasourceV2 API.

Closes #65", MODIFY pom.xml MODIFY AkkaStreamSourceSuite.scala MODIFY README.md MODIFY JavaMQTTStreamWordCount.java main MODIFY MQTTStreamWordCount.scala main ADD LongOffset.scala + apply - convert MODIFY MQTTStreamSource.scala e sourceSchema getEndOffset initialize readSchema createDataReaderFactories next setOffsetRange createMicroBatchReader fetchLastProcessedOffset createDataReader toString commit stop messageArrived getStartOffset deserializeOffset createSource get getBatch close MODIFY MessageStore.scala this get get this getInstance ADD test-BAHIR-83.sh MODIFY LocalMessageStoreSuite.scala MODIFY MQTTStreamSourceSuite.scala readBackStreamingResults writeStreamResults createStreamingDataframe createStreamingDataframe writeStreamResults MODIFY MQTTTestUtils.scala publishData teardown
0,BAHIR-181,b3902bac67edc2134bcc2c755fadc5c60c8ae01c,"username and password should be available for pyspark when using mqtt streaming When using spark-streaming-mqtt with pyspark to access rabbitmq, I  found there are no username and password provied for python api;

These two params are important and necessary for rabbitmq especially when using rabbitmq virtual hosts, so I added a group of functions here;","[BAHIR-164][BAHIR-165] Port Mqtt sql source to datasource v2 API

Migrating Mqtt spark structured streaming connector to DatasourceV2 API.

Closes #65", MODIFY pom.xml MODIFY AkkaStreamSourceSuite.scala MODIFY README.md MODIFY JavaMQTTStreamWordCount.java main MODIFY MQTTStreamWordCount.scala main ADD LongOffset.scala + apply - convert MODIFY MQTTStreamSource.scala e sourceSchema getEndOffset initialize readSchema createDataReaderFactories next setOffsetRange createMicroBatchReader fetchLastProcessedOffset createDataReader toString commit stop messageArrived getStartOffset deserializeOffset createSource get getBatch close MODIFY MessageStore.scala this get get this getInstance ADD test-BAHIR-83.sh MODIFY LocalMessageStoreSuite.scala MODIFY MQTTStreamSourceSuite.scala readBackStreamingResults writeStreamResults createStreamingDataframe createStreamingDataframe writeStreamResults MODIFY MQTTTestUtils.scala publishData teardown
0,BAHIR-23,76bfd8b2509128ae2a4ed7f59bcf4dac75c0296a,"Build should fail on Checkstyle violation Currently the maven build will fail for code style violations in Scala files but the build will succeed regardless of code style violations in Java files.

{code:xml}
      <plugin>
        ...
        <artifactId>scalastyle-maven-plugin</artifactId>
        ...
        <configuration>
          ...
          <failOnViolation>true</failOnViolation>
          ...
        </configuration>
        ...
      </plugin>
      <plugin>
        ...
        <artifactId>maven-checkstyle-plugin</artifactId>
        ...
        <configuration>
          ...
          <failOnViolation>false</failOnViolation>
          ...
        </configuration>
        ...
      </plugin>
{code}

As a consequence potential problems in the Java code may not get noticed.","[BAHIR-20] Create release helper scripts

Release script to automate :
- Preparing release artifacts
- Publishing maven artifacts in Scala 2.10 and 2.11
- Publishing snapshots", ADD release-build.sh
0,BAHIR-123,a70ff538ac48ac1576984304d273e7a1f25fc2a6,"Fix errors to support the latest version of Play JSON library for sql-cloudant The latest version is 2.6.2.  Error during mvn install -pl sql-cloudant after updating play-json to 2.6.2 in sql-cloudant/pom.xml:

[ERROR] /Users/estebanmlaver/emlaver-bahir/sql-cloudant/src/main/scala/org/apache/bahir/cloudant/common/JsonStoreConfigManager.scala:19: object typesafe is not a member of package com
[ERROR] import com.typesafe.config.ConfigFactory
[ERROR]            ^
[ERROR] /Users/estebanmlaver/emlaver-bahir/sql-cloudant/src/main/scala/org/apache/bahir/cloudant/common/JsonStoreConfigManager.scala:52: not found: value ConfigFactory
[ERROR]   private val configFactory = ConfigFactory.load()
[ERROR]                               ^
[ERROR] two errors found

Maven compile dependencies need to be added to pom.xml that existed in play-json 2.5.9 but were removed in 2.6.2.

Additional info. from Patrick Titzler between play-json versions 2.5.x and 2.6.x:
Looks like the parameter data type has been changed from `Seq[JsValue]` (https://www.playframework.com/documentation/2.5.x/api/scala/index.html#play.api.libs.json.JsArray) to `IndexedSeq[JsValue]` https://playframework.com/documentation/2.6.x/api/scala/index.html#play.api.libs.json.JsArray","[BAHIR-125] Update Bahir parent pom

- Default build using JAVA 8
- Update dependencies to align with Spark 2.2.0", MODIFY pom.xml
0,BAHIR-37,31b3e96d4680c54ff05d4b09f944106d22b62760,Prepare release based on Apache Spark 2.0.0  ,[BAHIR-52] Fix code paragraph formatting for Akka readme, MODIFY README.md
0,BAHIR-150,55c60e5dd25c7c696118d2f2c8760fe5a17c1354,"Jenkins PR builder should not abort build after first failed module Currently our Jenkins build skips building any further modules after the first build or test failure (see [PR #55|https://github.com/apache/bahir/pull/55#issuecomment-349487768])

We should change the Jenking PR build configuration to continue running the Maven build and report any/all failures after building and testing all modules.","[BAHIR-148] Use consistent MQTT client dependency version

Create a property to use a consistent version of the MQTT
client across all extensions based on MQTT.

For now, use org.eclipse.paho.client.mqttv3:1.1.0", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-40,5e07303c65e2c88cd392691bdfe9f68391f51b5c,[WEBSITE] Fix navigation toggle for small displays Reported by [~jreijn],"[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
0,BAHIR-128,eae02f29eb011f50bc313714e6cde62ce65804c4,"Test failing sporadically in sql-cloudant's CloudantChangesDFSuite This failure happened during pre-release testing for Bahir RC 2.2.0:
CloudantChangesDFSuite:
- load and save data from Cloudant database *** FAILED ***
  0 did not equal 1967 (CloudantChangesDFSuite.scala:49)

Partial stack trace:
{code:java}
Exception in thread ""Cloudant Receiver"" org.apache.spark.SparkException: Cannot add data as BlockGenerator has not been started or has been stopped
    at org.apache.spark.streaming.receiver.BlockGenerator.addData(BlockGenerator.scala:173)
    at org.apache.spark.streaming.receiver.ReceiverSupervisorImpl.pushSingle(ReceiverSupervisorImpl.scala:120)
    at org.apache.spark.streaming.receiver.Receiver.store(Receiver.scala:119)
    at org.apache.bahir.cloudant.internal.ChangesReceiver$$anonfun$org$apache$bahir$cloudant$internal$ChangesReceiver$$receive$1$$anonfun$apply$1.apply(ChangesReceiver.scala:82)
{code}","[BAHIR-138] fix deprecated warnings in sql-cloudant

Fix warnings in DefaultSource class, and in CloudantStreaming
and CloudantStreamingSelector examples.

How

Imported spark.implicits._ to convert Spark RDD to Dataset
Replaced deprecated json(RDD[String]) with json(Dataset[String])
Improved streaming examples:

Replaced registerTempTable with preferred createOrReplaceTempView
Replaced !isEmpty with nonEmpty
Use an accessible sales database so users can run the example without any setup
Fixed error message when stopping tests by adding logic to streaming
receiver to not store documents in Spark memory when stream has stopped

Closes #59", MODIFY CloudantStreaming.scala getInstance main MODIFY CloudantStreamingSelector.scala main MODIFY CloudantReceiver.scala receive
1,BAHIR-44,9ad566815b8e2e654547d6022d20016025d49923,Add new sql-streaming-mqtt to distribution  ,[BAHIR-44] Add new sql-streaming-mqtt to distribution profile, MODIFY pom.xml
0,BAHIR-15,fc1ef7f990880cad8ce69300bb9bbbb8ad260050,Enable RAT on Bahir builds RAT check for license headers compliance on source code,"[BAHIR-13] Update dependencies on spark-tags

The spark-test-tags and spark-tags were merged in
revision 8ad9f08c9 and the modules in Bahir needs
to be updated to properly use spark-tags dependency.", MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml MODIFY pom.xml
0,BAHIR-42,5e07303c65e2c88cd392691bdfe9f68391f51b5c,Refactor sql-streaming-mqtt examples to follow other projects pattern  ,"[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
0,BAHIR-19,c277d4902e7538609523eee0ded3950e0d14d260,Create Bahir source distribution  ,"[BAHIR-18] Configure examples as maven test sources

This PR configure examples as maven test resources to be
recognized by maven builds. This acomplish the following :

- The examples get compiled
- IDEs like IntelliJ or Eclipse recognize the
  <module>/examples/src/[java|scala] as source folders
- Keep the examples along with their additional dependencies
  excluded from the generated binaries

Closes #2", MODIFY pom.xml MODIFY JavaActorWordCount.java main MODIFY ActorWordCount.scala main MODIFY JavaAkkaUtilsSuite.java onReceive MODIFY MQTTWordCount.scala main MODIFY package.scala ADD AFINN-111.txt MODIFY JavaTwitterHashTagJoinSentiments.java main MODIFY TwitterAlgebirdCMS.scala main MODIFY TwitterAlgebirdHLL.scala main MODIFY TwitterHashTagJoinSentiments.scala main MODIFY TwitterPopularTags.scala main MODIFY pom.xml MODIFY package.scala MODIFY ZeroMQWordCount.scala main MODIFY package.scala
0,BAHIR-35,12f130846ef7523138e98e79bfd823f61acab3b3,"Include Python code in the binary jars for use with ""--packages ..."" Currently, to make use the PySpark code (i.e streaming-mqtt/python) a user will have to download the jar from Maven central or clone the code from GitHub and then have to find individual *.py files, create a zip and add that to the {{spark-submit}} command with the {{--py-files}} option, or, add them to the {{PYTHONPATH}} when running locally.

If we include the Python code in the binary build (to the jar that gets uploaded to Maven central), then users need not do any acrobatics besides using the {{--packages ...}} option.

An example where the Python code is part of the binary jar is the [GraphFrames|https://spark-packages.org/package/graphframes/graphframes] package.","[BAHIR-24] fix MQTT Python code, examples, add tests

Changes in this PR:

- remove unnecessary files from streaming-mqtt/python
- updated all *.py files with respect to the modified
  project structure pyspark.streaming.mqtt --> mqtt
- add test cases that were left out from the import and
  add shell script to run them:
    - streaming-mqtt/python-tests/run-python-tests.sh
    - streaming-mqtt/python-tests/tests.py
- modify MQTTTestUtils.scala to limit the required disk storage space
- modify bin/run-example script to setup PYTHONPATH to run Python examples

Closes #10", MODIFY .gitignore MODIFY run-example MODIFY pom.xml MODIFY mqtt_wordcount.py ADD run-python-tests.sh ADD tests.py tearDown _startContext test_mqtt_stream _retry_or_timeout _randomTopic test_mqtt_stream.retry setUp _startContext.getOutput DELETE __init__.py DELETE dstream.py rightOuterJoin countByValue pprint _validate_window_param glom.func reduceByWindow checkpoint mapValues repartition __init__ mapPartitionsWithIndex reduceByKeyAndWindow.invReduceFunc countByValueAndWindow glom updateStateByKey.reduceFunc _jdstream mapPartitions persist mapPartitions.func flatMapValues saveAsTextFiles.saveAsTextFile context flatMap combineByKey combineByKey.func groupByKeyAndWindow map.func groupByKey cogroup updateStateByKey filter reduce partitionBy transformWith reduceByKeyAndWindow.reduceFunc pprint.takeAndPrint countByWindow cache map reduceByKey reduceByKeyAndWindow transform flatMap.func join _jtime leftOuterJoin count filter.func saveAsTextFiles window _slideDuration slice fullOuterJoin __init__ union foreachRDD MODIFY mqtt.py _get_helper _printErrorMsg createStream MODIFY MQTTTestUtils.scala setup
0,BAHIR-27,858ad27ad2766ab85e8c41e0cfa45162e9f7e308,Add documentation for existing streaming connectors  ,[BAHIR-28] Add basic documentation for Akka connector, ADD README.md
0,BAHIR-189,a45bd84210b8e68640f97dd328e7e7053c8276e6,Update Siddhi version to 4.3 for flink-library-siddhi. Siddhi library was released in version 4.3. We should update the dependency for flink-library-siddhi module in bahir-flink.,"[BAHIR-65] Twitter integration test

Closes #80", MODIFY README.md MODIFY TwitterStreamSuite.scala shouldRunTest
0,BAHIR-30,29d8c7622cf9663e295d7616ae1e1b089fe80da9,Add documentation for streaming-twitter connector  ,[BAHIR-31] Add basic documentation for ZeroMQ connector, ADD README.md
0,BAHIR-43,5e07303c65e2c88cd392691bdfe9f68391f51b5c,Add missing apache license header to sql-mqtt file  ,"[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
0,BAHIR-51,415576ba702206ba9cfc5c8bdbdee4869a1e52ac,"Add additional MQTT options/parameters to MQTTInputDStream and MqttStreamSource We are using Spark Streaming in the automotive IOT environment with MQTT as the data source.
For security reasons our MQTT broker is protected by username and password (as is default for these kind of environments). At the moment it is not possible to set username/password when creating an MQTT Receiver (MQTTInputDStream or MqttStreamSource).

I propose that the MQTTInputDStream and MqttStreamSource be extended to optionally allow setting the following mqtt options / parameters:
* username
* password
* clientId
* cleanSession
* QoS
* connectionTimeout
* keepAliveInterval
* mqttVersion

If this proposal meets your approval I am willing to create a pull request with these changes implemented.


*Note*: The part for MqttInputDStream has been split off into BAHIR-53.","[BAHIR-61] Enable publishing release artifacts from a tag

Enable a --gitTag parameter to identify an RC tag to be used
when publishing artifacts to maven.", MODIFY release-build.sh
0,BAHIR-43,31b3e96d4680c54ff05d4b09f944106d22b62760,Add missing apache license header to sql-mqtt file  ,[BAHIR-52] Fix code paragraph formatting for Akka readme, MODIFY README.md
0,BAHIR-17,76bfd8b2509128ae2a4ed7f59bcf4dac75c0296a,Prepare release based on Apache Spark 2.0.0-preview  ,"[BAHIR-20] Create release helper scripts

Release script to automate :
- Preparing release artifacts
- Publishing maven artifacts in Scala 2.10 and 2.11
- Publishing snapshots", ADD release-build.sh
0,BAHIR-51,31b3e96d4680c54ff05d4b09f944106d22b62760,"Add additional MQTT options/parameters to MQTTInputDStream and MqttStreamSource We are using Spark Streaming in the automotive IOT environment with MQTT as the data source.
For security reasons our MQTT broker is protected by username and password (as is default for these kind of environments). At the moment it is not possible to set username/password when creating an MQTT Receiver (MQTTInputDStream or MqttStreamSource).

I propose that the MQTTInputDStream and MqttStreamSource be extended to optionally allow setting the following mqtt options / parameters:
* username
* password
* clientId
* cleanSession
* QoS
* connectionTimeout
* keepAliveInterval
* mqttVersion

If this proposal meets your approval I am willing to create a pull request with these changes implemented.


*Note*: The part for MqttInputDStream has been split off into BAHIR-53.",[BAHIR-52] Fix code paragraph formatting for Akka readme, MODIFY README.md
0,BAHIR-37,5e07303c65e2c88cd392691bdfe9f68391f51b5c,Prepare release based on Apache Spark 2.0.0  ,"[BAHIR-38] clean Ivy cache during Maven install phase

When we install the org.apache.bahir jars into the local
Maven repository we also need to clean the previous jar
files from the Ivy cache (~/iv2/cache/org.apache.bahir/*)
so spark-submit -packages ... will pick up the new version
from the the local Maven repository.

Closes #14", MODIFY pom.xml
0,BAHIR-187,172d7096147cd0be70687af893a4d71380ce47bf,"Reduce size of sql-cloudant test database Reduce the number of documents from 1967 to 100 in the n_flight.json test file.
 ","[BAHIR-183] Using HDFS for saving message for mqtt source.

Closes #78", MODIFY pom.xml MODIFY org.apache.spark.sql.sources.DataSourceRegister MODIFY CachedMQTTClient.scala createMqttClient MODIFY MQTTStreamSink.scala initialize MODIFY MQTTStreamSource.scala createMicroBatchReader MODIFY MQTTUtils.scala parseConfigParams ADD HDFSMQTTSourceProvider.scala createSource shortName sourceSchema ADD HdfsBasedMQTTStreamSource.scala startWriteNewDataFile initialize deliveryComplete connectionLost isBatchFile commit getOffsetValue addReceivedMessageInfo accept connectComplete getBatch messageArrived stop hasNewMessageForCurrentBatch ADD HDFSBasedMQTTStreamSourceSuite.scala createStreamingDataFrame readBackStreamingResults shutdownHadoop prepareHadoop writeStreamResults
